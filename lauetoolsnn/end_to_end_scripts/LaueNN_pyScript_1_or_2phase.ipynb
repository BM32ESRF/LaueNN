{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b90dda43",
   "metadata": {},
   "source": [
    "# Notebook script for generation of training dataset (supports single and two phase material)\n",
    "\n",
    "## For case of more than two phase, please use the LaueNN_pyScript_3_or_more_phase script\n",
    "\n",
    "## Different steps of data processing is outlined in this notebook (LaueToolsNN GUI does the same thing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c1aea3-9a35-4dae-89bd-240d75d852e6",
   "metadata": {},
   "source": [
    "## Define material and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "879b9b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaueNN path is /home/esrf/purushot/anaconda3/envs/lauenn/lib/python3.7/site-packages/lauetoolsnn\n",
      "['GaN', [3.189, 3.189, 5.185, 90, 90, 120], '191']\n",
      "['Si', [5.4309, 5.4309, 5.4309, 90, 90, 90], '227']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':     #enclosing required because of multiprocessing\n",
    "\n",
    "    ## Import modules used for this Notebook\n",
    "    import os\n",
    "    import json\n",
    "    \n",
    "    ## Get the path of the lauetoolsnn library\n",
    "    import lauetoolsnn\n",
    "    laueNN_path = os.path.dirname(lauetoolsnn.__file__)\n",
    "    print(\"LaueNN path is\", laueNN_path)\n",
    "    \n",
    "    ## Load the json of material and extinctions\n",
    "    with open(os.path.join(laueNN_path, 'lauetools','material.json'),'r') as f:\n",
    "        dict_Materials = json.load(f)\n",
    "    with open(os.path.join(laueNN_path, 'lauetools','extinction.json'),'r') as f:\n",
    "        extinction_json = json.load(f)\n",
    "\n",
    "    ## Modify the dictionary values to add new entries\n",
    "    dict_Materials[\"GaN\"] = [\"GaN\", [3.189, 3.189, 5.185, 90, 90, 120], \"191\"]\n",
    "    dict_Materials[\"Si\"] = [\"Si\", [5.4309, 5.4309, 5.4309, 90, 90, 90], \"227\"]\n",
    "\n",
    "    extinction_json[\"191\"] = \"191\"\n",
    "    extinction_json[\"227\"] = \"227\"\n",
    "\n",
    "    ## verify if extinction is present in CrystalParameters.py file of lauetools (Manually done for now)\n",
    "\n",
    "    ## dump the json back with new values\n",
    "    with open(os.path.join(laueNN_path, 'lauetools','material.json'), 'w') as fp:\n",
    "        json.dump(dict_Materials, fp)\n",
    "    with open(os.path.join(laueNN_path, 'lauetools','extinction.json'), 'w') as fp:\n",
    "        json.dump(extinction_json, fp)\n",
    "\n",
    "    ## Verify if the material is added to the library or not;\n",
    "    from lauetoolsnn.lauetools.dict_LaueTools import dict_Materials\n",
    "    ## if not, restart the console\n",
    "    print(dict_Materials[\"GaN\"])\n",
    "    print(dict_Materials[\"Si\"])\n",
    "    \n",
    "    # =============================================================================\n",
    "    # Step 0: Define the dictionary with all parameters \n",
    "    # =============================================================================\n",
    "    ## User Input dictionary with parameters\n",
    "    ## In case of only one phase/material, keep same value for material_ and material1_ key\n",
    "    input_params = {\n",
    "                    \"global_path\" : os.getcwd(),\n",
    "                    \"prefix\" : \"_2phase\",                 ## prefix for the folder to be created for training dataset\n",
    "\n",
    "                    \"material_\": [\"GaN\", \"Si\"],             ## same key as used in dict_LaueTools\n",
    "                    \"symmetry\": [\"hexagonal\", \"cubic\"],           ## crystal symmetry of material_\n",
    "                    \"SG\": [191, 227], #186                    ## Space group of material_ (None if not known)\n",
    "                    \"hkl_max_identify\" : [6,5],        ## Maximum hkl index to classify in a Laue pattern\n",
    "                    \"nb_grains_per_lp_mat\" : [2,1],        ## max grains to be generated in a Laue Image\n",
    "\n",
    "                    ## hkl_max_identify : can be \"auto\" or integer: Maximum index of HKL to build output classes\n",
    "                    \n",
    "                    # =============================================================================\n",
    "                    # ## Data generation settings\n",
    "                    # =============================================================================\n",
    "                    \"grains_nb_simulate\" : 500,    ## Number of orientations to generate (takes advantage of crystal symmetry)\n",
    "                    \"classes_with_frequency_to_remove\": [100,100], ## classes_with_frequency_to_remove: HKL class with less appearance than \n",
    "                                                                            ##  specified will be ignored in output\n",
    "                    \"desired_classes_output\": [\"all\",\"all\"], ## desired_classes_output : can be all or an integer: to limit the number of output classes\n",
    "\n",
    "                    \"include_small_misorientation\": False, ## to include additional data with small angle misorientation\n",
    "                    \"misorientation\": 5, ##only used if \"include_small_misorientation\" is True\n",
    "                    \"maximum_angle_to_search\":90, ## Angle of radial distribution to reconstruct the histogram (in deg)\n",
    "                    \"step_for_binning\" : 0.1,      ## bin widht of angular radial distribution in degree\n",
    "                    \n",
    "                    # =============================================================================\n",
    "                    #  ## Training parameters\n",
    "                    # =============================================================================\n",
    "                    \"orientation_generation\": \"random\", ## can be random or uniform\n",
    "                    \"batch_size\":100,               ## batches of files to use while training\n",
    "                    \"epochs\":8,                    ## number of epochs for training\n",
    "\n",
    "                    # =============================================================================\n",
    "                    # ## Detector parameters of the Experimental setup\n",
    "                    # =============================================================================\n",
    "                    ## Sample-detector distance, X center, Y center, two detector angles\n",
    "                    \"detectorparameters\" :  [79.61200, 977.8100, 932.1700, 0.4770000, 0.4470000],\n",
    "                    \"pixelsize\" : 0.0734,          ## Detector pixel size\n",
    "                    \"dim1\":2018,                   ## Dimensions of detector in pixels\n",
    "                    \"dim2\":2016,\n",
    "                    \"emin\" : 5,                    ## Minimum and maximum energy to use for simulating Laue Patterns\n",
    "                    \"emax\" : 22,\n",
    "                    \"ccd_label\" : \"sCMOS\",\n",
    "                    \n",
    "                    # =============================================================================\n",
    "                    # ## Prediction parameters\n",
    "                    # =============================================================================\n",
    "                    \"experimental_directory\": os.getcwd(),\n",
    "                    \"experimental_prefix\": r\"nw1_\",\n",
    "                    \"grid_size_x\" : 1,            ## Grid X and Y limit to generate the simulated dataset (a rectangular scan region)\n",
    "                    \"grid_size_y\" : 2,\n",
    "                    \n",
    "                    # =============================================================================\n",
    "                    # ## Prediction Settings\n",
    "                    # =============================================================================\n",
    "                    # model_weight_file: if none, it will select by default the latest H5 weight file, else provide a specific model\n",
    "                    # softmax_threshold_global: thresholding to limit the predicted spots search zone\n",
    "                    # cap_matchrate: any UB matrix providing MR less than this will be ignored\n",
    "                    # coeff: should be same as cap_matchrate or no? (this is for try previous UB matrix)\n",
    "                    # coeff_overlap: coefficient to limit the overlapping between spots; if more than this, new solution will be computed\n",
    "                    # mode_spotCycle: How to cycle through predicted spots (slow or graphmode )\n",
    "                    \"UB_matrix_to_detect\" : 3,\n",
    "                    \"matrix_tolerance\" : [0.6, 0.6],\n",
    "                    \"material_limit\" : [2, 1],\n",
    "                    \"material_phase_always_present\" : 2,\n",
    "                    \"softmax_threshold_global\" : 0.85,\n",
    "                    \"cap_matchrate\" : 0.10,\n",
    "                    \"coeff_overlap\" : 0.3,\n",
    "                    \"mode_spotCycle\" : \"graphmode\",\n",
    "                    ##true for few crystal and prefered texture case, otherwise time consuming; advised for single phase alone\n",
    "                    \"use_previous\" : False,\n",
    "                    \n",
    "                    # =============================================================================\n",
    "                    # # [PEAKSEARCH]\n",
    "                    # =============================================================================\n",
    "                    \"intensity_threshold\" : 1,## for skimage this is of image standard deviation\n",
    "                    \"boxsize\" : 10,## for skimage this is box size to fit\n",
    "                    \"fit_peaks_gaussian\" : 1,## for skimage this is of no sense\n",
    "                    \"FitPixelDev\" : 3, ## for skimage this is distance between peaks to avoid\n",
    "                    \"NumberMaxofFits\" : 3000,## for skimage this is maximum leastquare attempts before giving up\n",
    "                    \"mode\": \"skimage\",\n",
    "\n",
    "                    # =============================================================================\n",
    "                    # # [STRAINCALCULATION]\n",
    "                    # =============================================================================\n",
    "                    \"strain_compute\" : True,\n",
    "                    \"tolerance_strain_refinement\" : [[0.6,0.5,0.4,0.3,0.2,0.1],\n",
    "                                                     [0.6,0.5,0.4,0.3,0.2,0.1]],\n",
    "                    \"free_parameters\" : [\"b\",\"c\",\"alpha\",\"beta\",\"gamma\"],\n",
    "                    \n",
    "                    # =============================================================================\n",
    "                    # # [Additional settings]\n",
    "                    # =============================================================================\n",
    "                    \"residues_threshold\":0.25,\n",
    "                    \"nb_spots_global_threshold\":8,\n",
    "                    \"nb_spots_consider\" : 500,\n",
    "                    # User defined orientation matrix supplied in a file\n",
    "                    \"use_om_user\" : False,\n",
    "                    \"path_user_OM\" : \"\",\n",
    "                    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad431ffc",
   "metadata": {},
   "source": [
    "## Write config file with parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac3284dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_config_GUI = True\n",
    "if __name__ == '__main__':     #enclosing required because of multiprocessing\n",
    "    # global_path: path where all model related files will be saved\n",
    "    global_path = input_params[\"global_path\"]\n",
    "    \n",
    "    ## verify the length of material key\n",
    "    if len(input_params[\"material_\"]) > 2:\n",
    "        print(\"This script uses modules of LaueNN that only supports maximum of two materials; please use multi-material script incase of more than 2phases\")\n",
    "        print(\"The script will run, however, will only use the first 2 materials\")\n",
    "    \n",
    "    if len(input_params[\"material_\"]) == 1:\n",
    "        print(\"only one material is defined\")\n",
    "        ## modify the dictionary for two phase\n",
    "        input_params[\"material_\"].append(input_params[\"material_\"][0])\n",
    "        input_params[\"symmetry\"].append(input_params[\"symmetry\"][0])\n",
    "        input_params[\"SG\"].append(input_params[\"SG\"][0])\n",
    "        input_params[\"hkl_max_identify\"].append(input_params[\"hkl_max_identify\"][0])\n",
    "        input_params[\"nb_grains_per_lp_mat\"].append(input_params[\"nb_grains_per_lp_mat\"][0])\n",
    "        \n",
    "        input_params[\"classes_with_frequency_to_remove\"].append(input_params[\"classes_with_frequency_to_remove\"][0])\n",
    "        input_params[\"desired_classes_output\"].append(input_params[\"desired_classes_output\"][0])\n",
    "        input_params[\"matrix_tolerance\"].append(input_params[\"matrix_tolerance\"][0])\n",
    "        input_params[\"material_limit\"].append(input_params[\"material_limit\"][0])\n",
    "        input_params[\"tolerance_strain_refinement\"].append(input_params[\"tolerance_strain_refinement\"][0])\n",
    "        \n",
    "    if write_config_GUI:\n",
    "        ## write config file for GUI \n",
    "        if input_params[\"material_\"][0] != input_params[\"material_\"][1]:\n",
    "            text_file = open(os.path.join(global_path,input_params[\"material_\"][0]+\"_\"+input_params[\"material_\"][1]+input_params[\"prefix\"]+\".lauenn\"), \"w\")\n",
    "        else:\n",
    "            text_file = open(os.path.join(global_path,input_params[\"material_\"][0]+\"_\"+input_params[\"prefix\"]+\".lauenn\"), \"w\")\n",
    "\n",
    "        text_file.write(\"### config file for LaueNeuralNetwork GUI \\n\")\n",
    "        text_file.write(\"[GLOBAL_DIRECTORY]\\n\")\n",
    "        text_file.write(\"prefix = \"+input_params[\"prefix\"]+\" \\n\")\n",
    "        text_file.write(\"main_directory = \"+global_path+\"\\n\")\n",
    "        text_file.write(\"\\n\")\n",
    "        text_file.write(\"[MATERIAL]\\n\")\n",
    "        text_file.write(\"material = \"+input_params[\"material_\"][0]+\"\\n\")\n",
    "        text_file.write(\"symmetry = \"+input_params[\"symmetry\"][0]+\"\\n\")\n",
    "        text_file.write(\"space_group = \"+str(input_params[\"SG\"][0])+\"\\n\")\n",
    "        text_file.write(\"general_diffraction_rules = false\\n\")\n",
    "        text_file.write(\"\\n\")\n",
    "        text_file.write(\"material1 = \"+input_params[\"material_\"][1]+\"\\n\")\n",
    "        text_file.write(\"symmetry1 = \"+input_params[\"symmetry\"][1]+\"\\n\")\n",
    "        text_file.write(\"space_group1 = \"+str(input_params[\"SG\"][1])+\"\\n\")\n",
    "        text_file.write(\"general_diffraction_rules1 = false\\n\")\n",
    "        text_file.write(\"\\n\")\n",
    "        text_file.write(\"[DETECTOR]\\n\")\n",
    "        text_file.write(\"detectorfile = user_input\"+\" \\n\")\n",
    "        text_file.write(\"params =\"+ \",\".join(str(param) for param in input_params[\"detectorparameters\"])+\",\"+str(input_params[\"pixelsize\"])+\",\"+str(input_params[\"dim1\"])+\",\"+str(input_params[\"dim2\"])+\",\"+str(input_params[\"ccd_label\"])+\" \\n\")\n",
    "        text_file.write(\"emax = \"+str(input_params[\"emax\"])+\"\\n\")\n",
    "        text_file.write(\"emin = \"+str(input_params[\"emin\"])+\"\\n\")\n",
    "        text_file.write(\"\\n\")\n",
    "        text_file.write(\"[TRAINING]\\n\")\n",
    "        text_file.write(\"classes_with_frequency_to_remove = \"+str(input_params[\"classes_with_frequency_to_remove\"][0])+\"\\n\")\n",
    "        text_file.write(\"desired_classes_output = \"+str(input_params[\"desired_classes_output\"][0])+\"\\n\")\n",
    "        text_file.write(\"max_HKL_index = \"+str(input_params[\"hkl_max_identify\"][0])+\"\\n\")\n",
    "        text_file.write(\"max_nb_grains = \"+str(input_params[\"nb_grains_per_lp_mat\"][0])+\"\\n\")\n",
    "        text_file.write(\"classes_with_frequency_to_remove1 = \"+str(input_params[\"classes_with_frequency_to_remove\"][1])+\"\\n\")\n",
    "        text_file.write(\"desired_classes_output1 = \"+str(input_params[\"desired_classes_output\"][1])+\"\\n\")\n",
    "        text_file.write(\"max_HKL_index1 = \"+str(input_params[\"hkl_max_identify\"][1])+\"\\n\")\n",
    "        text_file.write(\"max_nb_grains1 = \"+str(input_params[\"nb_grains_per_lp_mat\"][1])+\"\\n\")\n",
    "        text_file.write(\"max_simulations = \"+str(input_params[\"grains_nb_simulate\"])+\"\\n\")\n",
    "        text_file.write(\"include_small_misorientation = \"+str(input_params[\"include_small_misorientation\"]).lower()+\"\\n\")\n",
    "        text_file.write(\"misorientation_angle = 1 \\n\")\n",
    "        text_file.write(\"angular_distance = \"+str(input_params[\"maximum_angle_to_search\"])+\"\\n\")\n",
    "        text_file.write(\"step_size = \"+str(input_params[\"step_for_binning\"])+\"\\n\")\n",
    "        text_file.write(\"batch_size = \"+str(input_params[\"batch_size\"])+\"\\n\")\n",
    "        text_file.write(\"epochs = \"+str(input_params[\"epochs\"])+\"\\n\")\n",
    "        text_file.write(\"\\n\")\n",
    "        text_file.write(\"[PREDICTION]\\n\")\n",
    "        text_file.write(\"UB_matrix_to_detect = \"+str(input_params[\"UB_matrix_to_detect\"])+\"\\n\")\n",
    "        text_file.write(\"matrix_tolerance = \"+str(input_params[\"matrix_tolerance\"][0])+\"\\n\")\n",
    "        text_file.write(\"matrix_tolerance1 = \"+str(input_params[\"matrix_tolerance\"][1])+\"\\n\")\n",
    "        text_file.write(\"material0_limit = \"+str(input_params[\"material_limit\"][0])+\"\\n\")\n",
    "        text_file.write(\"material1_limit = \"+str(input_params[\"material_limit\"][1])+\"\\n\")\n",
    "        text_file.write(\"softmax_threshold_global = \"+str(input_params[\"softmax_threshold_global\"])+\"\\n\")\n",
    "        text_file.write(\"cap_matchrate = \"+str(input_params[\"cap_matchrate\"])+\"\\n\")\n",
    "        text_file.write(\"coeff = 0.3\\n\")\n",
    "        text_file.write(\"coeff_overlap = \"+str(input_params[\"coeff_overlap\"])+\"\\n\")\n",
    "        text_file.write(\"mode_spotCycle = \"+str(input_params[\"mode_spotCycle\"])+\"\\n\")\n",
    "        text_file.write(\"use_previous = \"+str(input_params[\"use_previous\"]).lower()+\"\\n\")\n",
    "        text_file.write(\"\\n\")\n",
    "        text_file.write(\"[EXPERIMENT]\\n\")\n",
    "        text_file.write(\"experiment_directory = \"+input_params[\"experimental_directory\"]+\"\\n\")\n",
    "        text_file.write(\"experiment_file_prefix = \"+input_params[\"experimental_prefix\"]+\"\\n\")\n",
    "        text_file.write(\"image_grid_x = \"+str(input_params[\"grid_size_x\"])+\"\\n\")\n",
    "        text_file.write(\"image_grid_y = \"+str(input_params[\"grid_size_y\"])+\"\\n\")\n",
    "        text_file.write(\"\\n\")\n",
    "        text_file.write(\"[PEAKSEARCH]\\n\")\n",
    "        text_file.write(\"intensity_threshold = \"+str(input_params[\"intensity_threshold\"])+\"\\n\")\n",
    "        text_file.write(\"boxsize = \"+str(input_params[\"boxsize\"])+\"\\n\")\n",
    "        text_file.write(\"fit_peaks_gaussian = \"+str(input_params[\"fit_peaks_gaussian\"])+\"\\n\")\n",
    "        text_file.write(\"FitPixelDev = \"+str(input_params[\"FitPixelDev\"])+\"\\n\")\n",
    "        text_file.write(\"NumberMaxofFits = \"+str(input_params[\"NumberMaxofFits\"])+\"\\n\")\n",
    "        text_file.write(\"mode = \"+str(input_params[\"mode\"])+\"\\n\")\n",
    "        text_file.write(\"\\n\")\n",
    "        text_file.write(\"[STRAINCALCULATION]\\n\")\n",
    "        text_file.write(\"strain_compute = \"+str(input_params[\"strain_compute\"]).lower()+\"\\n\")\n",
    "        text_file.write(\"tolerance_strain_refinement =\"+ \",\".join(str(param) for param in input_params[\"tolerance_strain_refinement\"][0])+\"\\n\")\n",
    "        text_file.write(\"tolerance_strain_refinement1 =\"+ \",\".join(str(param) for param in input_params[\"tolerance_strain_refinement\"][1])+\"\\n\")\n",
    "        text_file.write(\"free_parameters =\"+ \",\".join(str(param) for param in input_params[\"free_parameters\"])+\"\\n\")\n",
    "        text_file.write(\"\\n\")\n",
    "        text_file.write(\"[DEVELOPMENT]\\n\")\n",
    "        text_file.write(\"material_phase_always_present=\"+str(input_params[\"material_phase_always_present\"])+\"\\n\")\n",
    "        text_file.write(\"write_MTEX_file= true \\n\")\n",
    "        text_file.write(\"[CALLER]\\n\")\n",
    "        text_file.write(\"residues_threshold=\"+str(input_params[\"residues_threshold\"])+\"\\n\")\n",
    "        text_file.write(\"nb_spots_global_threshold=\"+str(input_params[\"nb_spots_global_threshold\"])+\"\\n\")\n",
    "        text_file.write(\"option_global = v2 \\n\")\n",
    "        text_file.write(\"nb_spots_consider = \"+str(input_params[\"nb_spots_consider\"])+\"\\n\")\n",
    "        text_file.write(\"use_om_user = \"+str(input_params[\"use_om_user\"]).lower()+\"\\n\")\n",
    "        text_file.write(\"path_user_OM = \"+str(input_params[\"path_user_OM\"])+\"\\n\")\n",
    "        text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fe8cea-4e05-4c45-9fe6-3ae1356c6f7c",
   "metadata": {},
   "source": [
    "## Generate Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86f67cfd-1825-45ae-b328-4a5e36458441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save directory is : /home/esrf/purushot/Desktop/LaueNN_tutorial/TUtorial_1/LaueNN_script/GaN_Si_2phase\n",
      "Generating HKL objects\n",
      "Removing harmonics and building equivalent HKL objects\n",
      "Verifying if two different HKL class have same angular distribution (can be very time consuming depending on the symmetry)\n",
      "Finalizing the HKL objects\n",
      "Saved class HKL data in : /home/esrf/purushot/Desktop/LaueNN_tutorial/TUtorial_1/LaueNN_script/GaN_Si_2phase//classhkl_data_GaN.pickle\n",
      "Generating HKL objects\n",
      "Removing harmonics and building equivalent HKL objects\n",
      "Verifying if two different HKL class have same angular distribution (can be very time consuming depending on the symmetry)\n",
      "Finalizing the HKL objects\n",
      "Saved class HKL data in : /home/esrf/purushot/Desktop/LaueNN_tutorial/TUtorial_1/LaueNN_script/GaN_Si_2phase//classhkl_data_Si.pickle\n",
      "Verifying if two different HKL class have same angular distribution (can be very time consuming depending on the symmetry)\n",
      "Great! No two HKL class have same angular distribution\n",
      "Verifying if two different HKL class have same angular distribution (can be very time consuming depending on the symmetry)\n",
      "Great! No two HKL class have same angular distribution\n",
      "Generating training_data and saving them\n",
      "Verifying if two different HKL class have same angular distribution (can be very time consuming depending on the symmetry)\n",
      "Great! No two HKL class have same angular distribution\n",
      "Verifying if two different HKL class have same angular distribution (can be very time consuming depending on the symmetry)\n",
      "Great! No two HKL class have same angular distribution\n",
      "Generating testing_data and saving them\n",
      "First material index length: 139\n",
      "Second material index length: 40\n",
      "Class ID and frequency; check for data imbalance and select                             appropriate LOSS function for training the model\n",
      "[(62, 3558), (20, 3543), (102, 3531), (17, 3520), (83, 3516), (126, 3512), (43, 3489), (45, 3471), (24, 3434), (85, 3425), (64, 3405), (47, 3370), (105, 3335), (25, 3274), (68, 3235), (48, 3166), (127, 3118), (87, 3065), (69, 3021), (109, 2904), (88, 2877), (32, 2807), (129, 2726), (110, 2725), (33, 2586), (75, 2534), (130, 2517), (55, 2479), (76, 2294), (115, 2259), (95, 2172), (22, 2121), (46, 2040), (27, 2020), (116, 1992), (50, 1911), (66, 1905), (125, 1792), (19, 1792), (71, 1786), (101, 1784), (124, 1771), (18, 1771), (82, 1770), (136, 1768), (15, 1767), (99, 1764), (3, 1762), (63, 1761), (2, 1756), (103, 1755), (100, 1755), (35, 1755), (84, 1746), (42, 1745), (86, 1742), (60, 1739), (44, 1735), (61, 1735), (81, 1733), (41, 1730), (59, 1729), (21, 1729), (16, 1711), (5, 1710), (14, 1685), (65, 1685), (104, 1676), (56, 1664), (6, 1640), (90, 1606), (107, 1581), (106, 1576), (77, 1527), (26, 1508), (111, 1433), (31, 1420), (49, 1419), (128, 1407), (54, 1382), (70, 1350), (96, 1344), (10, 1301), (89, 1294), (28, 1286), (94, 1254), (51, 1251), (132, 1246), (114, 1160), (118, 1151), (36, 1138), (72, 1126), (131, 1109), (34, 1103), (4, 1050), (137, 1020), (91, 1000), (7, 997), (78, 968), (1, 891), (112, 867), (11, 866), (0, 859), (117, 813), (119, 707), (133, 699), (23, 690), (29, 651), (8, 644), (52, 610), (67, 602), (57, 526), (73, 523), (37, 502), (92, 462), (108, 455), (97, 348), (113, 346), (9, 313), (13, 261), (134, 241), (120, 239), (38, 72), (30, 64), (53, 44), (79, 27), (74, 22), (93, 2), (167, 3585), (148, 3525), (154, 3189), (156, 2865), (157, 2626), (164, 2502), (168, 1947), (145, 1782), (175, 1774), (150, 1770), (144, 1757), (169, 1754), (143, 1753), (162, 1749), (146, 1743), (177, 1734), (149, 1729), (142, 1687), (153, 1667), (171, 1656), (151, 1593), (172, 1492), (161, 1435), (147, 1320), (158, 1189), (159, 1188), (152, 1182), (165, 1149), (173, 1147), (166, 1078), (155, 939), (160, 901), (140, 845), (163, 844), (141, 568), (170, 565), (176, 444), (139, 442), (174, 397), (178, 189)]\n",
      "HKL : [-1.  3.  3.]; occurance : 3558: NN_weights : 1\n",
      "HKL : [-1.  4.  1.]; occurance : 3543: NN_weights : 1\n",
      "HKL : [-1.  3.  5.]; occurance : 3531: NN_weights : 1\n",
      "HKL : [-1.  3.  1.]; occurance : 3520: NN_weights : 1\n",
      "HKL : [-1.  3.  4.]; occurance : 3516: NN_weights : 1\n",
      "HKL : [-1.  3.  6.]; occurance : 3512: NN_weights : 1\n",
      "HKL : [-1.  3.  2.]; occurance : 3489: NN_weights : 1\n",
      "HKL : [-1.  4.  2.]; occurance : 3471: NN_weights : 1\n",
      "HKL : [-2.  5.  1.]; occurance : 3434: NN_weights : 1\n",
      "HKL : [-1.  4.  4.]; occurance : 3425: NN_weights : 1\n",
      "HKL : [-1.  4.  3.]; occurance : 3405: NN_weights : 1\n",
      "HKL : [-2.  5.  2.]; occurance : 3370: NN_weights : 1\n",
      "HKL : [-1.  4.  5.]; occurance : 3335: NN_weights : 1\n",
      "HKL : [-1.  5.  1.]; occurance : 3274: NN_weights : 1\n",
      "HKL : [-2.  5.  3.]; occurance : 3235: NN_weights : 1\n",
      "HKL : [-1.  5.  2.]; occurance : 3166: NN_weights : 1\n",
      "HKL : [-1.  4.  6.]; occurance : 3118: NN_weights : 1\n",
      "HKL : [-2.  5.  4.]; occurance : 3065: NN_weights : 1\n",
      "HKL : [-1.  5.  3.]; occurance : 3021: NN_weights : 1\n",
      "HKL : [-2.  5.  5.]; occurance : 2904: NN_weights : 1\n",
      "HKL : [-1.  5.  4.]; occurance : 2877: NN_weights : 1\n",
      "HKL : [-2.  6.  1.]; occurance : 2807: NN_weights : 1\n",
      "HKL : [-2.  5.  6.]; occurance : 2726: NN_weights : 1\n",
      "HKL : [-1.  5.  5.]; occurance : 2725: NN_weights : 1\n",
      "HKL : [-1.  6.  1.]; occurance : 2586: NN_weights : 1\n",
      "HKL : [-2.  6.  3.]; occurance : 2534: NN_weights : 1\n",
      "HKL : [-1.  5.  6.]; occurance : 2517: NN_weights : 1\n",
      "HKL : [-1.  6.  2.]; occurance : 2479: NN_weights : 1\n",
      "HKL : [-1.  6.  3.]; occurance : 2294: NN_weights : 1\n",
      "HKL : [-2.  6.  5.]; occurance : 2259: NN_weights : 1\n",
      "HKL : [-1.  6.  4.]; occurance : 2172: NN_weights : 1\n",
      "HKL : [3. 4. 1.]; occurance : 2121: NN_weights : 1\n",
      "HKL : [3. 4. 2.]; occurance : 2040: NN_weights : 1\n",
      "HKL : [2. 5. 1.]; occurance : 2020: NN_weights : 1\n",
      "HKL : [-1.  6.  5.]; occurance : 1992: NN_weights : 1\n",
      "HKL : [2. 5. 2.]; occurance : 1911: NN_weights : 1\n",
      "HKL : [3. 4. 3.]; occurance : 1905: NN_weights : 1\n",
      "HKL : [-1.  2.  6.]; occurance : 1792: NN_weights : 2\n",
      "HKL : [-2.  4.  1.]; occurance : 1792: NN_weights : 2\n",
      "HKL : [2. 5. 3.]; occurance : 1786: NN_weights : 2\n",
      "HKL : [0. 2. 5.]; occurance : 1784: NN_weights : 2\n",
      "HKL : [0. 1. 6.]; occurance : 1771: NN_weights : 2\n",
      "HKL : [0. 3. 1.]; occurance : 1771: NN_weights : 2\n",
      "HKL : [-1.  2.  4.]; occurance : 1770: NN_weights : 2\n",
      "HKL : [-1.  6.  6.]; occurance : 1768: NN_weights : 2\n",
      "HKL : [-1.  2.  1.]; occurance : 1767: NN_weights : 2\n",
      "HKL : [0. 1. 5.]; occurance : 1764: NN_weights : 2\n",
      "HKL : [-1.  4.  0.]; occurance : 1762: NN_weights : 2\n",
      "HKL : [-2.  4.  3.]; occurance : 1761: NN_weights : 2\n",
      "HKL : [-1.  3.  0.]; occurance : 1756: NN_weights : 2\n",
      "HKL : [0. 3. 5.]; occurance : 1755: NN_weights : 2\n",
      "HKL : [-1.  2.  5.]; occurance : 1755: NN_weights : 2\n",
      "HKL : [1. 6. 1.]; occurance : 1755: NN_weights : 2\n",
      "HKL : [0. 3. 4.]; occurance : 1746: NN_weights : 2\n",
      "HKL : [-1.  2.  2.]; occurance : 1745: NN_weights : 2\n",
      "HKL : [3. 4. 4.]; occurance : 1742: NN_weights : 2\n",
      "HKL : [-1.  2.  3.]; occurance : 1739: NN_weights : 2\n",
      "HKL : [0. 3. 2.]; occurance : 1735: NN_weights : 2\n",
      "HKL : [0. 2. 3.]; occurance : 1735: NN_weights : 2\n",
      "HKL : [0. 1. 4.]; occurance : 1733: NN_weights : 2\n",
      "HKL : [0. 1. 2.]; occurance : 1730: NN_weights : 2\n",
      "HKL : [0. 1. 3.]; occurance : 1729: NN_weights : 2\n",
      "HKL : [0. 4. 1.]; occurance : 1729: NN_weights : 2\n",
      "HKL : [0. 2. 1.]; occurance : 1711: NN_weights : 2\n",
      "HKL : [-2.  5.  0.]; occurance : 1710: NN_weights : 2\n",
      "HKL : [0. 1. 1.]; occurance : 1685: NN_weights : 2\n",
      "HKL : [0. 4. 3.]; occurance : 1685: NN_weights : 2\n",
      "HKL : [-2.  4.  5.]; occurance : 1676: NN_weights : 2\n",
      "HKL : [1. 6. 2.]; occurance : 1664: NN_weights : 2\n",
      "HKL : [-1.  5.  0.]; occurance : 1640: NN_weights : 2\n",
      "HKL : [2. 5. 4.]; occurance : 1606: NN_weights : 2\n",
      "HKL : [3. 4. 5.]; occurance : 1581: NN_weights : 2\n",
      "HKL : [0. 4. 5.]; occurance : 1576: NN_weights : 2\n",
      "HKL : [1. 6. 3.]; occurance : 1527: NN_weights : 2\n",
      "HKL : [0. 5. 1.]; occurance : 1508: NN_weights : 2\n",
      "HKL : [2. 5. 5.]; occurance : 1433: NN_weights : 2\n",
      "HKL : [-3.  6.  1.]; occurance : 1420: NN_weights : 2\n",
      "HKL : [0. 5. 2.]; occurance : 1419: NN_weights : 2\n",
      "HKL : [3. 4. 6.]; occurance : 1407: NN_weights : 2\n",
      "HKL : [-3.  6.  2.]; occurance : 1382: NN_weights : 2\n",
      "HKL : [0. 5. 3.]; occurance : 1350: NN_weights : 2\n",
      "HKL : [1. 6. 4.]; occurance : 1344: NN_weights : 2\n",
      "HKL : [-1.  6.  0.]; occurance : 1301: NN_weights : 2\n",
      "HKL : [0. 5. 4.]; occurance : 1294: NN_weights : 2\n",
      "HKL : [3. 5. 1.]; occurance : 1286: NN_weights : 2\n",
      "HKL : [-3.  6.  4.]; occurance : 1254: NN_weights : 2\n",
      "HKL : [3. 5. 2.]; occurance : 1251: NN_weights : 2\n",
      "HKL : [2. 5. 6.]; occurance : 1246: NN_weights : 2\n",
      "HKL : [-3.  6.  5.]; occurance : 1160: NN_weights : 3\n",
      "HKL : [1. 6. 5.]; occurance : 1151: NN_weights : 3\n",
      "HKL : [2. 6. 1.]; occurance : 1138: NN_weights : 3\n",
      "HKL : [3. 5. 3.]; occurance : 1126: NN_weights : 3\n",
      "HKL : [0. 5. 6.]; occurance : 1109: NN_weights : 3\n",
      "HKL : [0. 6. 1.]; occurance : 1103: NN_weights : 3\n",
      "HKL : [3. 4. 0.]; occurance : 1050: NN_weights : 3\n",
      "HKL : [1. 6. 6.]; occurance : 1020: NN_weights : 3\n",
      "HKL : [3. 5. 4.]; occurance : 1000: NN_weights : 3\n",
      "HKL : [2. 5. 0.]; occurance : 997: NN_weights : 3\n",
      "HKL : [2. 6. 3.]; occurance : 968: NN_weights : 3\n",
      "HKL : [-1.  2.  0.]; occurance : 891: NN_weights : 4\n",
      "HKL : [3. 5. 5.]; occurance : 867: NN_weights : 4\n",
      "HKL : [1. 6. 0.]; occurance : 866: NN_weights : 4\n",
      "HKL : [0. 1. 0.]; occurance : 859: NN_weights : 4\n",
      "HKL : [0. 6. 5.]; occurance : 813: NN_weights : 4\n",
      "HKL : [2. 6. 5.]; occurance : 707: NN_weights : 5\n",
      "HKL : [3. 5. 6.]; occurance : 699: NN_weights : 5\n",
      "HKL : [4. 4. 1.]; occurance : 690: NN_weights : 5\n",
      "HKL : [4. 5. 1.]; occurance : 651: NN_weights : 5\n",
      "HKL : [3. 5. 0.]; occurance : 644: NN_weights : 5\n",
      "HKL : [4. 5. 2.]; occurance : 610: NN_weights : 5\n",
      "HKL : [4. 4. 3.]; occurance : 602: NN_weights : 5\n",
      "HKL : [3. 6. 2.]; occurance : 526: NN_weights : 6\n",
      "HKL : [4. 5. 3.]; occurance : 523: NN_weights : 6\n",
      "HKL : [3. 6. 1.]; occurance : 502: NN_weights : 7\n",
      "HKL : [4. 5. 4.]; occurance : 462: NN_weights : 7\n",
      "HKL : [4. 4. 5.]; occurance : 455: NN_weights : 7\n",
      "HKL : [3. 6. 4.]; occurance : 348: NN_weights : 10\n",
      "HKL : [4. 5. 5.]; occurance : 346: NN_weights : 10\n",
      "HKL : [4. 5. 0.]; occurance : 313: NN_weights : 11\n",
      "HKL : [0. 0. 1.]; occurance : 261: NN_weights : 13\n",
      "HKL : [4. 5. 6.]; occurance : 241: NN_weights : 14\n",
      "HKL : [3. 6. 5.]; occurance : 239: NN_weights : 15\n",
      "HKL : [1. 3. 5.]; occurance : 3585: NN_weights : 1\n",
      "HKL : [1. 2. 3.]; occurance : 3525: NN_weights : 1\n",
      "HKL : [1. 2. 4.]; occurance : 3189: NN_weights : 1\n",
      "HKL : [1. 3. 4.]; occurance : 2865: NN_weights : 1\n",
      "HKL : [2. 3. 4.]; occurance : 2626: NN_weights : 1\n",
      "HKL : [1. 2. 5.]; occurance : 2502: NN_weights : 1\n",
      "HKL : [2. 3. 5.]; occurance : 1947: NN_weights : 1\n",
      "HKL : [0. 1. 3.]; occurance : 1782: NN_weights : 2\n",
      "HKL : [1. 5. 5.]; occurance : 1774: NN_weights : 2\n",
      "HKL : [1. 3. 3.]; occurance : 1770: NN_weights : 2\n",
      "HKL : [1. 2. 2.]; occurance : 1757: NN_weights : 2\n",
      "HKL : [3. 3. 5.]; occurance : 1754: NN_weights : 2\n",
      "HKL : [1. 1. 2.]; occurance : 1753: NN_weights : 2\n",
      "HKL : [1. 1. 5.]; occurance : 1749: NN_weights : 2\n",
      "HKL : [1. 1. 3.]; occurance : 1743: NN_weights : 2\n",
      "HKL : [3. 5. 5.]; occurance : 1734: NN_weights : 2\n",
      "HKL : [2. 2. 3.]; occurance : 1729: NN_weights : 2\n",
      "HKL : [0. 1. 2.]; occurance : 1687: NN_weights : 2\n",
      "HKL : [1. 1. 4.]; occurance : 1667: NN_weights : 2\n",
      "HKL : [1. 4. 5.]; occurance : 1656: NN_weights : 2\n",
      "HKL : [2. 3. 3.]; occurance : 1593: NN_weights : 2\n",
      "HKL : [2. 4. 5.]; occurance : 1492: NN_weights : 2\n",
      "HKL : [0. 1. 5.]; occurance : 1435: NN_weights : 2\n",
      "HKL : [0. 2. 3.]; occurance : 1320: NN_weights : 2\n",
      "HKL : [3. 3. 4.]; occurance : 1189: NN_weights : 3\n",
      "HKL : [1. 4. 4.]; occurance : 1188: NN_weights : 3\n",
      "HKL : [0. 1. 4.]; occurance : 1182: NN_weights : 3\n",
      "HKL : [2. 2. 5.]; occurance : 1149: NN_weights : 3\n",
      "HKL : [3. 4. 5.]; occurance : 1147: NN_weights : 3\n",
      "HKL : [0. 3. 5.]; occurance : 1078: NN_weights : 3\n",
      "HKL : [0. 3. 4.]; occurance : 939: NN_weights : 3\n",
      "HKL : [3. 4. 4.]; occurance : 901: NN_weights : 3\n",
      "HKL : [0. 1. 1.]; occurance : 845: NN_weights : 4\n",
      "HKL : [0. 2. 5.]; occurance : 844: NN_weights : 4\n",
      "HKL : [1. 1. 1.]; occurance : 568: NN_weights : 6\n",
      "HKL : [0. 4. 5.]; occurance : 565: NN_weights : 6\n",
      "HKL : [2. 5. 5.]; occurance : 444: NN_weights : 8\n",
      "HKL : [0. 0. 1.]; occurance : 442: NN_weights : 8\n",
      "HKL : [4. 4. 5.]; occurance : 397: NN_weights : 9\n",
      "HKL : [4. 5. 5.]; occurance : 189: NN_weights : 18\n",
      "17 classes removed from the classHKL object [removal frequency: 100] (before:179, now:162)\n",
      "17 classes removed from the classHKL object [removal frequency: 100] (before:179, now:162)\n",
      "Saved class weights data\n"
     ]
    }
   ],
   "source": [
    "generate_dataset = True\n",
    "if __name__ == '__main__':     #enclosing required because of multiprocessing\n",
    "   \n",
    "    if generate_dataset:\n",
    "        # =============================================================================\n",
    "        # Step 1 \n",
    "        # =============================================================================\n",
    "        ## if LaueToolsNN is properly installed\n",
    "        from lauetoolsnn.utils_lauenn import get_material_detail, generate_classHKL, generate_dataset, rmv_freq_class\n",
    "        '''\n",
    "        get_material_detail : Extract material parameters\n",
    "        generate_classHKL : Generates List of output HKL for the neural network\n",
    "        generate_dataset : Will simulate laue patterns and will build histogram for the hkl generated by classHKL\n",
    "        rmv_freq_class : cleans the output class dataset\n",
    "        '''\n",
    "        # ## Get material parameters \n",
    "        # ### Generates a folder with material name and gets material unit cell parameters \n",
    "        # ### and symmetry object from the get_material_detail function\n",
    "        material_= input_params[\"material_\"][0]\n",
    "        material1_= input_params[\"material_\"][1]\n",
    "        n = input_params[\"hkl_max_identify\"][0]\n",
    "        n1 = input_params[\"hkl_max_identify\"][1]\n",
    "        maximum_angle_to_search = input_params[\"maximum_angle_to_search\"]\n",
    "        step_for_binning = input_params[\"step_for_binning\"]\n",
    "        nb_grains_per_lp0 = input_params[\"nb_grains_per_lp_mat\"][0]\n",
    "        nb_grains_per_lp1 = input_params[\"nb_grains_per_lp_mat\"][1]\n",
    "        grains_nb_simulate = input_params[\"grains_nb_simulate\"]\n",
    "        detectorparameters = input_params[\"detectorparameters\"]\n",
    "        pixelsize = input_params[\"pixelsize\"]\n",
    "        emax = input_params[\"emax\"]\n",
    "        emin = input_params[\"emin\"]\n",
    "        symm_ = input_params[\"symmetry\"][0]\n",
    "        symm1_ = input_params[\"symmetry\"][1]\n",
    "        SG = input_params[\"SG\"][0]\n",
    "        SG1 = input_params[\"SG\"][1]\n",
    "        \n",
    "        ## read hkl information from a fit file in case too large HKLs\n",
    "        manual_hkl_list=False\n",
    "        if manual_hkl_list:\n",
    "            import numpy as np\n",
    "            temp = np.loadtxt(r\"img_0000_LT_1.fit\")\n",
    "            hkl_array = temp[:,2:5]\n",
    "            hkl_array1 = None\n",
    "        else:\n",
    "            hkl_array = None\n",
    "            hkl_array1 = None\n",
    "            \n",
    "        if material_ != material1_:\n",
    "            save_directory = os.path.join(global_path,material_+\"_\"+material1_+input_params[\"prefix\"])\n",
    "        else:\n",
    "            save_directory = os.path.join(global_path,material_+input_params[\"prefix\"])\n",
    "        print(\"save directory is : \"+save_directory)\n",
    "        if not os.path.exists(save_directory):\n",
    "            os.makedirs(save_directory)\n",
    "        \n",
    "        ## get unit cell parameters and other details required for simulating Laue patterns\n",
    "        rules, symmetry, lattice_material,\\\n",
    "            crystal, SG, rules1, symmetry1,\\\n",
    "            lattice_material1, crystal1, SG1 = get_material_detail(material_, SG, symm_,\n",
    "                                                                   material1_, SG1, symm1_)\n",
    "        \n",
    "        # ## Generate Neural network output classes (Laue spot hkls) using the generate_classHKL function\n",
    "        ## procedure for generation of GROUND TRUTH classes\n",
    "        # general_diff_cond = True will eliminate the hkl index that does not \n",
    "        # satisfy the general reflection conditions, otherwise they will be eliminated in the next stage\n",
    "        generate_classHKL(n, rules, lattice_material, symmetry, material_, \n",
    "                          crystal=crystal, SG=SG, general_diff_cond=False,\n",
    "                          save_directory=save_directory, write_to_console=print, \n",
    "                          ang_maxx = maximum_angle_to_search, \n",
    "                          step = step_for_binning, mat_listHKl=hkl_array)\n",
    "        \n",
    "        if material_ != material1_:\n",
    "            generate_classHKL(n1, rules1, lattice_material1, symmetry1, material1_, \n",
    "                              crystal=crystal1, SG=SG1, general_diff_cond=False,\n",
    "                              save_directory=save_directory, write_to_console=print, \n",
    "                              ang_maxx = maximum_angle_to_search, \n",
    "                              step = step_for_binning, mat_listHKl=hkl_array1)\n",
    "        \n",
    "        \n",
    "        # ## Generate Training and Testing dataset only for the output classes (Laue spot hkls) calculated in the Step 3\n",
    "        # ### Uses multiprocessing library\n",
    "        ############ GENERATING TRAINING DATA ##############\n",
    "        # data_realism =True ; will introduce noise and partial Laue patterns in the training dataset\n",
    "        # modelp can have either \"random\" for random orientation generation or \n",
    "        # \"uniform\" for uniform orientation generation\n",
    "        # include_scm (if True; misorientation_angle parameter need to be defined): \n",
    "        #this parameter introduces misoriented crystal of specific angle along a crystal axis in the training dataset\n",
    "        generate_dataset(material_=material_, material1_=material1_, ang_maxx=maximum_angle_to_search,\n",
    "                         step=step_for_binning, mode=0, \n",
    "                         nb_grains=nb_grains_per_lp0, nb_grains1=nb_grains_per_lp1, \n",
    "                         grains_nb_simulate=grains_nb_simulate, data_realism = True, \n",
    "                         detectorparameters=detectorparameters, pixelsize=pixelsize, \n",
    "                         type_=\"training_data\",\n",
    "                         var0 = 1, dim1=input_params[\"dim1\"], dim2=input_params[\"dim2\"], \n",
    "                         removeharmonics=1, save_directory=save_directory,\n",
    "                         write_to_console=print, \n",
    "                         emin=emin, emax=emax, \n",
    "                         modelp = input_params[\"orientation_generation\"],\n",
    "                         misorientation_angle = input_params[\"misorientation\"], \n",
    "                         general_diff_rules = False, \n",
    "                         crystal = crystal, crystal1 = crystal1, \n",
    "                         include_scm=input_params[\"include_small_misorientation\"])\n",
    "        \n",
    "        ############ GENERATING TESTING DATA ##############\n",
    "        factor = 5 # validation split for the training dataset  --> corresponds to 20% of total training dataset\n",
    "        generate_dataset(material_=material_, material1_=material1_, ang_maxx=maximum_angle_to_search,\n",
    "                         step=step_for_binning, mode=0, \n",
    "                         nb_grains=nb_grains_per_lp0, nb_grains1=nb_grains_per_lp1, \n",
    "                         grains_nb_simulate=grains_nb_simulate//factor, data_realism = True, \n",
    "                         detectorparameters=detectorparameters, pixelsize=pixelsize, \n",
    "                         type_=\"testing_data\",\n",
    "                         var0 = 1, dim1=input_params[\"dim1\"], dim2=input_params[\"dim2\"], \n",
    "                         removeharmonics=1, save_directory=save_directory,\n",
    "                         write_to_console=print, \n",
    "                         emin=emin, emax=emax, \n",
    "                         modelp = input_params[\"orientation_generation\"],\n",
    "                         misorientation_angle = input_params[\"misorientation\"], \n",
    "                         general_diff_rules = False, \n",
    "                         crystal = crystal, crystal1 = crystal1, \n",
    "                         include_scm=input_params[\"include_small_misorientation\"])\n",
    "        \n",
    "        ## Updating the ClassHKL list by removing the non-common HKL or less frequent HKL from the list\n",
    "        ## The non-common HKL can occur as a result of the detector position and energy used\n",
    "        # freq_rmv: remove output hkl if the training dataset has less tha 100 occurances of \n",
    "        # the considered hkl (freq_rmv1 for second phase)\n",
    "        # Weights (penalty during training) are also calculated based on the occurance\n",
    "        rmv_freq_class(freq_rmv = input_params[\"classes_with_frequency_to_remove\"][0], \n",
    "                       freq_rmv1 = input_params[\"classes_with_frequency_to_remove\"][1], \n",
    "                       save_directory=save_directory, \n",
    "                       material_=material_, \n",
    "                       material1_=material1_, \n",
    "                       write_to_console=print,\n",
    "                       list_hkl_keep=None, list_hkl_keep1=None)\n",
    "        \n",
    "        ## End of data generation for Neural network training: all files are saved in the \n",
    "        ## same folder to be later used for training and prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7c6027-169f-4fcd-969d-e35fa37bdaa5",
   "metadata": {},
   "source": [
    "## Train a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4070c2f-68ec-4d88-90de-276b30abab67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory where training dataset is stored is : /home/esrf/purushot/Desktop/LaueNN_tutorial/TUtorial_1/LaueNN_script/GaN_Si_2phase\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 900)               810900    \n",
      "                                                                 \n",
      " activation (Activation)     (None, 900)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 900)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1665)              1500165   \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 1665)              0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1665)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2430)              4048380   \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 2430)              0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 2430)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 162)               393822    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,753,267\n",
      "Trainable params: 6,753,267\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Number of spots in a batch of 100 files : 11013\n",
      "Min, Max class ID is 0, 161\n",
      "Epoch 1/8\n",
      "25/25 [==============================] - 31s 1s/step - loss: 6.0170 - fn: 246983.0000 - fp: 760.0000 - tn: 44455048.0000 - tp: 29140.0000 - precision: 0.9746 - accuracy: 0.1055 - val_loss: 0.9516 - val_fn: 20836.0000 - val_fp: 60.0000 - val_tn: 8814851.0000 - val_tp: 33915.0000 - val_precision: 0.9982 - val_accuracy: 0.6194\n",
      "Epoch 2/8\n",
      "25/25 [==============================] - 21s 874ms/step - loss: 1.2634 - fn: 73105.0000 - fp: 4871.0000 - tn: 44450928.0000 - tp: 203018.0000 - precision: 0.9766 - accuracy: 0.7352 - val_loss: 0.3896 - val_fn: 5904.0000 - val_fp: 474.0000 - val_tn: 8814437.0000 - val_tp: 48847.0000 - val_precision: 0.9904 - val_accuracy: 0.8922\n",
      "Epoch 3/8\n",
      "25/25 [==============================] - 24s 984ms/step - loss: 0.7132 - fn: 35595.0000 - fp: 5398.0000 - tn: 44450404.0000 - tp: 240528.0000 - precision: 0.9781 - accuracy: 0.8711 - val_loss: 0.2934 - val_fn: 4209.0000 - val_fp: 527.0000 - val_tn: 8814384.0000 - val_tp: 50542.0000 - val_precision: 0.9897 - val_accuracy: 0.9231\n",
      "Epoch 4/8\n",
      "25/25 [==============================] - 22s 899ms/step - loss: 0.5495 - fn: 27413.0000 - fp: 5069.0000 - tn: 44450732.0000 - tp: 248710.0000 - precision: 0.9800 - accuracy: 0.9007 - val_loss: 0.2499 - val_fn: 3544.0000 - val_fp: 571.0000 - val_tn: 8814340.0000 - val_tp: 51207.0000 - val_precision: 0.9890 - val_accuracy: 0.9353\n",
      "Epoch 5/8\n",
      "25/25 [==============================] - 22s 916ms/step - loss: 0.4609 - fn: 23484.0000 - fp: 4912.0000 - tn: 44450888.0000 - tp: 252639.0000 - precision: 0.9809 - accuracy: 0.9150 - val_loss: 0.2225 - val_fn: 3057.0000 - val_fp: 561.0000 - val_tn: 8814350.0000 - val_tp: 51694.0000 - val_precision: 0.9893 - val_accuracy: 0.9442\n",
      "Epoch 6/8\n",
      "25/25 [==============================] - 23s 952ms/step - loss: 0.3979 - fn: 20523.0000 - fp: 4725.0000 - tn: 44451084.0000 - tp: 255600.0000 - precision: 0.9818 - accuracy: 0.9257 - val_loss: 0.2071 - val_fn: 2799.0000 - val_fp: 583.0000 - val_tn: 8814328.0000 - val_tp: 51952.0000 - val_precision: 0.9889 - val_accuracy: 0.9489\n",
      "Epoch 7/8\n",
      "25/25 [==============================] - 24s 985ms/step - loss: 0.3549 - fn: 18428.0000 - fp: 4762.0000 - tn: 44451048.0000 - tp: 257695.0000 - precision: 0.9819 - accuracy: 0.9333 - val_loss: 0.1941 - val_fn: 2577.0000 - val_fp: 569.0000 - val_tn: 8814342.0000 - val_tp: 52174.0000 - val_precision: 0.9892 - val_accuracy: 0.9529\n",
      "Epoch 8/8\n",
      "25/25 [==============================] - 23s 960ms/step - loss: 0.3206 - fn: 16618.0000 - fp: 4489.0000 - tn: 44451308.0000 - tp: 259505.0000 - precision: 0.9830 - accuracy: 0.9398 - val_loss: 0.1918 - val_fn: 2520.0000 - val_fp: 625.0000 - val_tn: 8814286.0000 - val_tp: 52231.0000 - val_precision: 0.9882 - val_accuracy: 0.9540\n",
      "Saved model to disk\n",
      "Training Accuracy: 0.9398166537284851\n",
      "Training Loss: 0.3205784261226654\n",
      "Validation Accuracy: 0.9539734721183777\n",
      "Validation Loss: 0.1917879432439804\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97        19\n",
      "           1       0.95      0.95      0.95        22\n",
      "           2       1.00      0.97      0.99        35\n",
      "           3       0.95      1.00      0.97        38\n",
      "           4       1.00      1.00      1.00        24\n",
      "           5       0.95      0.95      0.95        40\n",
      "           6       1.00      0.95      0.97        39\n",
      "           7       1.00      0.83      0.91        18\n",
      "           8       1.00      1.00      1.00        13\n",
      "           9       1.00      1.00      1.00         9\n",
      "          10       0.97      1.00      0.98        30\n",
      "          11       1.00      1.00      1.00        16\n",
      "          12       1.00      1.00      1.00         6\n",
      "          13       0.95      1.00      0.97        39\n",
      "          14       1.00      0.95      0.98        43\n",
      "          15       1.00      1.00      1.00        38\n",
      "          16       0.95      0.95      0.95        79\n",
      "          17       0.98      0.95      0.96        43\n",
      "          18       0.97      0.93      0.95        42\n",
      "          19       0.95      0.96      0.96        78\n",
      "          20       0.98      0.98      0.98        43\n",
      "          21       0.90      0.96      0.93        49\n",
      "          22       0.88      1.00      0.94        15\n",
      "          23       0.96      0.95      0.95        75\n",
      "          24       0.96      0.91      0.93        77\n",
      "          25       1.00      1.00      1.00        36\n",
      "          26       0.97      0.92      0.95        39\n",
      "          27       0.88      0.96      0.92        24\n",
      "          28       0.94      0.94      0.94        16\n",
      "          29       0.97      1.00      0.98        29\n",
      "          30       0.96      0.99      0.97        68\n",
      "          31       0.98      0.98      0.98        53\n",
      "          32       1.00      0.96      0.98        24\n",
      "          33       0.97      0.92      0.94        37\n",
      "          34       0.92      0.92      0.92        13\n",
      "          35       0.77      0.91      0.83        11\n",
      "          36       0.95      0.97      0.96        37\n",
      "          37       0.95      1.00      0.97        36\n",
      "          38       0.97      0.96      0.97        72\n",
      "          39       1.00      1.00      1.00        33\n",
      "          40       0.98      0.90      0.94        72\n",
      "          41       0.96      0.96      0.96        46\n",
      "          42       0.97      0.94      0.96        70\n",
      "          43       0.94      0.97      0.96        69\n",
      "          44       0.97      1.00      0.99        34\n",
      "          45       0.95      0.93      0.94        43\n",
      "          46       0.93      1.00      0.97        28\n",
      "          47       1.00      0.94      0.97        16\n",
      "          48       0.92      1.00      0.96        33\n",
      "          49       0.93      0.91      0.92        56\n",
      "          50       1.00      0.94      0.97        34\n",
      "          51       0.79      0.92      0.85        12\n",
      "          52       1.00      1.00      1.00        36\n",
      "          53       0.94      1.00      0.97        31\n",
      "          54       1.00      1.00      1.00        42\n",
      "          55       0.97      0.95      0.96        65\n",
      "          56       0.97      0.97      0.97        34\n",
      "          57       1.00      0.96      0.98        67\n",
      "          58       0.92      1.00      0.96        36\n",
      "          59       0.97      0.93      0.95        41\n",
      "          60       1.00      0.94      0.97        17\n",
      "          61       0.97      1.00      0.98        65\n",
      "          62       0.94      0.90      0.92        73\n",
      "          63       1.00      1.00      1.00        35\n",
      "          64       0.97      1.00      0.99        38\n",
      "          65       1.00      0.95      0.97        20\n",
      "          66       1.00      0.94      0.97        16\n",
      "          67       0.96      0.91      0.93        55\n",
      "          68       0.96      0.96      0.96        55\n",
      "          69       0.94      0.91      0.93        34\n",
      "          70       0.94      1.00      0.97        17\n",
      "          71       1.00      1.00      1.00        36\n",
      "          72       0.88      0.94      0.91        31\n",
      "          73       0.96      0.97      0.96        67\n",
      "          74       1.00      1.00      1.00        40\n",
      "          75       0.96      0.99      0.97        68\n",
      "          76       0.94      0.89      0.91        36\n",
      "          77       0.98      0.98      0.98        59\n",
      "          78       1.00      0.90      0.95        63\n",
      "          79       1.00      0.94      0.97        32\n",
      "          80       0.91      0.97      0.94        32\n",
      "          81       1.00      1.00      1.00        20\n",
      "          82       0.89      0.89      0.89         9\n",
      "          83       0.95      0.95      0.95        22\n",
      "          84       0.98      0.92      0.95        59\n",
      "          85       0.93      1.00      0.96        37\n",
      "          86       0.71      1.00      0.83         5\n",
      "          87       0.94      1.00      0.97        33\n",
      "          88       0.94      0.94      0.94        34\n",
      "          89       0.92      1.00      0.96        35\n",
      "          90       0.95      0.98      0.97        61\n",
      "          91       0.95      0.93      0.94        42\n",
      "          92       0.97      1.00      0.98        32\n",
      "          93       0.97      0.89      0.93        73\n",
      "          94       0.97      1.00      0.99        38\n",
      "          95       1.00      0.93      0.96        29\n",
      "          96       1.00      1.00      1.00        10\n",
      "          97       0.98      0.94      0.96        53\n",
      "          98       0.98      0.93      0.95        56\n",
      "          99       0.93      0.96      0.94        26\n",
      "         100       0.93      0.93      0.93        15\n",
      "         101       0.90      1.00      0.95         9\n",
      "         102       1.00      1.00      1.00        12\n",
      "         103       0.95      0.90      0.92        39\n",
      "         104       0.96      0.94      0.95        51\n",
      "         105       0.86      0.95      0.90        19\n",
      "         106       0.97      0.97      0.97        32\n",
      "         107       1.00      0.82      0.90        11\n",
      "         108       1.00      1.00      1.00         6\n",
      "         109       0.97      0.92      0.95        38\n",
      "         110       0.98      1.00      0.99        42\n",
      "         111       0.95      0.98      0.97        63\n",
      "         112       1.00      0.92      0.96        64\n",
      "         113       0.90      0.90      0.90        21\n",
      "         114       0.93      0.93      0.93        43\n",
      "         115       0.97      0.95      0.96        59\n",
      "         116       0.96      1.00      0.98        25\n",
      "         117       1.00      1.00      1.00        21\n",
      "         118       0.82      1.00      0.90        14\n",
      "         119       0.60      1.00      0.75         3\n",
      "         120       0.97      0.86      0.91        36\n",
      "         121       1.00      0.95      0.97        20\n",
      "         122       1.00      1.00      1.00         8\n",
      "         123       0.92      1.00      0.96        12\n",
      "         124       1.00      1.00      1.00         8\n",
      "         125       1.00      1.00      1.00        28\n",
      "         126       0.96      0.93      0.95        28\n",
      "         127       0.96      0.96      0.96        28\n",
      "         128       0.97      0.97      0.97        32\n",
      "         129       1.00      0.97      0.98        30\n",
      "         130       1.00      0.91      0.95        23\n",
      "         131       0.98      1.00      0.99        54\n",
      "         132       0.97      1.00      0.98        29\n",
      "         133       0.83      1.00      0.91        25\n",
      "         134       0.93      1.00      0.96        25\n",
      "         135       0.93      1.00      0.96        25\n",
      "         136       0.97      0.94      0.95        33\n",
      "         137       0.98      1.00      0.99        51\n",
      "         138       0.91      1.00      0.95        20\n",
      "         139       0.95      0.95      0.95        40\n",
      "         140       1.00      1.00      1.00        38\n",
      "         141       0.89      1.00      0.94        17\n",
      "         142       0.88      1.00      0.94        15\n",
      "         143       1.00      1.00      1.00        11\n",
      "         144       0.96      0.89      0.92        27\n",
      "         145       0.97      1.00      0.98        32\n",
      "         146       0.95      1.00      0.97        19\n",
      "         147       1.00      1.00      1.00        41\n",
      "         148       0.96      1.00      0.98        22\n",
      "         149       1.00      1.00      1.00        22\n",
      "         150       0.98      0.98      0.98        51\n",
      "         151       0.96      0.96      0.96        28\n",
      "         152       1.00      1.00      1.00        31\n",
      "         153       1.00      1.00      1.00        12\n",
      "         154       0.95      1.00      0.97        19\n",
      "         155       1.00      0.96      0.98        24\n",
      "         156       0.94      1.00      0.97        16\n",
      "         157       1.00      1.00      1.00         2\n",
      "         158       0.89      1.00      0.94        24\n",
      "         159       0.86      1.00      0.92         6\n",
      "         160       0.90      1.00      0.95        28\n",
      "         161       0.67      1.00      0.80         2\n",
      "\n",
      "    accuracy                           0.96      5515\n",
      "   macro avg       0.95      0.97      0.96      5515\n",
      "weighted avg       0.96      0.96      0.96      5515\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_network = True\n",
    "if __name__ == '__main__':     #enclosing required because of multiprocessing\n",
    "\n",
    "    if train_network:\n",
    "        import numpy as np\n",
    "        import os\n",
    "        import _pickle as cPickle\n",
    "        import itertools\n",
    "        from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "        import matplotlib.pyplot as plt\n",
    "        ## if LaueToolsNN is properly installed\n",
    "        from lauetoolsnn.utils_lauenn import array_generator, array_generator_verify, vali_array\n",
    "        # ## Defining a neural network architecture or load a predefined one from NNmodels\n",
    "        from lauetoolsnn.NNmodels import model_arch_general, LoggingCallback\n",
    "            \n",
    "        material_= input_params[\"material_\"][0]\n",
    "        material1_= input_params[\"material_\"][1]\n",
    "        nb_grains_per_lp = input_params[\"nb_grains_per_lp_mat\"][0]\n",
    "        nb_grains_per_lp1 = input_params[\"nb_grains_per_lp_mat\"][1]\n",
    "        grains_nb_simulate = input_params[\"grains_nb_simulate\"]\n",
    "        \n",
    "        if material_ != material1_:\n",
    "            save_directory = os.path.join(global_path,material_+\"_\"+material1_+input_params[\"prefix\"])\n",
    "        else:\n",
    "            save_directory = os.path.join(global_path,material_+input_params[\"prefix\"])\n",
    "        \n",
    "        if not os.path.exists(save_directory):\n",
    "            print(\"The directory doesn't exists; please veify the path\")\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Directory where training dataset is stored is : \"+save_directory)\n",
    "        \n",
    "        # ## Load the necessary files generated in Step 1 script\n",
    "        # ### Loading the Output class and ground truth        \n",
    "        classhkl = np.load(save_directory+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_0\"]\n",
    "        angbins = np.load(save_directory+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_1\"]\n",
    "        loc_new = np.load(save_directory+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_2\"]\n",
    "        with open(save_directory+\"//class_weights.pickle\", \"rb\") as input_file:\n",
    "            class_weights = cPickle.load(input_file)\n",
    "        class_weights = class_weights[0]\n",
    "        \n",
    "        # ## Training  \n",
    "        # load model and train\n",
    "        model = model_arch_general(len(angbins)-1, len(classhkl),\n",
    "                                    kernel_coeff = 1e-5,\n",
    "                                    bias_coeff = 1e-6,\n",
    "                                    lr = 1e-3)\n",
    "        \n",
    "        ## temp function to quantify the spots and classes present in a batch\n",
    "        batch_size = input_params[\"batch_size\"] \n",
    "        trainy_inbatch = array_generator_verify(save_directory+\"//training_data\", \n",
    "                                                batch_size, \n",
    "                                                len(classhkl), \n",
    "                                                loc_new, \n",
    "                                                print)\n",
    "        print(\"Number of spots in a batch of %i files : %i\" %(batch_size, len(trainy_inbatch)))\n",
    "        print(\"Min, Max class ID is %i, %i\" %(np.min(trainy_inbatch), np.max(trainy_inbatch)))\n",
    "        \n",
    "        epochs = input_params[\"epochs\"] \n",
    "        ## Batch loading for numpy grain files (Keep low value to avoid overcharging the RAM)\n",
    "        if material_ != material1_:\n",
    "            nb_grains_list = list(range(nb_grains_per_lp+1))\n",
    "            nb_grains1_list = list(range(nb_grains_per_lp1+1))\n",
    "            list_permute = list(itertools.product(nb_grains_list, nb_grains1_list))\n",
    "            list_permute.pop(0)\n",
    "            steps_per_epoch = (len(list_permute) * grains_nb_simulate)//batch_size\n",
    "        else:\n",
    "            steps_per_epoch = int((nb_grains_per_lp * grains_nb_simulate) / batch_size)\n",
    "        \n",
    "        val_steps_per_epoch = int(steps_per_epoch / 5)\n",
    "        if steps_per_epoch == 0:\n",
    "            steps_per_epoch = 1\n",
    "        if val_steps_per_epoch == 0:\n",
    "            val_steps_per_epoch = 1 \n",
    "\n",
    "        ## Load generator objects from filepaths (iterators for Training and Testing datasets)\n",
    "        training_data_generator = array_generator(save_directory+\"//training_data\", batch_size,len(classhkl), loc_new, print)\n",
    "        testing_data_generator = array_generator(save_directory+\"//testing_data\", batch_size,len(classhkl), loc_new, print)\n",
    "        \n",
    "        # model save directory and filename\n",
    "        if material_ != material1_:\n",
    "            model_name = save_directory+\"//model_\"+material_+\"_\"+material1_\n",
    "        else:\n",
    "            model_name = save_directory+\"//model_\"+material_\n",
    "            \n",
    "        ######### TRAIN THE DATA\n",
    "        es = EarlyStopping(monitor='val_accuracy', mode='max', patience=2)\n",
    "        ms = ModelCheckpoint(save_directory+\"//best_val_acc_model.h5\", \n",
    "                             monitor='val_accuracy', \n",
    "                             mode='max', save_best_only=True)\n",
    "        lc = LoggingCallback(None, None, None, model, model_name)\n",
    "\n",
    "        ## Fitting function\n",
    "        stats_model = model.fit(\n",
    "                                training_data_generator, \n",
    "                                epochs=epochs, \n",
    "                                steps_per_epoch=steps_per_epoch,\n",
    "                                validation_data=testing_data_generator,\n",
    "                                validation_steps=val_steps_per_epoch,\n",
    "                                verbose=1,\n",
    "                                class_weight=class_weights,\n",
    "                                callbacks=[es, ms, lc]\n",
    "                                )\n",
    "        \n",
    "        # Save model config and weights\n",
    "        model_json = model.to_json()\n",
    "        with open(model_name+\".json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)            \n",
    "        # serialize weights to HDF5\n",
    "        model.save_weights(model_name+\".h5\")\n",
    "        print(\"Saved model to disk\")\n",
    "        \n",
    "        print( \"Training Accuracy: \"+str( stats_model.history['accuracy'][-1]))\n",
    "        print( \"Training Loss: \"+str( stats_model.history['loss'][-1]))\n",
    "        print( \"Validation Accuracy: \"+str( stats_model.history['val_accuracy'][-1]))\n",
    "        print( \"Validation Loss: \"+str( stats_model.history['val_loss'][-1]))\n",
    "        \n",
    "        # Plot the accuracy/loss v Epochs\n",
    "        epochs = range(1, len(model.history.history['loss']) + 1)\n",
    "        fig, ax = plt.subplots(1,2)\n",
    "        ax[0].plot(epochs, model.history.history['loss'], 'r', label='Training loss')\n",
    "        ax[0].plot(epochs, model.history.history['val_loss'], 'r', ls=\"dashed\", label='Validation loss')\n",
    "        ax[0].legend()\n",
    "        ax[1].plot(epochs, model.history.history['accuracy'], 'g', label='Training Accuracy')\n",
    "        ax[1].plot(epochs, model.history.history['val_accuracy'], 'g', ls=\"dashed\", label='Validation Accuracy')\n",
    "        ax[1].legend()\n",
    "        if material_ != material1_:\n",
    "            plt.savefig(save_directory+\"//loss_accuracy_\"+material_+\"_\"+material1_+\".png\", \n",
    "                        bbox_inches='tight',format='png', dpi=1000)\n",
    "        else:\n",
    "            plt.savefig(save_directory+\"//loss_accuracy_\"+material_+\".png\", \n",
    "                        bbox_inches='tight',format='png', dpi=1000)\n",
    "        plt.close()\n",
    "        \n",
    "        if material_ != material1_:\n",
    "            text_file = open(save_directory+\"//loss_accuracy_logger_\"+material_+\"_\"+material1_+\".txt\", \"w\")\n",
    "        else:\n",
    "            text_file = open(save_directory+\"//loss_accuracy_logger_\"+material_+\".txt\", \"w\")\n",
    "        \n",
    "        text_file.write(\"# EPOCH, LOSS, VAL_LOSS, ACCURACY, VAL_ACCURACY\" + \"\\n\")\n",
    "        for inj in range(len(epochs)):\n",
    "            string1 = str(epochs[inj]) + \",\"\n",
    "            string1 = string1 + str(model.history.history['loss'][inj])\n",
    "            string1 = string1 + \",\"+str(model.history.history['val_loss'][inj])\n",
    "            string1 = string1 + \",\"+str(model.history.history['accuracy'][inj])\n",
    "            string1 = string1 + \",\"+str(model.history.history['val_accuracy'][inj])+\" \\n\"  \n",
    "            text_file.write(string1)\n",
    "        text_file.close() \n",
    "        \n",
    "        # ## Stats on the trained model with sklearn metrics\n",
    "        from sklearn.metrics import classification_report\n",
    "        x_test, y_test = vali_array(save_directory+\"//testing_data\", 50, len(classhkl), loc_new, print)\n",
    "        y_test = np.argmax(y_test, axis=-1)\n",
    "        y_pred = np.argmax(model.predict(x_test), axis=-1)\n",
    "        print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4708d850-eca4-4c96-ae48-fba713e1295c",
   "metadata": {},
   "source": [
    "## Index the Laue patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf0fea36-c55f-44bc-b7e1-8ef2e7121ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs available :  64\n",
      "Directory where trained model is stored : /home/esrf/purushot/Desktop/LaueNN_tutorial/TUtorial_1/LaueNN_script/GaN_Si_2phase\n",
      "Writing settings file in /home/esrf/purushot/anaconda3/envs/lauenn/lib/python3.7/site-packages/lauetoolsnn/settings.ini\n",
      "Constructing model\n",
      "Uploading weights to model\n",
      "All model files found and loaded\n",
      "expected 2 files based on the XY grid (1,2) defined by user\n",
      "and found 2 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skimage mode (strict constraints) of PeakSearch is used for the PeakSearch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:00<00:00, 20.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skimage mode (strict constraints) of PeakSearch is used for the PeakSearch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_prediction = True\n",
    "if __name__ == '__main__':     #enclosing required because of multiprocessing\n",
    "   \n",
    "    if run_prediction:\n",
    "        import numpy as np\n",
    "        import multiprocessing, os, time, datetime\n",
    "        import configparser, glob, re\n",
    "        ## if LaueToolsNN is properly installed\n",
    "        from lauetoolsnn.utils_lauenn import  get_material_detail, new_MP_function, resource_path, global_plots\n",
    "        from lauetoolsnn.lauetools import dict_LaueTools as dictLT\n",
    "        from lauetoolsnn.NNmodels import read_hdf5\n",
    "        from keras.models import model_from_json\n",
    "        import _pickle as cPickle\n",
    "        from tqdm import tqdm\n",
    "\n",
    "        ncpu = multiprocessing.cpu_count()\n",
    "        print(\"Number of CPUs available : \", ncpu)\n",
    "\n",
    "        # ## Get material parameters \n",
    "        # ### Get model and data paths from the input\n",
    "        # ### User input parameters for various algorithms to compute the orientation matrix\n",
    "\n",
    "        material_= input_params[\"material_\"][0]\n",
    "        material1_= input_params[\"material_\"][1]\n",
    "        detectorparameters = input_params[\"detectorparameters\"]\n",
    "        pixelsize = input_params[\"pixelsize\"]\n",
    "        emax = input_params[\"emax\"]\n",
    "        emin = input_params[\"emin\"]\n",
    "        dim1 = input_params[\"dim1\"]\n",
    "        dim2 = input_params[\"dim2\"]\n",
    "        symm_ = input_params[\"symmetry\"][0]\n",
    "        symm1_ = input_params[\"symmetry\"][1]\n",
    "        SG = input_params[\"SG\"][0]\n",
    "        SG1 = input_params[\"SG\"][1]\n",
    "\n",
    "        if material_ != material1_:\n",
    "            model_direc = os.path.join(global_path,material_+\"_\"+material1_+input_params[\"prefix\"])\n",
    "        else:\n",
    "            model_direc = os.path.join(global_path,material_+input_params[\"prefix\"])\n",
    "\n",
    "        if not os.path.exists(model_direc):\n",
    "            print(\"The directory doesn't exists; please veify the path\")\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Directory where trained model is stored : \"+model_direc)\n",
    "\n",
    "        if material_ != material1_:\n",
    "            prefix1 = material_+\"_\"+material1_\n",
    "        else:\n",
    "            prefix1 = material_\n",
    "\n",
    "        filenameDirec = input_params[\"experimental_directory\"]\n",
    "        experimental_prefix = input_params[\"experimental_prefix\"]\n",
    "        lim_x, lim_y = input_params[\"grid_size_x\"], input_params[\"grid_size_y\"] \n",
    "        format_file = dictLT.dict_CCD[\"sCMOS\"][7]\n",
    "        ## Experimental peak search parameters in case of RAW LAUE PATTERNS from detector\n",
    "        intensity_threshold = input_params[\"intensity_threshold\"]\n",
    "        boxsize = input_params[\"boxsize\"]\n",
    "        fit_peaks_gaussian = input_params[\"fit_peaks_gaussian\"]\n",
    "        FitPixelDev = input_params[\"FitPixelDev\"]\n",
    "        NumberMaxofFits = input_params[\"NumberMaxofFits\"]\n",
    "        bkg_treatment = \"A-B\"\n",
    "        mode_peaksearch = input_params[\"mode\"]\n",
    "\n",
    "        ## get unit cell parameters and other details required for simulating Laue patterns\n",
    "        rules, symmetry, lattice_material,\\\n",
    "            crystal, SG, rules1, symmetry1,\\\n",
    "                lattice_material1, crystal1, SG1 = get_material_detail(material_, SG, symm_,\n",
    "                                                                   material1_, SG1, symm1_)\n",
    "\n",
    "        ## get proper Laue group to compute the inverse pole figure colors and write MTEX output file for orientation analysis\n",
    "        material0_lauegroup = \"3\"\n",
    "        ## incase of same material\n",
    "        material1_lauegroup = \"5\"\n",
    "\n",
    "        ## Requirements\n",
    "        ubmat = input_params[\"UB_matrix_to_detect\"] # How many orientation matrix to detect per Laue pattern\n",
    "        mode_spotCycle = input_params[\"mode_spotCycle\"] ## mode of calculation\n",
    "        use_previous_UBmatrix_name = input_params[\"use_previous\"] ## Try previous indexation solutions to speed up the process\n",
    "        strain_calculation = input_params[\"strain_compute\"] ## Strain refinement is required or not\n",
    "        ccd_label_global = input_params[\"ccd_label\"]\n",
    "\n",
    "        ## tolerance angle to match simulated and experimental spots for two materials\n",
    "        tolerance = input_params[\"matrix_tolerance\"][0]\n",
    "        tolerance1 = input_params[\"matrix_tolerance\"][1]\n",
    "\n",
    "        ## tolerance angle for strain refinements\n",
    "        tolerance_strain = input_params[\"tolerance_strain_refinement\"][0]\n",
    "        tolerance_strain1 = input_params[\"tolerance_strain_refinement\"][1]\n",
    "        strain_free_parameters = input_params[\"free_parameters\"]\n",
    "\n",
    "        ## Parameters to control the orientation matrix indexation\n",
    "        softmax_threshold_global = input_params[\"softmax_threshold_global\"] # softmax_threshold of the Neural network to consider\n",
    "        mr_threshold_global = 0.90 # match rate threshold to accept a solution immediately\n",
    "        cap_matchrate = input_params[\"cap_matchrate\"] * 100 ## any UB matrix providing MR less than this will be ignored\n",
    "        coeff = 0.30            ## coefficient to calculate the overlap of two solutions\n",
    "        coeff_overlap = input_params[\"coeff_overlap\"]    ##10% spots overlap is allowed with already indexed orientation\n",
    "        material0_limit = input_params[\"material_limit\"][0]  ## how many UB can be proposed for first material\n",
    "        material1_limit = input_params[\"material_limit\"][1] ## how many UB can be proposed for second material; this forces the orientation matrix deduction algorithm to find only a required materials matrix\n",
    "        material_phase_always_present = input_params[\"material_phase_always_present\"] ## in case if one phase is always present in a Laue pattern (useful for substrate cases)\n",
    "\n",
    "        ## Additional parameters to refine the orientation matrix construction process\n",
    "        use_om_user = str(input_params[\"use_om_user\"]).lower()\n",
    "        path_user_OM = input_params[\"path_user_OM\"]\n",
    "        nb_spots_consider = input_params[\"nb_spots_consider\"]\n",
    "        residues_threshold = input_params[\"residues_threshold\"]\n",
    "        nb_spots_global_threshold = input_params[\"nb_spots_global_threshold\"]\n",
    "        option_global = \"v2\"\n",
    "        additional_expression = [\"none\"] # for strain assumptions, like a==b for HCP\n",
    "\n",
    "        config_setting = configparser.ConfigParser()\n",
    "        filepath = resource_path('settings.ini')\n",
    "        print(\"Writing settings file in \" + filepath)\n",
    "        config_setting.read(filepath)\n",
    "        config_setting.set('CALLER', 'residues_threshold',str(residues_threshold))\n",
    "        config_setting.set('CALLER', 'nb_spots_global_threshold',str(nb_spots_global_threshold))\n",
    "        config_setting.set('CALLER', 'option_global',option_global)\n",
    "        config_setting.set('CALLER', 'use_om_user',use_om_user)\n",
    "        config_setting.set('CALLER', 'nb_spots_consider',str(nb_spots_consider))\n",
    "        config_setting.set('CALLER', 'path_user_OM',str(path_user_OM))\n",
    "        config_setting.set('CALLER', 'intensity', str(intensity_threshold))\n",
    "        config_setting.set('CALLER', 'boxsize', str(boxsize))\n",
    "        config_setting.set('CALLER', 'pixdev', str(FitPixelDev))\n",
    "        config_setting.set('CALLER', 'cap_softmax', str(softmax_threshold_global))\n",
    "        config_setting.set('CALLER', 'cap_mr', str(cap_matchrate/100.))\n",
    "        config_setting.set('CALLER', 'strain_free_parameters', \",\".join(strain_free_parameters))\n",
    "        config_setting.set('CALLER', 'additional_expression', \",\".join(additional_expression))\n",
    "        config_setting.set('CALLER', 'mode_peaksearch', str(mode_peaksearch))\n",
    "        with open(filepath, 'w') as configfile:\n",
    "            config_setting.write(configfile)\n",
    "\n",
    "        ## load model related files and generate the model\n",
    "        json_file = open(model_direc+\"//model_\"+prefix1+\".json\", 'r')\n",
    "        classhkl = np.load(model_direc+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_0\"]\n",
    "        angbins = np.load(model_direc+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_1\"]\n",
    "        ind_mat = np.load(model_direc+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_5\"]\n",
    "        ind_mat1 = np.load(model_direc+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_6\"]  \n",
    "        load_weights = model_direc + \"//model_\"+prefix1+\".h5\"\n",
    "        wb = read_hdf5(load_weights)\n",
    "        temp_key = list(wb.keys())\n",
    "\n",
    "        # # load json and create model\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        model = model_from_json(loaded_model_json)\n",
    "        print(\"Constructing model\")\n",
    "        model.load_weights(load_weights)\n",
    "        print(\"Uploading weights to model\")\n",
    "        print(\"All model files found and loaded\")\n",
    "\n",
    "        ct = time.time()\n",
    "        now = datetime.datetime.fromtimestamp(ct)\n",
    "        c_time = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "        hkl_all_class1 = None\n",
    "        with open(model_direc+\"//classhkl_data_nonpickled_\"+material_+\".pickle\", \"rb\") as input_file:\n",
    "            hkl_all_class0 = cPickle.load(input_file)[0]\n",
    "\n",
    "        if material_ != material1_:\n",
    "            with open(model_direc+\"//classhkl_data_nonpickled_\"+material1_+\".pickle\", \"rb\") as input_file:\n",
    "                hkl_all_class1 = cPickle.load(input_file)[0]\n",
    "\n",
    "        # ## Initialize variables and prepare arguments for multiprocessing module\n",
    "        col = [[] for i in range(int(ubmat))]\n",
    "        colx = [[] for i in range(int(ubmat))]\n",
    "        coly = [[] for i in range(int(ubmat))]\n",
    "        rotation_matrix = [[] for i in range(int(ubmat))]\n",
    "        strain_matrix = [[] for i in range(int(ubmat))]\n",
    "        strain_matrixs = [[] for i in range(int(ubmat))]\n",
    "        match_rate = [[] for i in range(int(ubmat))]\n",
    "        spots_len = [[] for i in range(int(ubmat))]\n",
    "        iR_pix = [[] for i in range(int(ubmat))]\n",
    "        fR_pix = [[] for i in range(int(ubmat))]\n",
    "        mat_global = [[] for i in range(int(ubmat))]\n",
    "        best_match = [[] for i in range(int(ubmat))]\n",
    "        spots1_global = [[] for i in range(int(ubmat))]\n",
    "        for i in range(int(ubmat)):\n",
    "            col[i].append(np.zeros((lim_x*lim_y,3)))\n",
    "            colx[i].append(np.zeros((lim_x*lim_y,3)))\n",
    "            coly[i].append(np.zeros((lim_x*lim_y,3)))\n",
    "            rotation_matrix[i].append(np.zeros((lim_x*lim_y,3,3)))\n",
    "            strain_matrix[i].append(np.zeros((lim_x*lim_y,3,3)))\n",
    "            strain_matrixs[i].append(np.zeros((lim_x*lim_y,3,3)))\n",
    "            match_rate[i].append(np.zeros((lim_x*lim_y,1)))\n",
    "            spots_len[i].append(np.zeros((lim_x*lim_y,1)))\n",
    "            iR_pix[i].append(np.zeros((lim_x*lim_y,1)))\n",
    "            fR_pix[i].append(np.zeros((lim_x*lim_y,1)))\n",
    "            mat_global[i].append(np.zeros((lim_x*lim_y,1)))\n",
    "            best_match[i].append([[] for jk in range(lim_x*lim_y)])\n",
    "            spots1_global[i].append([[] for jk in range(lim_x*lim_y)])\n",
    "\n",
    "        ##hack for multiprocessing, but very time consuming\n",
    "        if use_previous_UBmatrix_name:\n",
    "            np.savez_compressed(model_direc+'//rotation_matrix_indexed_1.npz', rotation_matrix, mat_global, match_rate, 0.0)\n",
    "\n",
    "        # =============================================================================\n",
    "        #         ## Multi-processing routine\n",
    "        # =============================================================================        \n",
    "        ## Number of files to generate\n",
    "        grid_files = np.zeros((lim_x,lim_y))\n",
    "        filenm = np.chararray((lim_x,lim_y), itemsize=1000)\n",
    "        grid_files = grid_files.ravel()\n",
    "        filenm = filenm.ravel()\n",
    "        count_global = lim_x * lim_y\n",
    "        list_of_files = glob.glob(filenameDirec+'//'+experimental_prefix+'*.'+format_file)\n",
    "        ## sort files\n",
    "        list_of_files.sort(key=lambda var:[int(x) if x.isdigit() else x for x in re.findall(r'[^0-9]|[0-9]+', var)])\n",
    "\n",
    "        if len(list_of_files) == count_global:\n",
    "            for ii in range(len(list_of_files)):\n",
    "                grid_files[ii] = ii\n",
    "                filenm[ii] = list_of_files[ii]     \n",
    "            print(\"expected \"+str(count_global)+\" files based on the XY grid (\"+str(lim_x)+\",\"+str(lim_y)+\") defined by user\")\n",
    "            print(\"and found \"+str(len(list_of_files))+\" files\")\n",
    "        else:\n",
    "            print(\"expected \"+str(count_global)+\" files based on the XY grid (\"+str(lim_x)+\",\"+str(lim_y)+\") defined by user\")\n",
    "            print(\"But found \"+str(len(list_of_files))+\" files (either all data is not written yet or maybe XY grid definition is not proper)\")\n",
    "            digits = len(str(count_global))\n",
    "            digits = max(digits,4)\n",
    "            # Temp fix\n",
    "            for ii in range(count_global):\n",
    "                text = str(ii)\n",
    "                if ii < 10000:\n",
    "                    string = text.zfill(4)\n",
    "                else:\n",
    "                    string = text.zfill(5)\n",
    "                file_name_temp = filenameDirec+'//'+experimental_prefix + string+'.'+format_file\n",
    "                ## store it in a grid \n",
    "                filenm[ii] = file_name_temp\n",
    "\n",
    "        check = np.zeros((count_global,int(ubmat)))\n",
    "        # =============================================================================\n",
    "        blacklist = None\n",
    "\n",
    "        ### Create a COR directory to be loaded in LaueTools\n",
    "        cor_file_directory = filenameDirec + \"//\" + experimental_prefix+\"CORfiles\"\n",
    "        if list_of_files[0].split(\".\")[-1] in ['cor',\"COR\",\"Cor\"]:\n",
    "            cor_file_directory = filenameDirec \n",
    "        if not os.path.exists(cor_file_directory):\n",
    "            os.makedirs(cor_file_directory)\n",
    "\n",
    "        try_prevs = False\n",
    "        files_treated = []\n",
    "\n",
    "        ##making a big argument list for each CPU\n",
    "        valu12 = [[filenm[ii].decode(), ii,\n",
    "                   rotation_matrix,\n",
    "                    strain_matrix,\n",
    "                    strain_matrixs,\n",
    "                    col,\n",
    "                    colx,\n",
    "                    coly,\n",
    "                    match_rate,\n",
    "                    spots_len, \n",
    "                    iR_pix, \n",
    "                    fR_pix,\n",
    "                    best_match,\n",
    "                    mat_global,\n",
    "                    check,\n",
    "                    detectorparameters,\n",
    "                    pixelsize,\n",
    "                    angbins,\n",
    "                    classhkl,\n",
    "                    hkl_all_class0,\n",
    "                    hkl_all_class1,\n",
    "                    emin,\n",
    "                    emax,\n",
    "                    material_,\n",
    "                    material1_,\n",
    "                    symmetry,\n",
    "                    symmetry1,   \n",
    "                    lim_x,\n",
    "                    lim_y,\n",
    "                    strain_calculation, \n",
    "                    ind_mat, ind_mat1,\n",
    "                    model_direc, float(tolerance),\n",
    "                    float(tolerance1),\n",
    "                    int(ubmat), ccd_label_global, \n",
    "                    None,\n",
    "                    float(intensity_threshold),\n",
    "                    int(boxsize),bkg_treatment,\n",
    "                    filenameDirec, \n",
    "                    experimental_prefix,\n",
    "                    blacklist,\n",
    "                    None,\n",
    "                    files_treated,\n",
    "                    try_prevs, ## try previous is kept true, incase if its stuck in loop\n",
    "                    wb,\n",
    "                    temp_key,\n",
    "                    cor_file_directory,\n",
    "                    mode_spotCycle,\n",
    "                    softmax_threshold_global,\n",
    "                    mr_threshold_global,\n",
    "                    cap_matchrate,\n",
    "                    tolerance_strain,\n",
    "                    tolerance_strain1,\n",
    "                    NumberMaxofFits,\n",
    "                    fit_peaks_gaussian,\n",
    "                    FitPixelDev,\n",
    "                    coeff,\n",
    "                    coeff_overlap,\n",
    "                    material0_limit,\n",
    "                    material1_limit,\n",
    "                    use_previous_UBmatrix_name,\n",
    "                    material_phase_always_present,\n",
    "                    crystal,\n",
    "                    crystal1,\n",
    "                    strain_free_parameters] for ii in range(count_global)]\n",
    "\n",
    "        ## test singel file prediction\n",
    "        # results = new_MP_function(valu12[0])\n",
    "        # best = results[-2][0][0][0]\n",
    "\n",
    "        args = zip(valu12)\n",
    "        with multiprocessing.Pool(ncpu) as pool:\n",
    "            results = pool.starmap(new_MP_function, tqdm(args, total=len(valu12)), chunksize=1)\n",
    "\n",
    "            for r_message_mpdata in results:\n",
    "                strain_matrix_mpdata, strain_matrixs_mpdata, rotation_matrix_mpdata, col_mpdata,\\\n",
    "                colx_mpdata, coly_mpdata, match_rate_mpdata, mat_global_mpdata,\\\n",
    "                    cnt_mpdata, meta_mpdata, files_treated_mpdata, spots_len_mpdata, \\\n",
    "                        iR_pixel_mpdata, fR_pixel_mpdata, best_match_mpdata, check_mpdata = r_message_mpdata\n",
    "\n",
    "                for i_mpdata in files_treated_mpdata:\n",
    "                    files_treated.append(i_mpdata)\n",
    "\n",
    "                for intmat_mpdata in range(int(ubmat)):\n",
    "                    check[cnt_mpdata,intmat_mpdata] = check_mpdata[cnt_mpdata,intmat_mpdata]\n",
    "                    mat_global[intmat_mpdata][0][cnt_mpdata] = mat_global_mpdata[intmat_mpdata][0][cnt_mpdata]\n",
    "                    strain_matrix[intmat_mpdata][0][cnt_mpdata,:,:] = strain_matrix_mpdata[intmat_mpdata][0][cnt_mpdata,:,:]\n",
    "                    strain_matrixs[intmat_mpdata][0][cnt_mpdata,:,:] = strain_matrixs_mpdata[intmat_mpdata][0][cnt_mpdata,:,:]\n",
    "                    rotation_matrix[intmat_mpdata][0][cnt_mpdata,:,:] = rotation_matrix_mpdata[intmat_mpdata][0][cnt_mpdata,:,:]\n",
    "                    col[intmat_mpdata][0][cnt_mpdata,:] = col_mpdata[intmat_mpdata][0][cnt_mpdata,:]\n",
    "                    colx[intmat_mpdata][0][cnt_mpdata,:] = colx_mpdata[intmat_mpdata][0][cnt_mpdata,:]\n",
    "                    coly[intmat_mpdata][0][cnt_mpdata,:] = coly_mpdata[intmat_mpdata][0][cnt_mpdata,:]\n",
    "                    match_rate[intmat_mpdata][0][cnt_mpdata] = match_rate_mpdata[intmat_mpdata][0][cnt_mpdata]\n",
    "                    spots_len[intmat_mpdata][0][cnt_mpdata] = spots_len_mpdata[intmat_mpdata][0][cnt_mpdata]\n",
    "                    iR_pix[intmat_mpdata][0][cnt_mpdata] = iR_pixel_mpdata[intmat_mpdata][0][cnt_mpdata]\n",
    "                    fR_pix[intmat_mpdata][0][cnt_mpdata] = fR_pixel_mpdata[intmat_mpdata][0][cnt_mpdata]\n",
    "                    best_match[intmat_mpdata][0][cnt_mpdata] = best_match_mpdata[intmat_mpdata][0][cnt_mpdata]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ff4a3c-6640-4c56-b9a1-38450a31f742",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcd6525c-c367-42be-864e-ded43865f9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data saved in  /home/esrf/purushot/Desktop/LaueNN_tutorial/TUtorial_1/LaueNN_script/GaN_Si_2phase//results__2phase_2022-11-15_12-51-41\n",
      "Number of Phases present (includes non indexed phase zero also) 3\n",
      "building KD tree...\n",
      "prediction statistics are generated\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1169x827 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1169x827 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1169x827 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1169x827 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1169x827 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1169x827 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lauetoolsnn.utils_lauenn import  global_plots, write_average_orientation, convert_pickle_to_hdf5, write_prediction_stats, write_MTEXdata\n",
    "\n",
    "### Save files and results\n",
    "save_directory_ = model_direc+\"//results_\"+input_params[\"prefix\"]+\"_\"+c_time\n",
    "if not os.path.exists(save_directory_):\n",
    "    os.makedirs(save_directory_)\n",
    "\n",
    "np.savez_compressed(save_directory_+ \"//results.npz\", \n",
    "                    best_match, mat_global, rotation_matrix, strain_matrix, \n",
    "                    strain_matrixs, col, colx, coly, match_rate, files_treated,\n",
    "                    lim_x, lim_y, spots_len, iR_pix, fR_pix,\n",
    "                    material_, material1_)\n",
    "## intermediate saving of pickle objects with results\n",
    "with open(save_directory_+ \"//results.pickle\", \"wb\") as output_file:\n",
    "        cPickle.dump([best_match, mat_global, rotation_matrix, strain_matrix, \n",
    "                      strain_matrixs, col, colx, coly, match_rate, files_treated,\n",
    "                      lim_x, lim_y, spots_len, iR_pix, fR_pix,\n",
    "                      material_, material1_, lattice_material, lattice_material1,\n",
    "                      symmetry, symmetry1, crystal, crystal1], output_file)\n",
    "print(\"data saved in \", save_directory_)\n",
    "\n",
    "## Lets save also a set of average UB matrix in text file to be used with user_OM setting    \n",
    "try:\n",
    "    write_average_orientation(save_directory_, mat_global, rotation_matrix,\n",
    "                                  match_rate, lim_x, lim_y, crystal, crystal1,\n",
    "                                  radius=10, grain_ang=5, pixel_grain_definition=3)\n",
    "except:\n",
    "    print(\"Error with Average orientation and grain index calculation\")\n",
    "\n",
    "try:\n",
    "    convert_pickle_to_hdf5(save_directory_, files_treated, rotation_matrix, strain_matrix, \n",
    "                           strain_matrixs, match_rate, spots_len, iR_pix, \n",
    "                           fR_pix, colx, coly, col, mat_global,\n",
    "                           material_, material1_, lim_x, lim_y)\n",
    "except:\n",
    "    print(\"Error writting H5 file\")\n",
    "\n",
    "try:\n",
    "    write_prediction_stats(save_directory_, material_, material1_, files_treated,\\\n",
    "                            lim_x, lim_y, best_match, strain_matrixs, strain_matrix, iR_pix,\\\n",
    "                            fR_pix,  mat_global)\n",
    "except:\n",
    "    print(\"Error writting prediction statistics file\")\n",
    "\n",
    "try:\n",
    "    write_MTEXdata(save_directory_, material_, material1_, rotation_matrix,\\\n",
    "                       lattice_material, lattice_material1, lim_x, lim_y, mat_global,\\\n",
    "                        input_params[\"symmetry\"][0], input_params[\"symmetry\"][1])\n",
    "except:\n",
    "    print(\"Error writting MTEX orientation file\")\n",
    "\n",
    "try:\n",
    "    global_plots(lim_x, lim_y, rotation_matrix, strain_matrix, strain_matrixs, \n",
    "                  col, colx, coly, match_rate, mat_global, spots_len, \n",
    "                  iR_pix, fR_pix, save_directory_, material_, material1_,\n",
    "                  match_rate_threshold=5, bins=30)\n",
    "except:\n",
    "    print(\"Error in the global plots module\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cd4ce5-aaa7-410e-8e54-8559ad1c1ec1",
   "metadata": {},
   "source": [
    "## Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c685496-ee6a-4883-ad1d-d037091651f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/x-hdf5": "/home/esrf/purushot/Desktop/LaueNN_tutorial/LaueNN_script/GaN_Si_2phase/results__2phase_2022-11-14_23-41-25/grain_all.h5",
      "text/plain": [
       "<jupyterlab_h5web.widget.H5Web object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jupyterlab_h5web import H5Web\n",
    "H5Web(os.path.join(save_directory_,\"grain_all.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed3a6e5-cf6c-415c-941c-df56776b6669",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lauenn",
   "language": "python",
   "name": "lauenn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
