{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b90dda43",
   "metadata": {},
   "source": [
    "# Notebook script for generation of training dataset (supports single and two phase material)\n",
    "\n",
    "## For case of more than two phase, please use the LaueNN_pyScript_3_or_more_phase script\n",
    "\n",
    "## Different steps of data processing is outlined in this notebook (LaueToolsNN GUI does the same thing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c1aea3-9a35-4dae-89bd-240d75d852e6",
   "metadata": {},
   "source": [
    "## Define material and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "879b9b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaueNN path is /home/esrf/purushot/anaconda3/envs/lauenn/lib/python3.7/site-packages/lauetoolsnn\n",
      "['GaN', [3.189, 3.189, 5.185, 90, 90, 120], '191']\n",
      "['Si', [5.4309, 5.4309, 5.4309, 90, 90, 90], '227']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':     #enclosing required because of multiprocessing\n",
    "\n",
    "    ## Import modules used for this Notebook\n",
    "    import os\n",
    "    import json\n",
    "    \n",
    "    ## Get the path of the lauetoolsnn library\n",
    "    import lauetoolsnn\n",
    "    laueNN_path = os.path.dirname(lauetoolsnn.__file__)\n",
    "    print(\"LaueNN path is\", laueNN_path)\n",
    "    \n",
    "    ## Load the json of material and extinctions\n",
    "    with open(os.path.join(laueNN_path, 'lauetools','material.json'),'r') as f:\n",
    "        dict_Materials = json.load(f)\n",
    "    with open(os.path.join(laueNN_path, 'lauetools','extinction.json'),'r') as f:\n",
    "        extinction_json = json.load(f)\n",
    "\n",
    "    ## Modify the dictionary values to add new entries\n",
    "    dict_Materials[\"GaN\"] = [\"GaN\", [3.189, 3.189, 5.185, 90, 90, 120], \"191\"]\n",
    "    dict_Materials[\"Si\"] = [\"Si\", [5.4309, 5.4309, 5.4309, 90, 90, 90], \"227\"]\n",
    "\n",
    "    extinction_json[\"191\"] = \"191\"\n",
    "    extinction_json[\"227\"] = \"227\"\n",
    "\n",
    "    ## verify if extinction is present in CrystalParameters.py file of lauetools (Manually done for now)\n",
    "\n",
    "    ## dump the json back with new values\n",
    "    with open(os.path.join(laueNN_path, 'lauetools','material.json'), 'w') as fp:\n",
    "        json.dump(dict_Materials, fp)\n",
    "    with open(os.path.join(laueNN_path, 'lauetools','extinction.json'), 'w') as fp:\n",
    "        json.dump(extinction_json, fp)\n",
    "\n",
    "    ## Verify if the material is added to the library or not;\n",
    "    from lauetoolsnn.lauetools.dict_LaueTools import dict_Materials\n",
    "    ## if not, restart the console\n",
    "    print(dict_Materials[\"GaN\"])\n",
    "    print(dict_Materials[\"Si\"])\n",
    "    \n",
    "    # =============================================================================\n",
    "    # Step 0: Define the dictionary with all parameters \n",
    "    # =============================================================================\n",
    "    ## User Input dictionary with parameters\n",
    "    ## In case of only one phase/material, keep same value for material_ and material1_ key\n",
    "    input_params = {\n",
    "                    \"global_path\" : os.getcwd(),\n",
    "                    \"prefix\" : \"_2phase\",                 ## prefix for the folder to be created for training dataset\n",
    "\n",
    "                    \"material_\": [\"GaN\", \"Si\"],             ## same key as used in dict_LaueTools\n",
    "                    \"symmetry\": [\"hexagonal\", \"cubic\"],           ## crystal symmetry of material_\n",
    "                    \"SG\": [191, 227], #186                    ## Space group of material_ (None if not known)\n",
    "                    \"hkl_max_identify\" : [6,5],        ## Maximum hkl index to classify in a Laue pattern\n",
    "                    \"nb_grains_per_lp_mat\" : [2,1],        ## max grains to be generated in a Laue Image\n",
    "\n",
    "                    ## hkl_max_identify : can be \"auto\" or integer: Maximum index of HKL to build output classes\n",
    "                    \n",
    "                    # =============================================================================\n",
    "                    # ## Data generation settings\n",
    "                    # =============================================================================\n",
    "                    \"grains_nb_simulate\" : 500,    ## Number of orientations to generate (takes advantage of crystal symmetry)\n",
    "                    \"classes_with_frequency_to_remove\": [100,100], ## classes_with_frequency_to_remove: HKL class with less appearance than \n",
    "                                                                            ##  specified will be ignored in output\n",
    "                    \"desired_classes_output\": [\"all\",\"all\"], ## desired_classes_output : can be all or an integer: to limit the number of output classes\n",
    "\n",
    "                    \"include_small_misorientation\": False, ## to include additional data with small angle misorientation\n",
    "                    \"misorientation\": 5, ##only used if \"include_small_misorientation\" is True\n",
    "                    \"maximum_angle_to_search\":90, ## Angle of radial distribution to reconstruct the histogram (in deg)\n",
    "                    \"step_for_binning\" : 0.1,      ## bin widht of angular radial distribution in degree\n",
    "                    \n",
    "                    # =============================================================================\n",
    "                    #  ## Training parameters\n",
    "                    # =============================================================================\n",
    "                    \"orientation_generation\": \"uniform\", ## can be random or uniform\n",
    "                    \"batch_size\":100,               ## batches of files to use while training\n",
    "                    \"epochs\":8,                    ## number of epochs for training\n",
    "\n",
    "                    # =============================================================================\n",
    "                    # ## Detector parameters of the Experimental setup\n",
    "                    # =============================================================================\n",
    "                    ## Sample-detector distance, X center, Y center, two detector angles\n",
    "                    \"detectorparameters\" :  [79.61200, 977.8100, 932.1700, 0.4770000, 0.4470000],\n",
    "                    \"pixelsize\" : 0.0734,          ## Detector pixel size\n",
    "                    \"dim1\":2018,                   ## Dimensions of detector in pixels\n",
    "                    \"dim2\":2016,\n",
    "                    \"emin\" : 5,                    ## Minimum and maximum energy to use for simulating Laue Patterns\n",
    "                    \"emax\" : 22,\n",
    "                    \"ccd_label\" : \"sCMOS\",\n",
    "                    \n",
    "                    # =============================================================================\n",
    "                    # ## Prediction parameters\n",
    "                    # =============================================================================\n",
    "                    \"experimental_directory\": os.getcwd(),\n",
    "                    \"experimental_prefix\": r\"nw1_\",\n",
    "                    \"grid_size_x\" : 1,            ## Grid X and Y limit to generate the simulated dataset (a rectangular scan region)\n",
    "                    \"grid_size_y\" : 2,\n",
    "                    \n",
    "                    # =============================================================================\n",
    "                    # ## Prediction Settings\n",
    "                    # =============================================================================\n",
    "                    # model_weight_file: if none, it will select by default the latest H5 weight file, else provide a specific model\n",
    "                    # softmax_threshold_global: thresholding to limit the predicted spots search zone\n",
    "                    # cap_matchrate: any UB matrix providing MR less than this will be ignored\n",
    "                    # coeff: should be same as cap_matchrate or no? (this is for try previous UB matrix)\n",
    "                    # coeff_overlap: coefficient to limit the overlapping between spots; if more than this, new solution will be computed\n",
    "                    # mode_spotCycle: How to cycle through predicted spots (slow or graphmode )\n",
    "                    \"UB_matrix_to_detect\" : 3,\n",
    "                    \"matrix_tolerance\" : [0.6, 0.6],\n",
    "                    \"material_limit\" : [2, 1],\n",
    "                    \"material_phase_always_present\" : 2,\n",
    "                    \"softmax_threshold_global\" : 0.85,\n",
    "                    \"cap_matchrate\" : 0.10,\n",
    "                    \"coeff_overlap\" : 0.3,\n",
    "                    \"mode_spotCycle\" : \"graphmode\",\n",
    "                    ##true for few crystal and prefered texture case, otherwise time consuming; advised for single phase alone\n",
    "                    \"use_previous\" : False,\n",
    "                    \n",
    "                    # =============================================================================\n",
    "                    # # [PEAKSEARCH]\n",
    "                    # =============================================================================\n",
    "                    \"intensity_threshold\" : 2,## for skimage this is of image standard deviation\n",
    "                    \"boxsize\" : 10,## for skimage this is box size to fit\n",
    "                    \"fit_peaks_gaussian\" : 1,## for skimage this is of no sense\n",
    "                    \"FitPixelDev\" : 15, ## for skimage this is distance between peaks to avoid\n",
    "                    \"NumberMaxofFits\" : 3000,## for skimage this is maximum leastquare attempts before giving up\n",
    "                    \"mode\": \"skimage\",\n",
    "\n",
    "                    # =============================================================================\n",
    "                    # # [STRAINCALCULATION]\n",
    "                    # =============================================================================\n",
    "                    \"strain_compute\" : True,\n",
    "                    \"tolerance_strain_refinement\" : [[0.6,0.5,0.4,0.3,0.2,0.1],\n",
    "                                                     [0.6,0.5,0.4,0.3,0.2,0.1]],\n",
    "                    \"free_parameters\" : [\"b\",\"c\",\"alpha\",\"beta\",\"gamma\"],\n",
    "                    \n",
    "                    # =============================================================================\n",
    "                    # # [Additional settings]\n",
    "                    # =============================================================================\n",
    "                    \"residues_threshold\":0.25,\n",
    "                    \"nb_spots_global_threshold\":8,\n",
    "                    \"nb_spots_consider\" : 500,\n",
    "                    # User defined orientation matrix supplied in a file\n",
    "                    \"use_om_user\" : False,\n",
    "                    \"path_user_OM\" : \"\",\n",
    "                    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad431ffc",
   "metadata": {},
   "source": [
    "## Write config file with parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac3284dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_config_GUI = False\n",
    "if __name__ == '__main__':     #enclosing required because of multiprocessing\n",
    "    # global_path: path where all model related files will be saved\n",
    "    global_path = input_params[\"global_path\"]\n",
    "    \n",
    "    ## verify the length of material key\n",
    "    if len(input_params[\"material_\"]) > 2:\n",
    "        print(\"This script uses modules of LaueNN that only supports maximum of two materials; please use multi-material script incase of more than 2phases\")\n",
    "        print(\"The script will run, however, will only use the first 2 materials\")\n",
    "    \n",
    "    if len(input_params[\"material_\"]) == 1:\n",
    "        print(\"only one material is defined\")\n",
    "        ## modify the dictionary for two phase\n",
    "        input_params[\"material_\"].append(input_params[\"material_\"][0])\n",
    "        input_params[\"symmetry\"].append(input_params[\"symmetry\"][0])\n",
    "        input_params[\"SG\"].append(input_params[\"SG\"][0])\n",
    "        input_params[\"hkl_max_identify\"].append(input_params[\"hkl_max_identify\"][0])\n",
    "        input_params[\"nb_grains_per_lp_mat\"].append(input_params[\"nb_grains_per_lp_mat\"][0])\n",
    "        \n",
    "        input_params[\"classes_with_frequency_to_remove\"].append(input_params[\"classes_with_frequency_to_remove\"][0])\n",
    "        input_params[\"desired_classes_output\"].append(input_params[\"desired_classes_output\"][0])\n",
    "        input_params[\"matrix_tolerance\"].append(input_params[\"matrix_tolerance\"][0])\n",
    "        input_params[\"material_limit\"].append(input_params[\"material_limit\"][0])\n",
    "        input_params[\"tolerance_strain_refinement\"].append(input_params[\"tolerance_strain_refinement\"][0])\n",
    "        \n",
    "    if write_config_GUI:\n",
    "        ## write config file for GUI \n",
    "        if input_params[\"material_\"][0] != input_params[\"material_\"][1]:\n",
    "            text_file = open(os.path.join(global_path,input_params[\"material_\"][0]+\"_\"+input_params[\"material_\"][1]+input_params[\"prefix\"]+\".lauenn\"), \"w\")\n",
    "        else:\n",
    "            text_file = open(os.path.join(global_path,input_params[\"material_\"][0]+\"_\"+input_params[\"prefix\"]+\".lauenn\"), \"w\")\n",
    "\n",
    "        text_file.write(\"### config file for LaueNeuralNetwork GUI \\n\")\n",
    "        text_file.write(\"[GLOBAL_DIRECTORY]\\n\")\n",
    "        text_file.write(\"prefix = \"+input_params[\"prefix\"]+\" \\n\")\n",
    "        text_file.write(\"main_directory = \"+global_path+\"\\n\")\n",
    "        text_file.write(\"\\n\")\n",
    "        text_file.write(\"[MATERIAL]\\n\")\n",
    "        text_file.write(\"material = \"+input_params[\"material_\"][0]+\"\\n\")\n",
    "        text_file.write(\"symmetry = \"+input_params[\"symmetry\"][0]+\"\\n\")\n",
    "        text_file.write(\"space_group = \"+str(input_params[\"SG\"][0])+\"\\n\")\n",
    "        text_file.write(\"general_diffraction_rules = false\\n\")\n",
    "        text_file.write(\"\\n\")\n",
    "        text_file.write(\"material1 = \"+input_params[\"material_\"][1]+\"\\n\")\n",
    "        text_file.write(\"symmetry1 = \"+input_params[\"symmetry\"][1]+\"\\n\")\n",
    "        text_file.write(\"space_group1 = \"+str(input_params[\"SG\"][1])+\"\\n\")\n",
    "        text_file.write(\"general_diffraction_rules1 = false\\n\")\n",
    "        text_file.write(\"\\n\")\n",
    "        text_file.write(\"[DETECTOR]\\n\")\n",
    "        text_file.write(\"detectorfile = user_input\"+\" \\n\")\n",
    "        text_file.write(\"params =\"+ \",\".join(str(param) for param in input_params[\"detectorparameters\"])+\",\"+str(input_params[\"pixelsize\"])+\",\"+str(input_params[\"dim1\"])+\",\"+str(input_params[\"dim2\"])+\",\"+str(input_params[\"ccd_label\"])+\" \\n\")\n",
    "        text_file.write(\"emax = \"+str(input_params[\"emax\"])+\"\\n\")\n",
    "        text_file.write(\"emin = \"+str(input_params[\"emin\"])+\"\\n\")\n",
    "        text_file.write(\"\\n\")\n",
    "        text_file.write(\"[TRAINING]\\n\")\n",
    "        text_file.write(\"classes_with_frequency_to_remove = \"+str(input_params[\"classes_with_frequency_to_remove\"][0])+\"\\n\")\n",
    "        text_file.write(\"desired_classes_output = \"+str(input_params[\"desired_classes_output\"][0])+\"\\n\")\n",
    "        text_file.write(\"max_HKL_index = \"+str(input_params[\"hkl_max_identify\"][0])+\"\\n\")\n",
    "        text_file.write(\"max_nb_grains = \"+str(input_params[\"nb_grains_per_lp_mat\"][0])+\"\\n\")\n",
    "        text_file.write(\"classes_with_frequency_to_remove1 = \"+str(input_params[\"classes_with_frequency_to_remove\"][1])+\"\\n\")\n",
    "        text_file.write(\"desired_classes_output1 = \"+str(input_params[\"desired_classes_output\"][1])+\"\\n\")\n",
    "        text_file.write(\"max_HKL_index1 = \"+str(input_params[\"hkl_max_identify\"][1])+\"\\n\")\n",
    "        text_file.write(\"max_nb_grains1 = \"+str(input_params[\"nb_grains_per_lp_mat\"][1])+\"\\n\")\n",
    "        text_file.write(\"max_simulations = \"+str(input_params[\"grains_nb_simulate\"])+\"\\n\")\n",
    "        text_file.write(\"include_small_misorientation = \"+str(input_params[\"include_small_misorientation\"]).lower()+\"\\n\")\n",
    "        text_file.write(\"misorientation_angle = 1 \\n\")\n",
    "        text_file.write(\"angular_distance = \"+str(input_params[\"maximum_angle_to_search\"])+\"\\n\")\n",
    "        text_file.write(\"step_size = \"+str(input_params[\"step_for_binning\"])+\"\\n\")\n",
    "        text_file.write(\"batch_size = \"+str(input_params[\"batch_size\"])+\"\\n\")\n",
    "        text_file.write(\"epochs = \"+str(input_params[\"epochs\"])+\"\\n\")\n",
    "        text_file.write(\"\\n\")\n",
    "        text_file.write(\"[PREDICTION]\\n\")\n",
    "        text_file.write(\"UB_matrix_to_detect = \"+str(input_params[\"UB_matrix_to_detect\"])+\"\\n\")\n",
    "        text_file.write(\"matrix_tolerance = \"+str(input_params[\"matrix_tolerance\"][0])+\"\\n\")\n",
    "        text_file.write(\"matrix_tolerance1 = \"+str(input_params[\"matrix_tolerance\"][1])+\"\\n\")\n",
    "        text_file.write(\"material0_limit = \"+str(input_params[\"material_limit\"][0])+\"\\n\")\n",
    "        text_file.write(\"material1_limit = \"+str(input_params[\"material_limit\"][1])+\"\\n\")\n",
    "        text_file.write(\"softmax_threshold_global = \"+str(input_params[\"softmax_threshold_global\"])+\"\\n\")\n",
    "        text_file.write(\"cap_matchrate = \"+str(input_params[\"cap_matchrate\"])+\"\\n\")\n",
    "        text_file.write(\"coeff = 0.3\\n\")\n",
    "        text_file.write(\"coeff_overlap = \"+str(input_params[\"coeff_overlap\"])+\"\\n\")\n",
    "        text_file.write(\"mode_spotCycle = \"+str(input_params[\"mode_spotCycle\"])+\"\\n\")\n",
    "        text_file.write(\"use_previous = \"+str(input_params[\"use_previous\"]).lower()+\"\\n\")\n",
    "        text_file.write(\"\\n\")\n",
    "        text_file.write(\"[EXPERIMENT]\\n\")\n",
    "        text_file.write(\"experiment_directory = \"+input_params[\"experimental_directory\"]+\"\\n\")\n",
    "        text_file.write(\"experiment_file_prefix = \"+input_params[\"experimental_prefix\"]+\"\\n\")\n",
    "        text_file.write(\"image_grid_x = \"+str(input_params[\"grid_size_x\"])+\"\\n\")\n",
    "        text_file.write(\"image_grid_y = \"+str(input_params[\"grid_size_y\"])+\"\\n\")\n",
    "        text_file.write(\"\\n\")\n",
    "        text_file.write(\"[PEAKSEARCH]\\n\")\n",
    "        text_file.write(\"intensity_threshold = \"+str(input_params[\"intensity_threshold\"])+\"\\n\")\n",
    "        text_file.write(\"boxsize = \"+str(input_params[\"boxsize\"])+\"\\n\")\n",
    "        text_file.write(\"fit_peaks_gaussian = \"+str(input_params[\"fit_peaks_gaussian\"])+\"\\n\")\n",
    "        text_file.write(\"FitPixelDev = \"+str(input_params[\"FitPixelDev\"])+\"\\n\")\n",
    "        text_file.write(\"NumberMaxofFits = \"+str(input_params[\"NumberMaxofFits\"])+\"\\n\")\n",
    "        text_file.write(\"mode = \"+str(input_params[\"mode\"])+\"\\n\")\n",
    "        text_file.write(\"\\n\")\n",
    "        text_file.write(\"[STRAINCALCULATION]\\n\")\n",
    "        text_file.write(\"strain_compute = \"+str(input_params[\"strain_compute\"]).lower()+\"\\n\")\n",
    "        text_file.write(\"tolerance_strain_refinement =\"+ \",\".join(str(param) for param in input_params[\"tolerance_strain_refinement\"][0])+\"\\n\")\n",
    "        text_file.write(\"tolerance_strain_refinement1 =\"+ \",\".join(str(param) for param in input_params[\"tolerance_strain_refinement\"][1])+\"\\n\")\n",
    "        text_file.write(\"free_parameters =\"+ \",\".join(str(param) for param in input_params[\"free_parameters\"])+\"\\n\")\n",
    "        text_file.write(\"\\n\")\n",
    "        text_file.write(\"[DEVELOPMENT]\\n\")\n",
    "        text_file.write(\"material_phase_always_present=\"+str(input_params[\"material_phase_always_present\"])+\"\\n\")\n",
    "        text_file.write(\"write_MTEX_file= true \\n\")\n",
    "        text_file.write(\"[CALLER]\\n\")\n",
    "        text_file.write(\"residues_threshold=\"+str(input_params[\"residues_threshold\"])+\"\\n\")\n",
    "        text_file.write(\"nb_spots_global_threshold=\"+str(input_params[\"nb_spots_global_threshold\"])+\"\\n\")\n",
    "        text_file.write(\"option_global = v2 \\n\")\n",
    "        text_file.write(\"nb_spots_consider = \"+str(input_params[\"nb_spots_consider\"])+\"\\n\")\n",
    "        text_file.write(\"use_om_user = \"+str(input_params[\"use_om_user\"]).lower()+\"\\n\")\n",
    "        text_file.write(\"path_user_OM = \"+str(input_params[\"path_user_OM\"])+\"\\n\")\n",
    "        text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fe8cea-4e05-4c45-9fe6-3ae1356c6f7c",
   "metadata": {},
   "source": [
    "## Generate Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86f67cfd-1825-45ae-b328-4a5e36458441",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_dataset = False\n",
    "if __name__ == '__main__':     #enclosing required because of multiprocessing\n",
    "   \n",
    "    if generate_dataset:\n",
    "        # =============================================================================\n",
    "        # Step 1 \n",
    "        # =============================================================================\n",
    "        ## if LaueToolsNN is properly installed\n",
    "        from lauetoolsnn.utils_lauenn import get_material_detail, generate_classHKL, generate_dataset, rmv_freq_class\n",
    "        '''\n",
    "        get_material_detail : Extract material parameters\n",
    "        generate_classHKL : Generates List of output HKL for the neural network\n",
    "        generate_dataset : Will simulate laue patterns and will build histogram for the hkl generated by classHKL\n",
    "        rmv_freq_class : cleans the output class dataset\n",
    "        '''\n",
    "        # ## Get material parameters \n",
    "        # ### Generates a folder with material name and gets material unit cell parameters \n",
    "        # ### and symmetry object from the get_material_detail function\n",
    "        material_= input_params[\"material_\"][0]\n",
    "        material1_= input_params[\"material_\"][1]\n",
    "        n = input_params[\"hkl_max_identify\"][0]\n",
    "        n1 = input_params[\"hkl_max_identify\"][1]\n",
    "        maximum_angle_to_search = input_params[\"maximum_angle_to_search\"]\n",
    "        step_for_binning = input_params[\"step_for_binning\"]\n",
    "        nb_grains_per_lp0 = input_params[\"nb_grains_per_lp_mat\"][0]\n",
    "        nb_grains_per_lp1 = input_params[\"nb_grains_per_lp_mat\"][1]\n",
    "        grains_nb_simulate = input_params[\"grains_nb_simulate\"]\n",
    "        detectorparameters = input_params[\"detectorparameters\"]\n",
    "        pixelsize = input_params[\"pixelsize\"]\n",
    "        emax = input_params[\"emax\"]\n",
    "        emin = input_params[\"emin\"]\n",
    "        symm_ = input_params[\"symmetry\"][0]\n",
    "        symm1_ = input_params[\"symmetry\"][1]\n",
    "        SG = input_params[\"SG\"][0]\n",
    "        SG1 = input_params[\"SG\"][1]\n",
    "        \n",
    "        ## read hkl information from a fit file in case too large HKLs\n",
    "        manual_hkl_list=False\n",
    "        if manual_hkl_list:\n",
    "            import numpy as np\n",
    "            temp = np.loadtxt(r\"img_0000_LT_1.fit\")\n",
    "            hkl_array = temp[:,2:5]\n",
    "            hkl_array1 = None\n",
    "        else:\n",
    "            hkl_array = None\n",
    "            hkl_array1 = None\n",
    "            \n",
    "        if material_ != material1_:\n",
    "            save_directory = os.path.join(global_path,material_+\"_\"+material1_+input_params[\"prefix\"])\n",
    "        else:\n",
    "            save_directory = os.path.join(global_path,material_+input_params[\"prefix\"])\n",
    "        print(\"save directory is : \"+save_directory)\n",
    "        if not os.path.exists(save_directory):\n",
    "            os.makedirs(save_directory)\n",
    "        \n",
    "        ## get unit cell parameters and other details required for simulating Laue patterns\n",
    "        rules, symmetry, lattice_material,\\\n",
    "            crystal, SG, rules1, symmetry1,\\\n",
    "            lattice_material1, crystal1, SG1 = get_material_detail(material_, SG, symm_,\n",
    "                                                                   material1_, SG1, symm1_)\n",
    "        \n",
    "        # ## Generate Neural network output classes (Laue spot hkls) using the generate_classHKL function\n",
    "        ## procedure for generation of GROUND TRUTH classes\n",
    "        # general_diff_cond = True will eliminate the hkl index that does not \n",
    "        # satisfy the general reflection conditions, otherwise they will be eliminated in the next stage\n",
    "        generate_classHKL(n, rules, lattice_material, symmetry, material_, \n",
    "                          crystal=crystal, SG=SG, general_diff_cond=False,\n",
    "                          save_directory=save_directory, write_to_console=print, \n",
    "                          ang_maxx = maximum_angle_to_search, \n",
    "                          step = step_for_binning, mat_listHKl=hkl_array)\n",
    "        \n",
    "        if material_ != material1_:\n",
    "            generate_classHKL(n1, rules1, lattice_material1, symmetry1, material1_, \n",
    "                              crystal=crystal1, SG=SG1, general_diff_cond=False,\n",
    "                              save_directory=save_directory, write_to_console=print, \n",
    "                              ang_maxx = maximum_angle_to_search, \n",
    "                              step = step_for_binning, mat_listHKl=hkl_array1)\n",
    "        \n",
    "        \n",
    "        # ## Generate Training and Testing dataset only for the output classes (Laue spot hkls) calculated in the Step 3\n",
    "        # ### Uses multiprocessing library\n",
    "        ############ GENERATING TRAINING DATA ##############\n",
    "        # data_realism =True ; will introduce noise and partial Laue patterns in the training dataset\n",
    "        # modelp can have either \"random\" for random orientation generation or \n",
    "        # \"uniform\" for uniform orientation generation\n",
    "        # include_scm (if True; misorientation_angle parameter need to be defined): \n",
    "        #this parameter introduces misoriented crystal of specific angle along a crystal axis in the training dataset\n",
    "        generate_dataset(material_=material_, material1_=material1_, ang_maxx=maximum_angle_to_search,\n",
    "                         step=step_for_binning, mode=0, \n",
    "                         nb_grains=nb_grains_per_lp0, nb_grains1=nb_grains_per_lp1, \n",
    "                         grains_nb_simulate=grains_nb_simulate, data_realism = True, \n",
    "                         detectorparameters=detectorparameters, pixelsize=pixelsize, \n",
    "                         type_=\"training_data\",\n",
    "                         var0 = 1, dim1=input_params[\"dim1\"], dim2=input_params[\"dim2\"], \n",
    "                         removeharmonics=1, save_directory=save_directory,\n",
    "                         write_to_console=print, \n",
    "                         emin=emin, emax=emax, \n",
    "                         modelp = input_params[\"orientation_generation\"],\n",
    "                         misorientation_angle = input_params[\"misorientation\"], \n",
    "                         general_diff_rules = False, \n",
    "                         crystal = crystal, crystal1 = crystal1, \n",
    "                         include_scm=input_params[\"include_small_misorientation\"])\n",
    "        \n",
    "        ############ GENERATING TESTING DATA ##############\n",
    "        factor = 5 # validation split for the training dataset  --> corresponds to 20% of total training dataset\n",
    "        generate_dataset(material_=material_, material1_=material1_, ang_maxx=maximum_angle_to_search,\n",
    "                         step=step_for_binning, mode=0, \n",
    "                         nb_grains=nb_grains_per_lp0, nb_grains1=nb_grains_per_lp1, \n",
    "                         grains_nb_simulate=grains_nb_simulate//factor, data_realism = True, \n",
    "                         detectorparameters=detectorparameters, pixelsize=pixelsize, \n",
    "                         type_=\"testing_data\",\n",
    "                         var0 = 1, dim1=input_params[\"dim1\"], dim2=input_params[\"dim2\"], \n",
    "                         removeharmonics=1, save_directory=save_directory,\n",
    "                         write_to_console=print, \n",
    "                         emin=emin, emax=emax, \n",
    "                         modelp = input_params[\"orientation_generation\"],\n",
    "                         misorientation_angle = input_params[\"misorientation\"], \n",
    "                         general_diff_rules = False, \n",
    "                         crystal = crystal, crystal1 = crystal1, \n",
    "                         include_scm=input_params[\"include_small_misorientation\"])\n",
    "        \n",
    "        ## Updating the ClassHKL list by removing the non-common HKL or less frequent HKL from the list\n",
    "        ## The non-common HKL can occur as a result of the detector position and energy used\n",
    "        # freq_rmv: remove output hkl if the training dataset has less tha 100 occurances of \n",
    "        # the considered hkl (freq_rmv1 for second phase)\n",
    "        # Weights (penalty during training) are also calculated based on the occurance\n",
    "        rmv_freq_class(freq_rmv = input_params[\"classes_with_frequency_to_remove\"][0], \n",
    "                       freq_rmv1 = input_params[\"classes_with_frequency_to_remove\"][1], \n",
    "                       save_directory=save_directory, \n",
    "                       material_=material_, \n",
    "                       material1_=material1_, \n",
    "                       write_to_console=print,\n",
    "                       list_hkl_keep=None, list_hkl_keep1=None)\n",
    "        \n",
    "        ## End of data generation for Neural network training: all files are saved in the \n",
    "        ## same folder to be later used for training and prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7c6027-169f-4fcd-969d-e35fa37bdaa5",
   "metadata": {},
   "source": [
    "## Train a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4070c2f-68ec-4d88-90de-276b30abab67",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_network = False\n",
    "if __name__ == '__main__':     #enclosing required because of multiprocessing\n",
    "\n",
    "    if train_network:\n",
    "        import numpy as np\n",
    "        import os\n",
    "        import _pickle as cPickle\n",
    "        import itertools\n",
    "        from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "        import matplotlib.pyplot as plt\n",
    "        ## if LaueToolsNN is properly installed\n",
    "        from lauetoolsnn.utils_lauenn import array_generator, array_generator_verify, vali_array\n",
    "        # ## Defining a neural network architecture or load a predefined one from NNmodels\n",
    "        from lauetoolsnn.NNmodels import model_arch_general, LoggingCallback\n",
    "            \n",
    "        material_= input_params[\"material_\"][0]\n",
    "        material1_= input_params[\"material_\"][1]\n",
    "        nb_grains_per_lp = input_params[\"nb_grains_per_lp_mat\"][0]\n",
    "        nb_grains_per_lp1 = input_params[\"nb_grains_per_lp_mat\"][1]\n",
    "        grains_nb_simulate = input_params[\"grains_nb_simulate\"]\n",
    "        \n",
    "        if material_ != material1_:\n",
    "            save_directory = os.path.join(global_path,material_+\"_\"+material1_+input_params[\"prefix\"])\n",
    "        else:\n",
    "            save_directory = os.path.join(global_path,material_+input_params[\"prefix\"])\n",
    "        \n",
    "        if not os.path.exists(save_directory):\n",
    "            print(\"The directory doesn't exists; please veify the path\")\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Directory where training dataset is stored is : \"+save_directory)\n",
    "        \n",
    "        # ## Load the necessary files generated in Step 1 script\n",
    "        # ### Loading the Output class and ground truth        \n",
    "        classhkl = np.load(save_directory+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_0\"]\n",
    "        angbins = np.load(save_directory+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_1\"]\n",
    "        loc_new = np.load(save_directory+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_2\"]\n",
    "        with open(save_directory+\"//class_weights.pickle\", \"rb\") as input_file:\n",
    "            class_weights = cPickle.load(input_file)\n",
    "        class_weights = class_weights[0]\n",
    "        \n",
    "        # ## Training  \n",
    "        # load model and train\n",
    "        model = model_arch_general(len(angbins)-1, len(classhkl),\n",
    "                                    kernel_coeff = 1e-5,\n",
    "                                    bias_coeff = 1e-6,\n",
    "                                    lr = 1e-3)\n",
    "        \n",
    "        ## temp function to quantify the spots and classes present in a batch\n",
    "        batch_size = input_params[\"batch_size\"] \n",
    "        trainy_inbatch = array_generator_verify(save_directory+\"//training_data\", \n",
    "                                                batch_size, \n",
    "                                                len(classhkl), \n",
    "                                                loc_new, \n",
    "                                                print)\n",
    "        print(\"Number of spots in a batch of %i files : %i\" %(batch_size, len(trainy_inbatch)))\n",
    "        print(\"Min, Max class ID is %i, %i\" %(np.min(trainy_inbatch), np.max(trainy_inbatch)))\n",
    "        \n",
    "        epochs = input_params[\"epochs\"] \n",
    "        ## Batch loading for numpy grain files (Keep low value to avoid overcharging the RAM)\n",
    "        if material_ != material1_:\n",
    "            nb_grains_list = list(range(nb_grains_per_lp+1))\n",
    "            nb_grains1_list = list(range(nb_grains_per_lp1+1))\n",
    "            list_permute = list(itertools.product(nb_grains_list, nb_grains1_list))\n",
    "            list_permute.pop(0)\n",
    "            steps_per_epoch = (len(list_permute) * grains_nb_simulate)//batch_size\n",
    "        else:\n",
    "            steps_per_epoch = int((nb_grains_per_lp * grains_nb_simulate) / batch_size)\n",
    "        \n",
    "        val_steps_per_epoch = int(steps_per_epoch / 5)\n",
    "        if steps_per_epoch == 0:\n",
    "            steps_per_epoch = 1\n",
    "        if val_steps_per_epoch == 0:\n",
    "            val_steps_per_epoch = 1 \n",
    "\n",
    "        ## Load generator objects from filepaths (iterators for Training and Testing datasets)\n",
    "        training_data_generator = array_generator(save_directory+\"//training_data\", batch_size,len(classhkl), loc_new, print)\n",
    "        testing_data_generator = array_generator(save_directory+\"//testing_data\", batch_size,len(classhkl), loc_new, print)\n",
    "        \n",
    "        # model save directory and filename\n",
    "        if material_ != material1_:\n",
    "            model_name = save_directory+\"//model_\"+material_+\"_\"+material1_\n",
    "        else:\n",
    "            model_name = save_directory+\"//model_\"+material_\n",
    "            \n",
    "        ######### TRAIN THE DATA\n",
    "        es = EarlyStopping(monitor='val_accuracy', mode='max', patience=2)\n",
    "        ms = ModelCheckpoint(save_directory+\"//best_val_acc_model.h5\", \n",
    "                             monitor='val_accuracy', \n",
    "                             mode='max', save_best_only=True)\n",
    "        lc = LoggingCallback(None, None, None, model, model_name)\n",
    "\n",
    "        ## Fitting function\n",
    "        stats_model = model.fit(\n",
    "                                training_data_generator, \n",
    "                                epochs=epochs, \n",
    "                                steps_per_epoch=steps_per_epoch,\n",
    "                                validation_data=testing_data_generator,\n",
    "                                validation_steps=val_steps_per_epoch,\n",
    "                                verbose=1,\n",
    "                                class_weight=class_weights,\n",
    "                                callbacks=[es, ms, lc]\n",
    "                                )\n",
    "        \n",
    "        # Save model config and weights\n",
    "        model_json = model.to_json()\n",
    "        with open(model_name+\".json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)            \n",
    "        # serialize weights to HDF5\n",
    "        model.save_weights(model_name+\".h5\")\n",
    "        print(\"Saved model to disk\")\n",
    "        \n",
    "        print( \"Training Accuracy: \"+str( stats_model.history['accuracy'][-1]))\n",
    "        print( \"Training Loss: \"+str( stats_model.history['loss'][-1]))\n",
    "        print( \"Validation Accuracy: \"+str( stats_model.history['val_accuracy'][-1]))\n",
    "        print( \"Validation Loss: \"+str( stats_model.history['val_loss'][-1]))\n",
    "        \n",
    "        # Plot the accuracy/loss v Epochs\n",
    "        epochs = range(1, len(model.history.history['loss']) + 1)\n",
    "        fig, ax = plt.subplots(1,2)\n",
    "        ax[0].plot(epochs, model.history.history['loss'], 'r', label='Training loss')\n",
    "        ax[0].plot(epochs, model.history.history['val_loss'], 'r', ls=\"dashed\", label='Validation loss')\n",
    "        ax[0].legend()\n",
    "        ax[1].plot(epochs, model.history.history['accuracy'], 'g', label='Training Accuracy')\n",
    "        ax[1].plot(epochs, model.history.history['val_accuracy'], 'g', ls=\"dashed\", label='Validation Accuracy')\n",
    "        ax[1].legend()\n",
    "        if material_ != material1_:\n",
    "            plt.savefig(save_directory+\"//loss_accuracy_\"+material_+\"_\"+material1_+\".png\", \n",
    "                        bbox_inches='tight',format='png', dpi=1000)\n",
    "        else:\n",
    "            plt.savefig(save_directory+\"//loss_accuracy_\"+material_+\".png\", \n",
    "                        bbox_inches='tight',format='png', dpi=1000)\n",
    "        plt.close()\n",
    "        \n",
    "        if material_ != material1_:\n",
    "            text_file = open(save_directory+\"//loss_accuracy_logger_\"+material_+\"_\"+material1_+\".txt\", \"w\")\n",
    "        else:\n",
    "            text_file = open(save_directory+\"//loss_accuracy_logger_\"+material_+\".txt\", \"w\")\n",
    "        \n",
    "        text_file.write(\"# EPOCH, LOSS, VAL_LOSS, ACCURACY, VAL_ACCURACY\" + \"\\n\")\n",
    "        for inj in range(len(epochs)):\n",
    "            string1 = str(epochs[inj]) + \",\"\n",
    "            string1 = string1 + str(model.history.history['loss'][inj])\n",
    "            string1 = string1 + \",\"+str(model.history.history['val_loss'][inj])\n",
    "            string1 = string1 + \",\"+str(model.history.history['accuracy'][inj])\n",
    "            string1 = string1 + \",\"+str(model.history.history['val_accuracy'][inj])+\" \\n\"  \n",
    "            text_file.write(string1)\n",
    "        text_file.close() \n",
    "        \n",
    "        # ## Stats on the trained model with sklearn metrics\n",
    "        from sklearn.metrics import classification_report\n",
    "        x_test, y_test = vali_array(save_directory+\"//testing_data\", 50, len(classhkl), loc_new, print)\n",
    "        y_test = np.argmax(y_test, axis=-1)\n",
    "        y_pred = np.argmax(model.predict(x_test), axis=-1)\n",
    "        print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4708d850-eca4-4c96-ae48-fba713e1295c",
   "metadata": {},
   "source": [
    "## Index the Laue patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf0fea36-c55f-44bc-b7e1-8ef2e7121ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjustText library not installed\n",
      "Number of CPUs available :  64\n",
      "Directory where trained model is stored : /home/esrf/purushot/Desktop/LaueNN_tutorial/LaueNN_script/GaN_Si_2phase\n",
      "Writing settings file in /home/esrf/purushot/anaconda3/envs/lauenn/lib/python3.7/site-packages/lauetoolsnn/settings.ini\n",
      "Constructing model\n",
      "Uploading weights to model\n",
      "All model files found and loaded\n",
      "expected 2 files based on the XY grid (1,2) defined by user\n",
      "and found 2 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skimage mode (strict constraints) of PeakSearch is used for the PeakSearch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 39.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skimage mode (strict constraints) of PeakSearch is used for the PeakSearch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_prediction = True\n",
    "if __name__ == '__main__':     #enclosing required because of multiprocessing\n",
    "   \n",
    "    if run_prediction:\n",
    "        import numpy as np\n",
    "        import multiprocessing, os, time, datetime\n",
    "        import configparser, glob, re\n",
    "        ## if LaueToolsNN is properly installed\n",
    "        from lauetoolsnn.utils_lauenn import  get_material_detail, new_MP_function, resource_path, global_plots\n",
    "        from lauetoolsnn.lauetools import dict_LaueTools as dictLT\n",
    "        from lauetoolsnn.NNmodels import read_hdf5\n",
    "        from keras.models import model_from_json\n",
    "        import _pickle as cPickle\n",
    "        from tqdm import tqdm\n",
    "\n",
    "        ncpu = multiprocessing.cpu_count()\n",
    "        print(\"Number of CPUs available : \", ncpu)\n",
    "\n",
    "        # ## Get material parameters \n",
    "        # ### Get model and data paths from the input\n",
    "        # ### User input parameters for various algorithms to compute the orientation matrix\n",
    "\n",
    "        material_= input_params[\"material_\"][0]\n",
    "        material1_= input_params[\"material_\"][1]\n",
    "        detectorparameters = input_params[\"detectorparameters\"]\n",
    "        pixelsize = input_params[\"pixelsize\"]\n",
    "        emax = input_params[\"emax\"]\n",
    "        emin = input_params[\"emin\"]\n",
    "        dim1 = input_params[\"dim1\"]\n",
    "        dim2 = input_params[\"dim2\"]\n",
    "        symm_ = input_params[\"symmetry\"][0]\n",
    "        symm1_ = input_params[\"symmetry\"][1]\n",
    "        SG = input_params[\"SG\"][0]\n",
    "        SG1 = input_params[\"SG\"][1]\n",
    "\n",
    "        if material_ != material1_:\n",
    "            model_direc = os.path.join(global_path,material_+\"_\"+material1_+input_params[\"prefix\"])\n",
    "        else:\n",
    "            model_direc = os.path.join(global_path,material_+input_params[\"prefix\"])\n",
    "\n",
    "        if not os.path.exists(model_direc):\n",
    "            print(\"The directory doesn't exists; please veify the path\")\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Directory where trained model is stored : \"+model_direc)\n",
    "\n",
    "        if material_ != material1_:\n",
    "            prefix1 = material_+\"_\"+material1_\n",
    "        else:\n",
    "            prefix1 = material_\n",
    "\n",
    "        filenameDirec = input_params[\"experimental_directory\"]\n",
    "        experimental_prefix = input_params[\"experimental_prefix\"]\n",
    "        lim_x, lim_y = input_params[\"grid_size_x\"], input_params[\"grid_size_y\"] \n",
    "        format_file = dictLT.dict_CCD[\"sCMOS\"][7]\n",
    "        ## Experimental peak search parameters in case of RAW LAUE PATTERNS from detector\n",
    "        intensity_threshold = input_params[\"intensity_threshold\"]\n",
    "        boxsize = input_params[\"boxsize\"]\n",
    "        fit_peaks_gaussian = input_params[\"fit_peaks_gaussian\"]\n",
    "        FitPixelDev = input_params[\"FitPixelDev\"]\n",
    "        NumberMaxofFits = input_params[\"NumberMaxofFits\"]\n",
    "        bkg_treatment = \"A-B\"\n",
    "        mode_peaksearch = input_params[\"mode\"]\n",
    "\n",
    "        ## get unit cell parameters and other details required for simulating Laue patterns\n",
    "        rules, symmetry, lattice_material,\\\n",
    "            crystal, SG, rules1, symmetry1,\\\n",
    "                lattice_material1, crystal1, SG1 = get_material_detail(material_, SG, symm_,\n",
    "                                                                   material1_, SG1, symm1_)\n",
    "\n",
    "        ## get proper Laue group to compute the inverse pole figure colors and write MTEX output file for orientation analysis\n",
    "        material0_lauegroup = \"3\"\n",
    "        ## incase of same material\n",
    "        material1_lauegroup = \"5\"\n",
    "\n",
    "        ## Requirements\n",
    "        ubmat = input_params[\"UB_matrix_to_detect\"] # How many orientation matrix to detect per Laue pattern\n",
    "        mode_spotCycle = input_params[\"mode_spotCycle\"] ## mode of calculation\n",
    "        use_previous_UBmatrix_name = input_params[\"use_previous\"] ## Try previous indexation solutions to speed up the process\n",
    "        strain_calculation = input_params[\"strain_compute\"] ## Strain refinement is required or not\n",
    "        ccd_label_global = input_params[\"ccd_label\"]\n",
    "\n",
    "        ## tolerance angle to match simulated and experimental spots for two materials\n",
    "        tolerance = input_params[\"matrix_tolerance\"][0]\n",
    "        tolerance1 = input_params[\"matrix_tolerance\"][1]\n",
    "\n",
    "        ## tolerance angle for strain refinements\n",
    "        tolerance_strain = input_params[\"tolerance_strain_refinement\"][0]\n",
    "        tolerance_strain1 = input_params[\"tolerance_strain_refinement\"][1]\n",
    "        strain_free_parameters = input_params[\"free_parameters\"]\n",
    "\n",
    "        ## Parameters to control the orientation matrix indexation\n",
    "        softmax_threshold_global = input_params[\"softmax_threshold_global\"] # softmax_threshold of the Neural network to consider\n",
    "        mr_threshold_global = 0.90 # match rate threshold to accept a solution immediately\n",
    "        cap_matchrate = input_params[\"cap_matchrate\"] * 100 ## any UB matrix providing MR less than this will be ignored\n",
    "        coeff = 0.30            ## coefficient to calculate the overlap of two solutions\n",
    "        coeff_overlap = input_params[\"coeff_overlap\"]    ##10% spots overlap is allowed with already indexed orientation\n",
    "        material0_limit = input_params[\"material_limit\"][0]  ## how many UB can be proposed for first material\n",
    "        material1_limit = input_params[\"material_limit\"][1] ## how many UB can be proposed for second material; this forces the orientation matrix deduction algorithm to find only a required materials matrix\n",
    "        material_phase_always_present = input_params[\"material_phase_always_present\"] ## in case if one phase is always present in a Laue pattern (useful for substrate cases)\n",
    "\n",
    "        ## Additional parameters to refine the orientation matrix construction process\n",
    "        use_om_user = str(input_params[\"use_om_user\"]).lower()\n",
    "        path_user_OM = input_params[\"path_user_OM\"]\n",
    "        nb_spots_consider = input_params[\"nb_spots_consider\"]\n",
    "        residues_threshold = input_params[\"residues_threshold\"]\n",
    "        nb_spots_global_threshold = input_params[\"nb_spots_global_threshold\"]\n",
    "        option_global = \"v2\"\n",
    "        additional_expression = [\"none\"] # for strain assumptions, like a==b for HCP\n",
    "\n",
    "        config_setting = configparser.ConfigParser()\n",
    "        filepath = resource_path('settings.ini')\n",
    "        print(\"Writing settings file in \" + filepath)\n",
    "        config_setting.read(filepath)\n",
    "        config_setting.set('CALLER', 'residues_threshold',str(residues_threshold))\n",
    "        config_setting.set('CALLER', 'nb_spots_global_threshold',str(nb_spots_global_threshold))\n",
    "        config_setting.set('CALLER', 'option_global',option_global)\n",
    "        config_setting.set('CALLER', 'use_om_user',use_om_user)\n",
    "        config_setting.set('CALLER', 'nb_spots_consider',str(nb_spots_consider))\n",
    "        config_setting.set('CALLER', 'path_user_OM',str(path_user_OM))\n",
    "        config_setting.set('CALLER', 'intensity', str(intensity_threshold))\n",
    "        config_setting.set('CALLER', 'boxsize', str(boxsize))\n",
    "        config_setting.set('CALLER', 'pixdev', str(FitPixelDev))\n",
    "        config_setting.set('CALLER', 'cap_softmax', str(softmax_threshold_global))\n",
    "        config_setting.set('CALLER', 'cap_mr', str(cap_matchrate/100.))\n",
    "        config_setting.set('CALLER', 'strain_free_parameters', \",\".join(strain_free_parameters))\n",
    "        config_setting.set('CALLER', 'additional_expression', \",\".join(additional_expression))\n",
    "        config_setting.set('CALLER', 'mode_peaksearch', str(mode_peaksearch))\n",
    "        with open(filepath, 'w') as configfile:\n",
    "            config_setting.write(configfile)\n",
    "\n",
    "        ## load model related files and generate the model\n",
    "        json_file = open(model_direc+\"//model_\"+prefix1+\".json\", 'r')\n",
    "        classhkl = np.load(model_direc+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_0\"]\n",
    "        angbins = np.load(model_direc+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_1\"]\n",
    "        ind_mat = np.load(model_direc+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_5\"]\n",
    "        ind_mat1 = np.load(model_direc+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_6\"]  \n",
    "        load_weights = model_direc + \"//model_\"+prefix1+\".h5\"\n",
    "        wb = read_hdf5(load_weights)\n",
    "        temp_key = list(wb.keys())\n",
    "\n",
    "        # # load json and create model\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        model = model_from_json(loaded_model_json)\n",
    "        print(\"Constructing model\")\n",
    "        model.load_weights(load_weights)\n",
    "        print(\"Uploading weights to model\")\n",
    "        print(\"All model files found and loaded\")\n",
    "\n",
    "        ct = time.time()\n",
    "        now = datetime.datetime.fromtimestamp(ct)\n",
    "        c_time = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "        hkl_all_class1 = None\n",
    "        with open(model_direc+\"//classhkl_data_nonpickled_\"+material_+\".pickle\", \"rb\") as input_file:\n",
    "            hkl_all_class0 = cPickle.load(input_file)[0]\n",
    "\n",
    "        if material_ != material1_:\n",
    "            with open(model_direc+\"//classhkl_data_nonpickled_\"+material1_+\".pickle\", \"rb\") as input_file:\n",
    "                hkl_all_class1 = cPickle.load(input_file)[0]\n",
    "\n",
    "        # ## Initialize variables and prepare arguments for multiprocessing module\n",
    "        col = [[] for i in range(int(ubmat))]\n",
    "        colx = [[] for i in range(int(ubmat))]\n",
    "        coly = [[] for i in range(int(ubmat))]\n",
    "        rotation_matrix = [[] for i in range(int(ubmat))]\n",
    "        strain_matrix = [[] for i in range(int(ubmat))]\n",
    "        strain_matrixs = [[] for i in range(int(ubmat))]\n",
    "        match_rate = [[] for i in range(int(ubmat))]\n",
    "        spots_len = [[] for i in range(int(ubmat))]\n",
    "        iR_pix = [[] for i in range(int(ubmat))]\n",
    "        fR_pix = [[] for i in range(int(ubmat))]\n",
    "        mat_global = [[] for i in range(int(ubmat))]\n",
    "        best_match = [[] for i in range(int(ubmat))]\n",
    "        spots1_global = [[] for i in range(int(ubmat))]\n",
    "        for i in range(int(ubmat)):\n",
    "            col[i].append(np.zeros((lim_x*lim_y,3)))\n",
    "            colx[i].append(np.zeros((lim_x*lim_y,3)))\n",
    "            coly[i].append(np.zeros((lim_x*lim_y,3)))\n",
    "            rotation_matrix[i].append(np.zeros((lim_x*lim_y,3,3)))\n",
    "            strain_matrix[i].append(np.zeros((lim_x*lim_y,3,3)))\n",
    "            strain_matrixs[i].append(np.zeros((lim_x*lim_y,3,3)))\n",
    "            match_rate[i].append(np.zeros((lim_x*lim_y,1)))\n",
    "            spots_len[i].append(np.zeros((lim_x*lim_y,1)))\n",
    "            iR_pix[i].append(np.zeros((lim_x*lim_y,1)))\n",
    "            fR_pix[i].append(np.zeros((lim_x*lim_y,1)))\n",
    "            mat_global[i].append(np.zeros((lim_x*lim_y,1)))\n",
    "            best_match[i].append([[] for jk in range(lim_x*lim_y)])\n",
    "            spots1_global[i].append([[] for jk in range(lim_x*lim_y)])\n",
    "\n",
    "        ##hack for multiprocessing, but very time consuming\n",
    "        if use_previous_UBmatrix_name:\n",
    "            np.savez_compressed(model_direc+'//rotation_matrix_indexed_1.npz', rotation_matrix, mat_global, match_rate, 0.0)\n",
    "\n",
    "        # =============================================================================\n",
    "        #         ## Multi-processing routine\n",
    "        # =============================================================================        \n",
    "        ## Number of files to generate\n",
    "        grid_files = np.zeros((lim_x,lim_y))\n",
    "        filenm = np.chararray((lim_x,lim_y), itemsize=1000)\n",
    "        grid_files = grid_files.ravel()\n",
    "        filenm = filenm.ravel()\n",
    "        count_global = lim_x * lim_y\n",
    "        list_of_files = glob.glob(filenameDirec+'//'+experimental_prefix+'*.'+format_file)\n",
    "        ## sort files\n",
    "        list_of_files.sort(key=lambda var:[int(x) if x.isdigit() else x for x in re.findall(r'[^0-9]|[0-9]+', var)])\n",
    "\n",
    "        if len(list_of_files) == count_global:\n",
    "            for ii in range(len(list_of_files)):\n",
    "                grid_files[ii] = ii\n",
    "                filenm[ii] = list_of_files[ii]     \n",
    "            print(\"expected \"+str(count_global)+\" files based on the XY grid (\"+str(lim_x)+\",\"+str(lim_y)+\") defined by user\")\n",
    "            print(\"and found \"+str(len(list_of_files))+\" files\")\n",
    "        else:\n",
    "            print(\"expected \"+str(count_global)+\" files based on the XY grid (\"+str(lim_x)+\",\"+str(lim_y)+\") defined by user\")\n",
    "            print(\"But found \"+str(len(list_of_files))+\" files (either all data is not written yet or maybe XY grid definition is not proper)\")\n",
    "            digits = len(str(count_global))\n",
    "            digits = max(digits,4)\n",
    "            # Temp fix\n",
    "            for ii in range(count_global):\n",
    "                text = str(ii)\n",
    "                if ii < 10000:\n",
    "                    string = text.zfill(4)\n",
    "                else:\n",
    "                    string = text.zfill(5)\n",
    "                file_name_temp = filenameDirec+'//'+experimental_prefix + string+'.'+format_file\n",
    "                ## store it in a grid \n",
    "                filenm[ii] = file_name_temp\n",
    "\n",
    "        check = np.zeros((count_global,int(ubmat)))\n",
    "        # =============================================================================\n",
    "        blacklist = None\n",
    "\n",
    "        ### Create a COR directory to be loaded in LaueTools\n",
    "        cor_file_directory = filenameDirec + \"//\" + experimental_prefix+\"CORfiles\"\n",
    "        if list_of_files[0].split(\".\")[-1] in ['cor',\"COR\",\"Cor\"]:\n",
    "            cor_file_directory = filenameDirec \n",
    "        if not os.path.exists(cor_file_directory):\n",
    "            os.makedirs(cor_file_directory)\n",
    "\n",
    "        try_prevs = False\n",
    "        files_treated = []\n",
    "\n",
    "        ##making a big argument list for each CPU\n",
    "        valu12 = [[filenm[ii].decode(), ii,\n",
    "                   rotation_matrix,\n",
    "                    strain_matrix,\n",
    "                    strain_matrixs,\n",
    "                    col,\n",
    "                    colx,\n",
    "                    coly,\n",
    "                    match_rate,\n",
    "                    spots_len, \n",
    "                    iR_pix, \n",
    "                    fR_pix,\n",
    "                    best_match,\n",
    "                    mat_global,\n",
    "                    check,\n",
    "                    detectorparameters,\n",
    "                    pixelsize,\n",
    "                    angbins,\n",
    "                    classhkl,\n",
    "                    hkl_all_class0,\n",
    "                    hkl_all_class1,\n",
    "                    emin,\n",
    "                    emax,\n",
    "                    material_,\n",
    "                    material1_,\n",
    "                    symmetry,\n",
    "                    symmetry1,   \n",
    "                    lim_x,\n",
    "                    lim_y,\n",
    "                    strain_calculation, \n",
    "                    ind_mat, ind_mat1,\n",
    "                    model_direc, float(tolerance),\n",
    "                    float(tolerance1),\n",
    "                    int(ubmat), ccd_label_global, \n",
    "                    None,\n",
    "                    float(intensity_threshold),\n",
    "                    int(boxsize),bkg_treatment,\n",
    "                    filenameDirec, \n",
    "                    experimental_prefix,\n",
    "                    blacklist,\n",
    "                    None,\n",
    "                    files_treated,\n",
    "                    try_prevs, ## try previous is kept true, incase if its stuck in loop\n",
    "                    wb,\n",
    "                    temp_key,\n",
    "                    cor_file_directory,\n",
    "                    mode_spotCycle,\n",
    "                    softmax_threshold_global,\n",
    "                    mr_threshold_global,\n",
    "                    cap_matchrate,\n",
    "                    tolerance_strain,\n",
    "                    tolerance_strain1,\n",
    "                    NumberMaxofFits,\n",
    "                    fit_peaks_gaussian,\n",
    "                    FitPixelDev,\n",
    "                    coeff,\n",
    "                    coeff_overlap,\n",
    "                    material0_limit,\n",
    "                    material1_limit,\n",
    "                    use_previous_UBmatrix_name,\n",
    "                    material_phase_always_present,\n",
    "                    crystal,\n",
    "                    crystal1,\n",
    "                    strain_free_parameters] for ii in range(count_global)]\n",
    "\n",
    "        ## test singel file prediction\n",
    "        # results = new_MP_function(valu12[0])\n",
    "        # best = results[-2][0][0][0]\n",
    "\n",
    "        args = zip(valu12)\n",
    "        with multiprocessing.Pool(ncpu) as pool:\n",
    "            results = pool.starmap(new_MP_function, tqdm(args, total=len(valu12)), chunksize=1)\n",
    "\n",
    "            for r_message_mpdata in results:\n",
    "                strain_matrix_mpdata, strain_matrixs_mpdata, rotation_matrix_mpdata, col_mpdata,\\\n",
    "                colx_mpdata, coly_mpdata, match_rate_mpdata, mat_global_mpdata,\\\n",
    "                    cnt_mpdata, meta_mpdata, files_treated_mpdata, spots_len_mpdata, \\\n",
    "                        iR_pixel_mpdata, fR_pixel_mpdata, best_match_mpdata, check_mpdata = r_message_mpdata\n",
    "\n",
    "                for i_mpdata in files_treated_mpdata:\n",
    "                    files_treated.append(i_mpdata)\n",
    "\n",
    "                for intmat_mpdata in range(int(ubmat)):\n",
    "                    check[cnt_mpdata,intmat_mpdata] = check_mpdata[cnt_mpdata,intmat_mpdata]\n",
    "                    mat_global[intmat_mpdata][0][cnt_mpdata] = mat_global_mpdata[intmat_mpdata][0][cnt_mpdata]\n",
    "                    strain_matrix[intmat_mpdata][0][cnt_mpdata,:,:] = strain_matrix_mpdata[intmat_mpdata][0][cnt_mpdata,:,:]\n",
    "                    strain_matrixs[intmat_mpdata][0][cnt_mpdata,:,:] = strain_matrixs_mpdata[intmat_mpdata][0][cnt_mpdata,:,:]\n",
    "                    rotation_matrix[intmat_mpdata][0][cnt_mpdata,:,:] = rotation_matrix_mpdata[intmat_mpdata][0][cnt_mpdata,:,:]\n",
    "                    col[intmat_mpdata][0][cnt_mpdata,:] = col_mpdata[intmat_mpdata][0][cnt_mpdata,:]\n",
    "                    colx[intmat_mpdata][0][cnt_mpdata,:] = colx_mpdata[intmat_mpdata][0][cnt_mpdata,:]\n",
    "                    coly[intmat_mpdata][0][cnt_mpdata,:] = coly_mpdata[intmat_mpdata][0][cnt_mpdata,:]\n",
    "                    match_rate[intmat_mpdata][0][cnt_mpdata] = match_rate_mpdata[intmat_mpdata][0][cnt_mpdata]\n",
    "                    spots_len[intmat_mpdata][0][cnt_mpdata] = spots_len_mpdata[intmat_mpdata][0][cnt_mpdata]\n",
    "                    iR_pix[intmat_mpdata][0][cnt_mpdata] = iR_pixel_mpdata[intmat_mpdata][0][cnt_mpdata]\n",
    "                    fR_pix[intmat_mpdata][0][cnt_mpdata] = fR_pixel_mpdata[intmat_mpdata][0][cnt_mpdata]\n",
    "                    best_match[intmat_mpdata][0][cnt_mpdata] = best_match_mpdata[intmat_mpdata][0][cnt_mpdata]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ff4a3c-6640-4c56-b9a1-38450a31f742",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcd6525c-c367-42be-864e-ded43865f9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data saved in  /home/esrf/purushot/Desktop/LaueNN_tutorial/LaueNN_script/GaN_Si_2phase//results__2phase_2022-11-14_23-41-25\n",
      "Number of Phases present (includes non indexed phase zero also) 3\n",
      "building KD tree...\n",
      "prediction statistics are generated\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1169x827 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1169x827 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1169x827 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1169x827 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1169x827 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1169x827 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lauetoolsnn.utils_lauenn import  global_plots, write_average_orientation, convert_pickle_to_hdf5, write_prediction_stats, write_MTEXdata\n",
    "\n",
    "### Save files and results\n",
    "save_directory_ = model_direc+\"//results_\"+input_params[\"prefix\"]+\"_\"+c_time\n",
    "if not os.path.exists(save_directory_):\n",
    "    os.makedirs(save_directory_)\n",
    "\n",
    "np.savez_compressed(save_directory_+ \"//results.npz\", \n",
    "                    best_match, mat_global, rotation_matrix, strain_matrix, \n",
    "                    strain_matrixs, col, colx, coly, match_rate, files_treated,\n",
    "                    lim_x, lim_y, spots_len, iR_pix, fR_pix,\n",
    "                    material_, material1_)\n",
    "## intermediate saving of pickle objects with results\n",
    "with open(save_directory_+ \"//results.pickle\", \"wb\") as output_file:\n",
    "        cPickle.dump([best_match, mat_global, rotation_matrix, strain_matrix, \n",
    "                      strain_matrixs, col, colx, coly, match_rate, files_treated,\n",
    "                      lim_x, lim_y, spots_len, iR_pix, fR_pix,\n",
    "                      material_, material1_, lattice_material, lattice_material1,\n",
    "                      symmetry, symmetry1, crystal, crystal1], output_file)\n",
    "print(\"data saved in \", save_directory_)\n",
    "\n",
    "## Lets save also a set of average UB matrix in text file to be used with user_OM setting    \n",
    "try:\n",
    "    write_average_orientation(save_directory_, mat_global, rotation_matrix,\n",
    "                                  match_rate, lim_x, lim_y, crystal, crystal1,\n",
    "                                  radius=10, grain_ang=5, pixel_grain_definition=3)\n",
    "except:\n",
    "    print(\"Error with Average orientation and grain index calculation\")\n",
    "\n",
    "try:\n",
    "    convert_pickle_to_hdf5(save_directory_, files_treated, rotation_matrix, strain_matrix, \n",
    "                           strain_matrixs, match_rate, spots_len, iR_pix, \n",
    "                           fR_pix, colx, coly, col, mat_global,\n",
    "                           material_, material1_, lim_x, lim_y)\n",
    "except:\n",
    "    print(\"Error writting H5 file\")\n",
    "\n",
    "try:\n",
    "    write_prediction_stats(save_directory_, material_, material1_, files_treated,\\\n",
    "                            lim_x, lim_y, best_match, strain_matrixs, strain_matrix, iR_pix,\\\n",
    "                            fR_pix,  mat_global)\n",
    "except:\n",
    "    print(\"Error writting prediction statistics file\")\n",
    "\n",
    "try:\n",
    "    write_MTEXdata(save_directory_, material_, material1_, rotation_matrix,\\\n",
    "                       lattice_material, lattice_material1, lim_x, lim_y, mat_global,\\\n",
    "                        input_params[\"symmetry\"][0], input_params[\"symmetry\"][1])\n",
    "except:\n",
    "    print(\"Error writting MTEX orientation file\")\n",
    "\n",
    "try:\n",
    "    global_plots(lim_x, lim_y, rotation_matrix, strain_matrix, strain_matrixs, \n",
    "                  col, colx, coly, match_rate, mat_global, spots_len, \n",
    "                  iR_pix, fR_pix, save_directory_, material_, material1_,\n",
    "                  match_rate_threshold=5, bins=30)\n",
    "except:\n",
    "    print(\"Error in the global plots module\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cd4ce5-aaa7-410e-8e54-8559ad1c1ec1",
   "metadata": {},
   "source": [
    "## Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c685496-ee6a-4883-ad1d-d037091651f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/x-hdf5": "/home/esrf/purushot/Desktop/LaueNN_tutorial/LaueNN_script/GaN_Si_2phase/results__2phase_2022-11-14_23-41-25/grain_all.h5",
      "text/plain": [
       "<jupyterlab_h5web.widget.H5Web object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jupyterlab_h5web import H5Web\n",
    "H5Web(os.path.join(save_directory_,\"grain_all.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed3a6e5-cf6c-415c-941c-df56776b6669",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lauenn",
   "language": "python",
   "name": "lauenn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
