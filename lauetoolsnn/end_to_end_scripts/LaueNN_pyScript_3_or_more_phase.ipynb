{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61bda473-6e51-4914-826e-21c50f0f61d1",
   "metadata": {},
   "source": [
    "# Notebook script for Training the neural network (supports More than 2 phases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296c62c0-e898-4212-a783-412947bd31a3",
   "metadata": {},
   "source": [
    "## Define material and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2ad7bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaueNN path is /home/esrf/purushot/anaconda3/envs/lauenn/lib/python3.7/site-packages/lauetoolsnn\n",
      "['GaN', [3.189, 3.189, 5.185, 90, 90, 120], 'wurtzite']\n",
      "['Si', [5.4309, 5.4309, 5.4309, 90, 90, 90], 'dia']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':     #enclosing required because of multiprocessing\n",
    "\n",
    "    ## Import modules used for this Notebook\n",
    "    import os\n",
    "    import json\n",
    "    \n",
    "    ## Get the path of the lauetoolsnn library\n",
    "    import lauetoolsnn\n",
    "    laueNN_path = os.path.dirname(lauetoolsnn.__file__)\n",
    "    print(\"LaueNN path is\", laueNN_path)\n",
    "    \n",
    "    ## Load the json of material and extinctions\n",
    "    with open(os.path.join(laueNN_path, 'lauetools','material.json'),'r') as f:\n",
    "        dict_Materials = json.load(f)\n",
    "    with open(os.path.join(laueNN_path, 'lauetools','extinction.json'),'r') as f:\n",
    "        extinction_json = json.load(f)\n",
    "\n",
    "    ## Modify the dictionary values to add new entries\n",
    "    dict_Materials[\"GaN\"] = [\"GaN\", [3.189, 3.189, 5.185, 90, 90, 120], \"wurtzite\"]\n",
    "    dict_Materials[\"Si\"] = [\"Si\", [5.4309, 5.4309, 5.4309, 90, 90, 90], \"dia\"]\n",
    "\n",
    "    extinction_json[\"wurtzite\"] = \"wurtzite\"\n",
    "    extinction_json[\"dia\"] = \"dia\"\n",
    "\n",
    "    ## verify if extinction is present in CrystalParameters.py file of lauetools (Manually done for now)\n",
    "\n",
    "    ## dump the json back with new values\n",
    "    with open(os.path.join(laueNN_path, 'lauetools','material.json'), 'w') as fp:\n",
    "        json.dump(dict_Materials, fp)\n",
    "    with open(os.path.join(laueNN_path, 'lauetools','extinction.json'), 'w') as fp:\n",
    "        json.dump(extinction_json, fp)\n",
    "\n",
    "    ## Verify if the material is added to the library or not;\n",
    "    from lauetoolsnn.lauetools.dict_LaueTools import dict_Materials\n",
    "    ## if not, restart the console\n",
    "    print(dict_Materials[\"GaN\"])\n",
    "    print(dict_Materials[\"Si\"])\n",
    "    \n",
    "    # =============================================================================\n",
    "    # Step 0: Define the dictionary with all parameters \n",
    "    # =============================================================================\n",
    "    ## User Input dictionary with parameters\n",
    "    ## In case of only one phase/material, keep same value for material_ and material1_ key\n",
    "    input_params = {\n",
    "                    \"global_path\" : os.getcwd(),\n",
    "                    \"prefix\" : \"_MultiMaterial\",                 ## prefix for the folder to be created for training dataset\n",
    "\n",
    "                    \"material_\": [\"GaN\", \"Si\"],             ## same key as used in dict_LaueTools\n",
    "                    \"symmetry\": [\"hexagonal\", \"cubic\"],           ## crystal symmetry of material_\n",
    "                    \"SG\": [191, 227], #186                    ## Space group of material_ (None if not known)\n",
    "                    \"hkl_max_identify\" : [6,5],        ## Maximum hkl index to classify in a Laue pattern\n",
    "                    \"nb_grains_per_lp\" : [2,1],        ## max grains to be generated in a Laue Image\n",
    "\n",
    "                    ## hkl_max_identify : can be \"auto\" or integer: Maximum index of HKL to build output classes\n",
    "                    \n",
    "                    # =============================================================================\n",
    "                    # ## Data generation settings\n",
    "                    # =============================================================================\n",
    "                    \"grains_nb_simulate\" : 500,    ## Number of orientations to generate (takes advantage of crystal symmetry)\n",
    "                    \"classes_with_frequency_to_remove\": [100,100], ## classes_with_frequency_to_remove: HKL class with less appearance than \n",
    "                                                                            ##  specified will be ignored in output\n",
    "                    \"desired_classes_output\": [\"all\",\"all\"], ## desired_classes_output : can be all or an integer: to limit the number of output classes\n",
    "                    \"list_hkl_keep\" : None, #[[(0,0,1)],[(0,0,0)]],\n",
    "                    \"maximum_angle_to_search\":90, ## Angle of radial distribution to reconstruct the histogram (in deg)\n",
    "                    \"step_for_binning\" : 0.1,      ## bin widht of angular radial distribution in degree\n",
    "                    \n",
    "                    # =============================================================================\n",
    "                    #  ## Training parameters\n",
    "                    # =============================================================================\n",
    "                    \"orientation_generation\": \"random\", ## could be \"uniform\" or \"random\"\n",
    "                    \"batch_size\":100,               ## batches of files to use while training\n",
    "                    \"epochs\":8,                    ## number of epochs for training\n",
    "\n",
    "                    # =============================================================================\n",
    "                    # ## Detector parameters of the Experimental setup\n",
    "                    # =============================================================================\n",
    "                    ## Sample-detector distance, X center, Y center, two detector angles\n",
    "                    \"detectorparameters\" :  [79.61200, 977.8100, 932.1700, 0.4770000, 0.4470000], \n",
    "                    \"pixelsize\" : 0.0734,          ## Detector pixel size\n",
    "                    \"dim1\":2018,                   ## Dimensions of detector in pixels\n",
    "                    \"dim2\":2016,\n",
    "                    \"emin\" : 5,                    ## Minimum and maximum energy to use for simulating Laue Patterns\n",
    "                    \"emax\" : 22,\n",
    "                    \"ccd_label\" : \"sCMOS\",\n",
    "                    \n",
    "                    # =============================================================================\n",
    "                    # ## Prediction parameters\n",
    "                    # =============================================================================\n",
    "                    \"experimental_directory\": os.getcwd(),\n",
    "                    \"experimental_prefix\": r\"nw1_\",\n",
    "                    \"grid_size_x\" : 1,            ## Grid X and Y limit to generate the simulated dataset (a rectangular scan region)\n",
    "                    \"grid_size_y\" : 2,\n",
    "                    \n",
    "                    # =============================================================================\n",
    "                    # ## Prediction Settings\n",
    "                    # =============================================================================\n",
    "                    # model_weight_file: if none, it will select by default the latest H5 weight file, else provide a specific model\n",
    "                    # softmax_threshold_global: thresholding to limit the predicted spots search zone\n",
    "                    # cap_matchrate: any UB matrix providing MR less than this will be ignored\n",
    "                    # coeff: should be same as cap_matchrate or no? (this is for when use_previous is True)\n",
    "                    # coeff_overlap: coefficient to limit the overlapping between spots; if more than this, new solution will be computed\n",
    "                    # mode_spotCycle: How to cycle through predicted spots (slow or graphmode )\n",
    "                    \"UB_matrix_to_detect\" : 3,\n",
    "                    \"matrix_tolerance\" : [0.6, 0.6],\n",
    "                    \"material_limit\" : [2, 1],\n",
    "                    \"material_phase_always_present\" : None,#[2,1,1],\n",
    "                    \"softmax_threshold_global\" : 0.85,\n",
    "                    \"cap_matchrate\" : 0.10,\n",
    "                    \"coeff_overlap\" : 0.3,\n",
    "                    \"mode_spotCycle\" : \"graphmode\",\n",
    "                    ##true for few crystal and prefered texture case, otherwise time consuming; advised for single phase alone\n",
    "                    \"use_previous\" : False,\n",
    "                    \n",
    "                    # =============================================================================\n",
    "                    # # [PEAKSEARCH]\n",
    "                    # =============================================================================\n",
    "                    \"intensity_threshold\" : 1,## for skimage this is of image standard deviation\n",
    "                    \"boxsize\" : 10,## for skimage this is box size to fit\n",
    "                    \"fit_peaks_gaussian\" : 1,## for skimage this is of no sense\n",
    "                    \"FitPixelDev\" : 3, ## for skimage this is distance between peaks to avoid\n",
    "                    \"NumberMaxofFits\" : 3000,## for skimage this is maximum leastquare attempts before giving up\n",
    "                    \"mode\": \"skimage\",\n",
    "\n",
    "                    # =============================================================================\n",
    "                    # # [STRAINCALCULATION]\n",
    "                    # =============================================================================\n",
    "                    \"strain_compute\" : True,\n",
    "                    \"tolerance_strain_refinement\" : [[0.6,0.5,0.4,0.3,0.2,0.1],\n",
    "                                                     [0.6,0.5,0.4,0.3,0.2,0.1]],\n",
    "                    \"free_parameters\" : [\"b\",\"c\",\"alpha\",\"beta\",\"gamma\"],\n",
    "                    \n",
    "                    # =============================================================================\n",
    "                    # # [Additional settings]\n",
    "                    # =============================================================================\n",
    "                    \"residues_threshold\":0.25,\n",
    "                    \"nb_spots_global_threshold\":8,\n",
    "                    \"nb_spots_consider\" : 500,\n",
    "                    # User defined orientation matrix supplied in a file\n",
    "                    \"use_om_user\" : False,\n",
    "                    \"path_user_OM\" : \"\",\n",
    "                    }    \n",
    "    # =============================================================================\n",
    "    # END OF USER INPUT\n",
    "    # ============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa0a493-5a72-4c14-be8d-01d54f073260",
   "metadata": {},
   "source": [
    "## Generate dataset for neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebdbbba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjustText library not installed\n",
      "save directory is : /home/esrf/purushot/Desktop/LaueNN_tutorial/TUtorial_1/LaueNN_script/GaN_Si_MultiMaterial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating HKL objects\n",
      "Removing harmonics and building equivalent HKL objects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:01<00:01,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying if two different HKL class have same angular distribution (can be very time consuming depending on the symmetry)\n",
      "Finalizing the HKL objects\n",
      "Saved class HKL data in : /home/esrf/purushot/Desktop/LaueNN_tutorial/TUtorial_1/LaueNN_script/GaN_Si_MultiMaterial//classhkl_data_GaN.pickle\n",
      "Generating HKL objects\n",
      "Removing harmonics and building equivalent HKL objects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying if two different HKL class have same angular distribution (can be very time consuming depending on the symmetry)\n",
      "Finalizing the HKL objects\n",
      "Saved class HKL data in : /home/esrf/purushot/Desktop/LaueNN_tutorial/TUtorial_1/LaueNN_script/GaN_Si_MultiMaterial//classhkl_data_Si.pickle\n",
      "Generating training_data and saving them\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 3/2500 [00:07<1:49:49,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating testing_data and saving them\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 1/500 [00:02<16:40,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaN material index length: 139\n",
      "Si material index length: 40\n",
      "Class ID and frequency; check for data imbalance and select appropriate LOSS function for training the model\n",
      "Most common classhkl elements in GaN are:\n",
      "[(62, 3535), (83, 3531), (43, 3504), (64, 3461), (102, 3446), (85, 3440), (126, 3439), (45, 3432), (20, 3416), (17, 3392), (24, 3324), (105, 3289), (47, 3257), (25, 3219), (68, 3179), (48, 3115), (127, 3075), (87, 3032), (69, 3028), (88, 2899), (109, 2885), (110, 2712), (32, 2690), (129, 2625), (75, 2523), (33, 2428), (55, 2428), (130, 2418), (76, 2243), (115, 2215), (95, 2107), (116, 1912), (84, 1776), (101, 1757), (3, 1752), (18, 1742), (103, 1742), (61, 1732), (82, 1732), (59, 1731), (99, 1728), (2, 1726), (21, 1725), (124, 1721), (15, 1715), (42, 1715), (44, 1714), (136, 1708), (81, 1708), (41, 1704), (60, 1695), (125, 1693), (5, 1684), (16, 1679), (65, 1666), (14, 1633), (6, 1607), (106, 1559), (26, 1466), (100, 1456), (49, 1399), (70, 1346), (54, 1345), (19, 1340), (89, 1270), (63, 1263), (10, 1248), (94, 1230), (104, 1097), (131, 1061), (34, 1049), (31, 909), (1, 855), (0, 787), (117, 776), (114, 734), (22, 666), (46, 659), (27, 614), (50, 611), (66, 611), (86, 566), (71, 564), (35, 546), (107, 537), (90, 526), (56, 519), (111, 483), (128, 459), (77, 454), (132, 410), (28, 407), (96, 406), (118, 384), (51, 377), (137, 347), (4, 343), (72, 338), (36, 333), (91, 331), (7, 312), (78, 284), (112, 275), (11, 274), (13, 262), (133, 223), (119, 216), (8, 206), (29, 198), (52, 191), (73, 154), (37, 144), (57, 127), (92, 119), (97, 112), (113, 99), (120, 99), (9, 97), (134, 69), (38, 20), (79, 12), (53, 7), (93, 1)]\n",
      "Most common classhkl elements in Si are:\n",
      "[(167, 3559), (148, 3430), (156, 2769), (164, 2512), (168, 1855), (162, 1814), (169, 1792), (175, 1755), (150, 1743), (145, 1737), (177, 1735), (153, 1733), (146, 1710), (143, 1678), (142, 1632), (171, 1556), (151, 1497), (161, 1443), (166, 1074), (158, 1013), (173, 977), (144, 954), (140, 806), (141, 535), (139, 446), (147, 435), (176, 424), (178, 144), (152, 133), (149, 90)]\n",
      "HKL : [-1.  3.  3.]; occurance : 3535; NN_weights : 1; material: GaN\n",
      "HKL : [-1.  3.  4.]; occurance : 3531; NN_weights : 1; material: GaN\n",
      "HKL : [-1.  3.  2.]; occurance : 3504; NN_weights : 1; material: GaN\n",
      "HKL : [-1.  4.  3.]; occurance : 3461; NN_weights : 1; material: GaN\n",
      "HKL : [-1.  3.  5.]; occurance : 3446; NN_weights : 1; material: GaN\n",
      "HKL : [-1.  4.  4.]; occurance : 3440; NN_weights : 1; material: GaN\n",
      "HKL : [-1.  3.  6.]; occurance : 3439; NN_weights : 1; material: GaN\n",
      "HKL : [-1.  4.  2.]; occurance : 3432; NN_weights : 1; material: GaN\n",
      "HKL : [-1.  4.  1.]; occurance : 3416; NN_weights : 1; material: GaN\n",
      "HKL : [-1.  3.  1.]; occurance : 3392; NN_weights : 1; material: GaN\n",
      "HKL : [-2.  5.  1.]; occurance : 3324; NN_weights : 1; material: GaN\n",
      "HKL : [-1.  4.  5.]; occurance : 3289; NN_weights : 1; material: GaN\n",
      "HKL : [-2.  5.  2.]; occurance : 3257; NN_weights : 1; material: GaN\n",
      "HKL : [-1.  5.  1.]; occurance : 3219; NN_weights : 1; material: GaN\n",
      "HKL : [-2.  5.  3.]; occurance : 3179; NN_weights : 1; material: GaN\n",
      "HKL : [-1.  5.  2.]; occurance : 3115; NN_weights : 1; material: GaN\n",
      "HKL : [-1.  4.  6.]; occurance : 3075; NN_weights : 1; material: GaN\n",
      "HKL : [-2.  5.  4.]; occurance : 3032; NN_weights : 1; material: GaN\n",
      "HKL : [-1.  5.  3.]; occurance : 3028; NN_weights : 1; material: GaN\n",
      "HKL : [-1.  5.  4.]; occurance : 2899; NN_weights : 1; material: GaN\n",
      "HKL : [-2.  5.  5.]; occurance : 2885; NN_weights : 1; material: GaN\n",
      "HKL : [-1.  5.  5.]; occurance : 2712; NN_weights : 1; material: GaN\n",
      "HKL : [-2.  6.  1.]; occurance : 2690; NN_weights : 1; material: GaN\n",
      "HKL : [-2.  5.  6.]; occurance : 2625; NN_weights : 1; material: GaN\n",
      "HKL : [-2.  6.  3.]; occurance : 2523; NN_weights : 1; material: GaN\n",
      "HKL : [-1.  6.  1.]; occurance : 2428; NN_weights : 1; material: GaN\n",
      "HKL : [-1.  6.  2.]; occurance : 2428; NN_weights : 1; material: GaN\n",
      "HKL : [-1.  5.  6.]; occurance : 2418; NN_weights : 1; material: GaN\n",
      "HKL : [-1.  6.  3.]; occurance : 2243; NN_weights : 1; material: GaN\n",
      "HKL : [-2.  6.  5.]; occurance : 2215; NN_weights : 1; material: GaN\n",
      "HKL : [-1.  6.  4.]; occurance : 2107; NN_weights : 1; material: GaN\n",
      "HKL : [-1.  6.  5.]; occurance : 1912; NN_weights : 1; material: GaN\n",
      "HKL : [0. 3. 4.]; occurance : 1776; NN_weights : 2; material: GaN\n",
      "HKL : [0. 2. 5.]; occurance : 1757; NN_weights : 2; material: GaN\n",
      "HKL : [-1.  4.  0.]; occurance : 1752; NN_weights : 2; material: GaN\n",
      "HKL : [0. 3. 1.]; occurance : 1742; NN_weights : 2; material: GaN\n",
      "HKL : [0. 3. 5.]; occurance : 1742; NN_weights : 2; material: GaN\n",
      "HKL : [0. 2. 3.]; occurance : 1732; NN_weights : 2; material: GaN\n",
      "HKL : [-1.  2.  4.]; occurance : 1732; NN_weights : 2; material: GaN\n",
      "HKL : [0. 1. 3.]; occurance : 1731; NN_weights : 2; material: GaN\n",
      "HKL : [0. 1. 5.]; occurance : 1728; NN_weights : 2; material: GaN\n",
      "HKL : [-1.  3.  0.]; occurance : 1726; NN_weights : 2; material: GaN\n",
      "HKL : [0. 4. 1.]; occurance : 1725; NN_weights : 2; material: GaN\n",
      "HKL : [0. 1. 6.]; occurance : 1721; NN_weights : 2; material: GaN\n",
      "HKL : [-1.  2.  1.]; occurance : 1715; NN_weights : 2; material: GaN\n",
      "HKL : [-1.  2.  2.]; occurance : 1715; NN_weights : 2; material: GaN\n",
      "HKL : [0. 3. 2.]; occurance : 1714; NN_weights : 2; material: GaN\n",
      "HKL : [-1.  6.  6.]; occurance : 1708; NN_weights : 2; material: GaN\n",
      "HKL : [0. 1. 4.]; occurance : 1708; NN_weights : 2; material: GaN\n",
      "HKL : [0. 1. 2.]; occurance : 1704; NN_weights : 2; material: GaN\n",
      "HKL : [-1.  2.  3.]; occurance : 1695; NN_weights : 2; material: GaN\n",
      "HKL : [-1.  2.  6.]; occurance : 1693; NN_weights : 2; material: GaN\n",
      "HKL : [-2.  5.  0.]; occurance : 1684; NN_weights : 2; material: GaN\n",
      "HKL : [0. 2. 1.]; occurance : 1679; NN_weights : 2; material: GaN\n",
      "HKL : [0. 4. 3.]; occurance : 1666; NN_weights : 2; material: GaN\n",
      "HKL : [0. 1. 1.]; occurance : 1633; NN_weights : 2; material: GaN\n",
      "HKL : [-1.  5.  0.]; occurance : 1607; NN_weights : 2; material: GaN\n",
      "HKL : [0. 4. 5.]; occurance : 1559; NN_weights : 2; material: GaN\n",
      "HKL : [0. 5. 1.]; occurance : 1466; NN_weights : 2; material: GaN\n",
      "HKL : [-1.  2.  5.]; occurance : 1456; NN_weights : 2; material: GaN\n",
      "HKL : [0. 5. 2.]; occurance : 1399; NN_weights : 2; material: GaN\n",
      "HKL : [0. 5. 3.]; occurance : 1346; NN_weights : 2; material: GaN\n",
      "HKL : [-3.  6.  2.]; occurance : 1345; NN_weights : 2; material: GaN\n",
      "HKL : [-2.  4.  1.]; occurance : 1340; NN_weights : 2; material: GaN\n",
      "HKL : [0. 5. 4.]; occurance : 1270; NN_weights : 2; material: GaN\n",
      "HKL : [-2.  4.  3.]; occurance : 1263; NN_weights : 2; material: GaN\n",
      "HKL : [-1.  6.  0.]; occurance : 1248; NN_weights : 2; material: GaN\n",
      "HKL : [-3.  6.  4.]; occurance : 1230; NN_weights : 2; material: GaN\n",
      "HKL : [-2.  4.  5.]; occurance : 1097; NN_weights : 3; material: GaN\n",
      "HKL : [0. 5. 6.]; occurance : 1061; NN_weights : 3; material: GaN\n",
      "HKL : [0. 6. 1.]; occurance : 1049; NN_weights : 3; material: GaN\n",
      "HKL : [-3.  6.  1.]; occurance : 909; NN_weights : 3; material: GaN\n",
      "HKL : [-1.  2.  0.]; occurance : 855; NN_weights : 4; material: GaN\n",
      "HKL : [0. 1. 0.]; occurance : 787; NN_weights : 4; material: GaN\n",
      "HKL : [0. 6. 5.]; occurance : 776; NN_weights : 4; material: GaN\n",
      "HKL : [-3.  6.  5.]; occurance : 734; NN_weights : 4; material: GaN\n",
      "HKL : [3. 4. 1.]; occurance : 666; NN_weights : 5; material: GaN\n",
      "HKL : [3. 4. 2.]; occurance : 659; NN_weights : 5; material: GaN\n",
      "HKL : [2. 5. 1.]; occurance : 614; NN_weights : 5; material: GaN\n",
      "HKL : [2. 5. 2.]; occurance : 611; NN_weights : 5; material: GaN\n",
      "HKL : [3. 4. 3.]; occurance : 611; NN_weights : 5; material: GaN\n",
      "HKL : [3. 4. 4.]; occurance : 566; NN_weights : 6; material: GaN\n",
      "HKL : [2. 5. 3.]; occurance : 564; NN_weights : 6; material: GaN\n",
      "HKL : [1. 6. 1.]; occurance : 546; NN_weights : 6; material: GaN\n",
      "HKL : [3. 4. 5.]; occurance : 537; NN_weights : 6; material: GaN\n",
      "HKL : [2. 5. 4.]; occurance : 526; NN_weights : 6; material: GaN\n",
      "HKL : [1. 6. 2.]; occurance : 519; NN_weights : 6; material: GaN\n",
      "HKL : [2. 5. 5.]; occurance : 483; NN_weights : 7; material: GaN\n",
      "HKL : [3. 4. 6.]; occurance : 459; NN_weights : 7; material: GaN\n",
      "HKL : [1. 6. 3.]; occurance : 454; NN_weights : 7; material: GaN\n",
      "HKL : [2. 5. 6.]; occurance : 410; NN_weights : 8; material: GaN\n",
      "HKL : [3. 5. 1.]; occurance : 407; NN_weights : 8; material: GaN\n",
      "HKL : [1. 6. 4.]; occurance : 406; NN_weights : 8; material: GaN\n",
      "HKL : [1. 6. 5.]; occurance : 384; NN_weights : 9; material: GaN\n",
      "HKL : [3. 5. 2.]; occurance : 377; NN_weights : 9; material: GaN\n",
      "HKL : [1. 6. 6.]; occurance : 347; NN_weights : 10; material: GaN\n",
      "HKL : [3. 4. 0.]; occurance : 343; NN_weights : 10; material: GaN\n",
      "HKL : [3. 5. 3.]; occurance : 338; NN_weights : 10; material: GaN\n",
      "HKL : [2. 6. 1.]; occurance : 333; NN_weights : 10; material: GaN\n",
      "HKL : [3. 5. 4.]; occurance : 331; NN_weights : 10; material: GaN\n",
      "HKL : [2. 5. 0.]; occurance : 312; NN_weights : 11; material: GaN\n",
      "HKL : [2. 6. 3.]; occurance : 284; NN_weights : 12; material: GaN\n",
      "HKL : [3. 5. 5.]; occurance : 275; NN_weights : 12; material: GaN\n",
      "HKL : [1. 6. 0.]; occurance : 274; NN_weights : 12; material: GaN\n",
      "HKL : [0. 0. 1.]; occurance : 262; NN_weights : 13; material: GaN\n",
      "HKL : [3. 5. 6.]; occurance : 223; NN_weights : 15; material: GaN\n",
      "HKL : [2. 6. 5.]; occurance : 216; NN_weights : 16; material: GaN\n",
      "HKL : [3. 5. 0.]; occurance : 206; NN_weights : 17; material: GaN\n",
      "HKL : [4. 5. 1.]; occurance : 198; NN_weights : 17; material: GaN\n",
      "HKL : [4. 5. 2.]; occurance : 191; NN_weights : 18; material: GaN\n",
      "HKL : [4. 5. 3.]; occurance : 154; NN_weights : 23; material: GaN\n",
      "HKL : [3. 6. 1.]; occurance : 144; NN_weights : 24; material: GaN\n",
      "HKL : [3. 6. 2.]; occurance : 127; NN_weights : 28; material: GaN\n",
      "HKL : [4. 5. 4.]; occurance : 119; NN_weights : 29; material: GaN\n",
      "HKL : [3. 6. 4.]; occurance : 112; NN_weights : 31; material: GaN\n",
      "HKL : [1. 3. 5.]; occurance : 3559; NN_weights : 1; material: Si\n",
      "HKL : [1. 2. 3.]; occurance : 3430; NN_weights : 1; material: Si\n",
      "HKL : [1. 3. 4.]; occurance : 2769; NN_weights : 1; material: Si\n",
      "HKL : [1. 2. 5.]; occurance : 2512; NN_weights : 1; material: Si\n",
      "HKL : [2. 3. 5.]; occurance : 1855; NN_weights : 1; material: Si\n",
      "HKL : [1. 1. 5.]; occurance : 1814; NN_weights : 1; material: Si\n",
      "HKL : [3. 3. 5.]; occurance : 1792; NN_weights : 1; material: Si\n",
      "HKL : [1. 5. 5.]; occurance : 1755; NN_weights : 2; material: Si\n",
      "HKL : [1. 3. 3.]; occurance : 1743; NN_weights : 2; material: Si\n",
      "HKL : [0. 1. 3.]; occurance : 1737; NN_weights : 2; material: Si\n",
      "HKL : [3. 5. 5.]; occurance : 1735; NN_weights : 2; material: Si\n",
      "HKL : [1. 1. 4.]; occurance : 1733; NN_weights : 2; material: Si\n",
      "HKL : [1. 1. 3.]; occurance : 1710; NN_weights : 2; material: Si\n",
      "HKL : [1. 1. 2.]; occurance : 1678; NN_weights : 2; material: Si\n",
      "HKL : [0. 1. 2.]; occurance : 1632; NN_weights : 2; material: Si\n",
      "HKL : [1. 4. 5.]; occurance : 1556; NN_weights : 2; material: Si\n",
      "HKL : [2. 3. 3.]; occurance : 1497; NN_weights : 2; material: Si\n",
      "HKL : [0. 1. 5.]; occurance : 1443; NN_weights : 2; material: Si\n",
      "HKL : [0. 3. 5.]; occurance : 1074; NN_weights : 3; material: Si\n",
      "HKL : [3. 3. 4.]; occurance : 1013; NN_weights : 3; material: Si\n",
      "HKL : [3. 4. 5.]; occurance : 977; NN_weights : 3; material: Si\n",
      "HKL : [1. 2. 2.]; occurance : 954; NN_weights : 3; material: Si\n",
      "HKL : [0. 1. 1.]; occurance : 806; NN_weights : 4; material: Si\n",
      "HKL : [1. 1. 1.]; occurance : 535; NN_weights : 6; material: Si\n",
      "HKL : [0. 0. 1.]; occurance : 446; NN_weights : 7; material: Si\n",
      "HKL : [0. 2. 3.]; occurance : 435; NN_weights : 8; material: Si\n",
      "HKL : [2. 5. 5.]; occurance : 424; NN_weights : 8; material: Si\n",
      "HKL : [4. 5. 5.]; occurance : 144; NN_weights : 24; material: Si\n",
      "HKL : [0. 1. 4.]; occurance : 133; NN_weights : 26; material: Si\n",
      "35 classes removed from the classHKL object [removal frequency: [100, 100]] (before:179, now:144)\n",
      "35 classes removed from the classHKL object [removal frequency: [100, 100]] (before:179, now:144)\n",
      "Saved class weights data\n"
     ]
    }
   ],
   "source": [
    "generate_dataset_MM = True\n",
    "\n",
    "if __name__ == '__main__':     #enclosing required because of multiprocessing\n",
    "    # global_path: path where all model related files will be saved\n",
    "    global_path = input_params[\"global_path\"]\n",
    "    \n",
    "    if generate_dataset_MM:\n",
    "        import os\n",
    "        from tqdm import trange\n",
    "        ## if LaueToolsNN is properly installed\n",
    "        from lauetoolsnn.utils_lauenn import generate_classHKL, generate_multimat_dataset, \\\n",
    "                                        rmv_freq_class_MM, get_multimaterial_detail\n",
    "                            \n",
    "        material_= input_params[\"material_\"]\n",
    "        n = input_params[\"hkl_max_identify\"]\n",
    "        maximum_angle_to_search = input_params[\"maximum_angle_to_search\"]\n",
    "        step_for_binning = input_params[\"step_for_binning\"]\n",
    "        nb_grains_per_lp = input_params[\"nb_grains_per_lp\"]\n",
    "        grains_nb_simulate = input_params[\"grains_nb_simulate\"]\n",
    "        detectorparameters = input_params[\"detectorparameters\"]\n",
    "        pixelsize = input_params[\"pixelsize\"]\n",
    "        emax = input_params[\"emax\"]\n",
    "        emin = input_params[\"emin\"]\n",
    "        symm_ = input_params[\"symmetry\"]\n",
    "        SG = input_params[\"SG\"]\n",
    "        \n",
    "        if len(material_) > 1:\n",
    "            prefix_mat = material_[0]\n",
    "            for ino, imat in enumerate(material_):\n",
    "                if ino == 0:\n",
    "                    continue\n",
    "                prefix_mat = prefix_mat + \"_\" + imat\n",
    "        else:\n",
    "            prefix_mat = material_[0]\n",
    "        \n",
    "        save_directory = os.path.join(global_path,prefix_mat+input_params[\"prefix\"])\n",
    "\n",
    "        print(\"save directory is : \"+save_directory)\n",
    "        if not os.path.exists(save_directory):\n",
    "            os.makedirs(save_directory)\n",
    "        \n",
    "        ## get unit cell parameters and other details required for simulating Laue patterns\n",
    "        rules, symmetry, lattice_material, \\\n",
    "                                crystal, SG = get_multimaterial_detail(material_, SG, symm_)\n",
    "            \n",
    "        ### generate_classHKL_multimat\n",
    "        ## procedure for generation of GROUND TRUTH classes\n",
    "        # general_diff_cond = True will eliminate the hkl index that does not satisfy the general reflection conditions\n",
    "        # mat_listHKl: provide a numpy array of hkls to be added to the list of output hkl\n",
    "        for ino in trange(len(material_)):\n",
    "            generate_classHKL(n[ino], rules[ino], lattice_material[ino], \\\n",
    "                              symmetry[ino], material_[ino], \\\n",
    "                              crystal=crystal[ino], SG=SG[ino], general_diff_cond=False,\n",
    "                              save_directory=save_directory, write_to_console=print, \\\n",
    "                              ang_maxx = maximum_angle_to_search, \\\n",
    "                              step = step_for_binning, mat_listHKl = None)\n",
    "        \n",
    "        # ## Generate Training and Testing dataset only for the output classes (Laue spot hkls) calculated in the Step 3\n",
    "        # ### Uses multiprocessing library\n",
    "        ############ GENERATING MULTI MATERIAL TRAINING DATA ##############\n",
    "        # data_realism =True ; will introduce noise and partial Laue patterns in the training dataset\n",
    "        # modelp can have either \"random\" for random orientation generation or \"uniform\" for uniform orientation generation\n",
    "        # include_scm (if True; misorientation_angle parameter need to be defined): this parameter introduces misoriented crystal of \n",
    "        # specific angle along a crystal axis in the training dataset    \n",
    "        generate_multimat_dataset(material_=material_, \n",
    "                                 ang_maxx=maximum_angle_to_search,\n",
    "                                 step=step_for_binning, \n",
    "                                 nb_grains=nb_grains_per_lp, \n",
    "                                 grains_nb_simulate=grains_nb_simulate, \n",
    "                                 data_realism = True, \n",
    "                                 detectorparameters=detectorparameters, \n",
    "                                 pixelsize=pixelsize, \n",
    "                                 type_=\"training_data\",\n",
    "                                 var0 = 1, \n",
    "                                 dim1=input_params[\"dim1\"], \n",
    "                                 dim2=input_params[\"dim2\"], \n",
    "                                 removeharmonics=1, \n",
    "                                 save_directory=save_directory,\n",
    "                                 write_to_console=print, \n",
    "                                 emin=emin, \n",
    "                                 emax=emax, \n",
    "                                 modelp = input_params[\"orientation_generation\"],\n",
    "                                 general_diff_rules = False, \n",
    "                                 crystal = crystal,)\n",
    "        \n",
    "        ############ GENERATING TESTING DATA ##############\n",
    "        factor = 5 # validation split for the training dataset  --> corresponds to 20% of total training dataset\n",
    "        generate_multimat_dataset(material_=material_, \n",
    "                                 ang_maxx=maximum_angle_to_search,\n",
    "                                 step=step_for_binning, \n",
    "                                 nb_grains=nb_grains_per_lp, \n",
    "                                 grains_nb_simulate=grains_nb_simulate//factor, \n",
    "                                 data_realism = True, \n",
    "                                 detectorparameters=detectorparameters, \n",
    "                                 pixelsize=pixelsize, \n",
    "                                 type_=\"testing_data\",\n",
    "                                 var0 = 1, \n",
    "                                 dim1=input_params[\"dim1\"], \n",
    "                                 dim2=input_params[\"dim2\"], \n",
    "                                 removeharmonics=1, \n",
    "                                 save_directory=save_directory,\n",
    "                                 write_to_console=print, \n",
    "                                 emin=emin, \n",
    "                                 emax=emax, \n",
    "                                 modelp = input_params[\"orientation_generation\"],\n",
    "                                 general_diff_rules = False, \n",
    "                                 crystal = crystal,)\n",
    "        ### Updating the ClassHKL list by removing the non-common HKL or less frequent HKL from the list\n",
    "        ## The non-common HKL can occur as a result of the detector position and energy used\n",
    "        # freq_rmv: remove output hkl if the training dataset has less tha 100 occurances of the considered hkl (freq_rmv1 for second phase)\n",
    "        # Weights (penalty during training) are also calculated based on the occurance\n",
    "\n",
    "        freq_rmv = input_params[\"classes_with_frequency_to_remove\"]\n",
    "        elements = input_params[\"desired_classes_output\"]\n",
    "        list_hkl_keep = input_params[\"list_hkl_keep\"]\n",
    "        \n",
    "        rmv_freq_class_MM(freq_rmv = freq_rmv, elements = elements,\n",
    "                          save_directory = save_directory, material_ = material_,\n",
    "                          write_to_console = print, progress=None, qapp=None,\n",
    "                          list_hkl_keep = list_hkl_keep)\n",
    "        ## End of data generation for Neural network training: all files are saved in the same folder \n",
    "        ## to be later used for training and prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c4edfe-1ecc-4707-8a21-9b646c98f2cd",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dda2478-2ff0-4d2a-87d1-f06a7d330446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n",
      "save directory is : /home/esrf/purushot/Desktop/LaueNN_tutorial/TUtorial_1/LaueNN_script/GaN_Si_MultiMaterial\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 900)               810900    \n",
      "                                                                 \n",
      " activation (Activation)     (None, 900)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 900)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1530)              1378530   \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 1530)              0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1530)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2160)              3306960   \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 2160)              0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 2160)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 144)               311184    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,807,574\n",
      "Trainable params: 5,807,574\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Number of spots in a batch of 100 files : 9285\n",
      "Min, Max class ID is 0, 143\n",
      "Epoch 1/8\n",
      "25/25 [==============================] - 25s 991ms/step - loss: 7.0579 - fn: 201561.0000 - fp: 368.0000 - tn: 31124014.0000 - tp: 16092.0000 - precision: 0.9776 - accuracy: 0.0739 - val_loss: 1.1050 - val_fn: 19998.0000 - val_fp: 42.0000 - val_tn: 6190714.0000 - val_tp: 23294.0000 - val_precision: 0.9982 - val_accuracy: 0.5381\n",
      "Epoch 2/8\n",
      "25/25 [==============================] - 15s 618ms/step - loss: 1.5557 - fn: 67635.0000 - fp: 3629.0000 - tn: 31120752.0000 - tp: 150018.0000 - precision: 0.9764 - accuracy: 0.6893 - val_loss: 0.3946 - val_fn: 4856.0000 - val_fp: 323.0000 - val_tn: 6190433.0000 - val_tp: 38436.0000 - val_precision: 0.9917 - val_accuracy: 0.8878\n",
      "Epoch 3/8\n",
      "25/25 [==============================] - 17s 702ms/step - loss: 0.8262 - fn: 30642.0000 - fp: 4263.0000 - tn: 31120112.0000 - tp: 187011.0000 - precision: 0.9777 - accuracy: 0.8592 - val_loss: 0.3070 - val_fn: 3588.0000 - val_fp: 427.0000 - val_tn: 6190329.0000 - val_tp: 39704.0000 - val_precision: 0.9894 - val_accuracy: 0.9171\n",
      "Epoch 4/8\n",
      "25/25 [==============================] - 16s 673ms/step - loss: 0.6200 - fn: 23285.0000 - fp: 4276.0000 - tn: 31120106.0000 - tp: 194368.0000 - precision: 0.9785 - accuracy: 0.8930 - val_loss: 0.2619 - val_fn: 3011.0000 - val_fp: 466.0000 - val_tn: 6190290.0000 - val_tp: 40281.0000 - val_precision: 0.9886 - val_accuracy: 0.9304\n",
      "Epoch 5/8\n",
      "25/25 [==============================] - 16s 666ms/step - loss: 0.5091 - fn: 19429.0000 - fp: 4109.0000 - tn: 31120272.0000 - tp: 198224.0000 - precision: 0.9797 - accuracy: 0.9107 - val_loss: 0.2320 - val_fn: 2659.0000 - val_fp: 459.0000 - val_tn: 6190297.0000 - val_tp: 40633.0000 - val_precision: 0.9888 - val_accuracy: 0.9386\n",
      "Epoch 6/8\n",
      "25/25 [==============================] - 16s 647ms/step - loss: 0.4339 - fn: 16850.0000 - fp: 3950.0000 - tn: 31120428.0000 - tp: 200803.0000 - precision: 0.9807 - accuracy: 0.9226 - val_loss: 0.2162 - val_fn: 2398.0000 - val_fp: 509.0000 - val_tn: 6190247.0000 - val_tp: 40894.0000 - val_precision: 0.9877 - val_accuracy: 0.9446\n",
      "Epoch 7/8\n",
      "25/25 [==============================] - 16s 657ms/step - loss: 0.3784 - fn: 14882.0000 - fp: 3883.0000 - tn: 31120496.0000 - tp: 202771.0000 - precision: 0.9812 - accuracy: 0.9316 - val_loss: 0.2030 - val_fn: 2206.0000 - val_fp: 524.0000 - val_tn: 6190232.0000 - val_tp: 41086.0000 - val_precision: 0.9874 - val_accuracy: 0.9490\n",
      "Epoch 8/8\n",
      "25/25 [==============================] - 16s 662ms/step - loss: 0.3416 - fn: 13543.0000 - fp: 3755.0000 - tn: 31120628.0000 - tp: 204110.0000 - precision: 0.9819 - accuracy: 0.9378 - val_loss: 0.1958 - val_fn: 2086.0000 - val_fp: 523.0000 - val_tn: 6190233.0000 - val_tp: 41206.0000 - val_precision: 0.9875 - val_accuracy: 0.9518\n",
      "Saved model to disk\n",
      "Training Accuracy: 0.9377771019935608\n",
      "Training Loss: 0.3416038155555725\n",
      "Validation Accuracy: 0.9518156051635742\n",
      "Validation Loss: 0.19582803547382355\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        20\n",
      "           1       0.94      1.00      0.97        17\n",
      "           2       1.00      1.00      1.00        30\n",
      "           3       0.97      0.87      0.92        38\n",
      "           4       1.00      1.00      1.00         5\n",
      "           5       1.00      1.00      1.00        31\n",
      "           6       1.00      0.94      0.97        34\n",
      "           7       0.50      0.75      0.60         4\n",
      "           8       0.75      1.00      0.86         3\n",
      "           9       1.00      1.00      1.00        24\n",
      "          10       1.00      1.00      1.00         2\n",
      "          11       1.00      1.00      1.00         4\n",
      "          12       0.97      0.97      0.97        31\n",
      "          13       0.97      1.00      0.98        30\n",
      "          14       1.00      1.00      1.00        35\n",
      "          15       0.97      0.97      0.97        64\n",
      "          16       0.98      1.00      0.99        40\n",
      "          17       1.00      0.88      0.93        24\n",
      "          18       1.00      0.93      0.96        70\n",
      "          19       1.00      0.97      0.99        40\n",
      "          20       0.85      1.00      0.92        11\n",
      "          21       0.98      1.00      0.99        63\n",
      "          22       1.00      0.97      0.98        66\n",
      "          23       1.00      1.00      1.00        34\n",
      "          24       0.83      1.00      0.91         5\n",
      "          25       0.78      1.00      0.88         7\n",
      "          26       0.57      1.00      0.73         4\n",
      "          27       0.94      0.88      0.91        17\n",
      "          28       0.94      1.00      0.97        50\n",
      "          29       0.98      0.95      0.96        55\n",
      "          30       1.00      0.96      0.98        24\n",
      "          31       1.00      1.00      1.00         4\n",
      "          32       0.50      1.00      0.67         1\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       0.97      0.97      0.97        33\n",
      "          35       0.96      1.00      0.98        26\n",
      "          36       0.98      0.94      0.96        69\n",
      "          37       0.97      0.97      0.97        35\n",
      "          38       0.94      0.98      0.96        66\n",
      "          39       0.89      0.89      0.89         9\n",
      "          40       1.00      0.90      0.95        60\n",
      "          41       1.00      0.97      0.98        66\n",
      "          42       1.00      1.00      1.00        35\n",
      "          43       0.90      1.00      0.95         9\n",
      "          44       0.67      1.00      0.80         4\n",
      "          45       0.50      1.00      0.67         2\n",
      "          46       0.95      1.00      0.97        19\n",
      "          47       1.00      0.93      0.97        46\n",
      "          48       1.00      1.00      1.00         4\n",
      "          49       0.67      1.00      0.80         2\n",
      "          50       0.96      0.96      0.96        28\n",
      "          51       0.92      0.96      0.94        24\n",
      "          52       0.93      1.00      0.97        28\n",
      "          53       0.97      0.94      0.95        63\n",
      "          54       1.00      0.91      0.95        22\n",
      "          55       0.98      0.94      0.96        66\n",
      "          56       0.97      0.89      0.93        35\n",
      "          57       0.78      1.00      0.88         7\n",
      "          58       1.00      0.96      0.98        56\n",
      "          59       1.00      0.97      0.98        61\n",
      "          60       0.96      0.93      0.94        27\n",
      "          61       0.80      0.80      0.80         5\n",
      "          62       0.75      1.00      0.86         3\n",
      "          63       1.00      1.00      1.00         2\n",
      "          64       0.94      1.00      0.97        45\n",
      "          65       0.95      0.95      0.95        41\n",
      "          66       0.71      0.83      0.77         6\n",
      "          67       1.00      1.00      1.00         1\n",
      "          68       1.00      0.92      0.96        25\n",
      "          69       1.00      1.00      1.00        28\n",
      "          70       0.96      0.96      0.96        51\n",
      "          71       0.97      0.88      0.92        34\n",
      "          72       1.00      0.95      0.98        62\n",
      "          73       0.78      1.00      0.88         7\n",
      "          74       0.98      0.96      0.97        51\n",
      "          75       0.95      0.98      0.96        56\n",
      "          76       1.00      0.96      0.98        28\n",
      "          77       0.78      1.00      0.88         7\n",
      "          78       0.67      1.00      0.80         2\n",
      "          79       1.00      1.00      1.00         4\n",
      "          80       0.95      1.00      0.98        21\n",
      "          81       0.98      0.95      0.97        44\n",
      "          82       1.00      1.00      1.00         7\n",
      "          83       0.00      0.00      0.00         0\n",
      "          84       0.89      1.00      0.94        31\n",
      "          85       1.00      1.00      1.00        23\n",
      "          86       1.00      1.00      1.00        35\n",
      "          87       0.94      0.96      0.95        52\n",
      "          88       1.00      0.97      0.98        31\n",
      "          89       1.00      1.00      1.00        18\n",
      "          90       0.96      0.92      0.94        53\n",
      "          91       0.97      0.97      0.97        36\n",
      "          92       0.83      1.00      0.91         5\n",
      "          93       0.93      0.96      0.94        52\n",
      "          94       1.00      0.96      0.98        55\n",
      "          95       0.83      1.00      0.91         5\n",
      "          96       0.75      1.00      0.86         3\n",
      "          97       1.00      1.00      1.00        18\n",
      "          98       1.00      0.98      0.99        45\n",
      "          99       1.00      0.93      0.96        43\n",
      "         100       1.00      1.00      1.00        12\n",
      "         101       0.75      1.00      0.86         6\n",
      "         102       0.75      1.00      0.86         3\n",
      "         103       0.97      0.97      0.97        29\n",
      "         104       1.00      1.00      1.00        29\n",
      "         105       0.96      1.00      0.98        52\n",
      "         106       0.88      0.98      0.93        47\n",
      "         107       1.00      0.50      0.67         4\n",
      "         108       0.97      0.95      0.96        40\n",
      "         109       0.98      0.94      0.96        52\n",
      "         110       0.92      1.00      0.96        22\n",
      "         111       1.00      1.00      1.00         7\n",
      "         112       0.60      1.00      0.75         3\n",
      "         113       1.00      0.91      0.96        35\n",
      "         114       1.00      0.80      0.89         5\n",
      "         115       1.00      1.00      1.00        10\n",
      "         116       0.82      0.90      0.86        10\n",
      "         117       0.89      1.00      0.94         8\n",
      "         118       0.96      0.96      0.96        25\n",
      "         119       0.90      0.96      0.93        27\n",
      "         120       0.87      1.00      0.93        13\n",
      "         121       1.00      0.93      0.96        29\n",
      "         122       1.00      0.97      0.98        30\n",
      "         123       1.00      1.00      1.00         7\n",
      "         124       0.98      1.00      0.99        49\n",
      "         125       0.93      0.96      0.94        26\n",
      "         126       1.00      1.00      1.00        21\n",
      "         127       1.00      1.00      1.00         2\n",
      "         128       1.00      0.97      0.98        32\n",
      "         129       0.95      0.90      0.92        41\n",
      "         130       0.92      0.92      0.92        13\n",
      "         131       0.91      0.95      0.93        21\n",
      "         132       0.97      1.00      0.99        33\n",
      "         133       0.97      0.93      0.95        41\n",
      "         134       0.90      1.00      0.95        18\n",
      "         135       1.00      0.94      0.97        52\n",
      "         136       0.96      0.93      0.95        28\n",
      "         137       0.95      0.95      0.95        21\n",
      "         138       0.96      0.89      0.92        27\n",
      "         139       0.85      0.92      0.88        12\n",
      "         140       0.87      0.93      0.90        29\n",
      "         141       1.00      1.00      1.00         6\n",
      "         142       0.87      1.00      0.93        20\n",
      "         143       0.33      1.00      0.50         1\n",
      "\n",
      "    accuracy                           0.96      3824\n",
      "   macro avg       0.92      0.96      0.93      3824\n",
      "weighted avg       0.97      0.96      0.96      3824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_network_MM = True\n",
    "\n",
    "if __name__ == '__main__':     #enclosing required because of multiprocessing\n",
    "\n",
    "    if train_network_MM:\n",
    "        import os\n",
    "        import numpy as np\n",
    "        import _pickle as cPickle\n",
    "        import itertools\n",
    "        from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "        import matplotlib.pyplot as plt\n",
    "        ## if LaueToolsNN is properly installed\n",
    "        from lauetoolsnn.utils_lauenn import array_generator, array_generator_verify, vali_array\n",
    "        from lauetoolsnn.NNmodels import model_arch_general, LoggingCallback\n",
    "        \n",
    "        material_= input_params[\"material_\"]\n",
    "        epochs = input_params[\"epochs\"]\n",
    "        batch_size = input_params[\"batch_size\"] \n",
    "        # ### number of files it will generate fro training\n",
    "        nb_grains_list = []\n",
    "        for ino, imat in enumerate(material_):\n",
    "            nb_grains_list.append(list(range(input_params[\"nb_grains_per_lp\"][ino]+1)))\n",
    "        list_permute = list(itertools.product(*nb_grains_list))\n",
    "        list_permute.pop(0)\n",
    "        print(len(list_permute)*input_params[\"grains_nb_simulate\"])\n",
    "        # ## Get material parameters \n",
    "        # ### Generates a folder with material name and gets material unit cell parameters and symmetry object \n",
    "        # from the get_material_detail function        \n",
    "        if len(material_) > 1:\n",
    "            prefix_mat = material_[0]\n",
    "            for ino, imat in enumerate(material_):\n",
    "                if ino == 0:\n",
    "                    continue\n",
    "                prefix_mat = prefix_mat + \"_\" + imat\n",
    "        else:\n",
    "            prefix_mat = material_[0]\n",
    "        \n",
    "        save_directory = os.path.join(global_path,prefix_mat+input_params[\"prefix\"])\n",
    "\n",
    "        print(\"save directory is : \"+save_directory)\n",
    "        if not os.path.exists(save_directory):\n",
    "            os.makedirs(save_directory)\n",
    "        \n",
    "        classhkl = np.load(save_directory+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_0\"]\n",
    "        angbins = np.load(save_directory+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_1\"]\n",
    "        loc_new = np.load(save_directory+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_2\"]\n",
    "        with open(save_directory+\"//class_weights.pickle\", \"rb\") as input_file:\n",
    "            class_weights = cPickle.load(input_file)\n",
    "        class_weights = class_weights[0]\n",
    "        \n",
    "        # ##  Training\n",
    "        # model save directory and filename\n",
    "        if len(material_) > 1:\n",
    "            prefix_mat = material_[0]\n",
    "            for ino, imat in enumerate(material_):\n",
    "                if ino == 0:\n",
    "                    continue\n",
    "                prefix_mat = prefix_mat + \"_\" + imat\n",
    "        else:\n",
    "            prefix_mat = material_[0]\n",
    "            \n",
    "        model_name = os.path.join(save_directory,\"model_\"+prefix_mat)\n",
    "        \n",
    "        # Define model and train\n",
    "        model = model_arch_general( len(angbins)-1, len(classhkl),\n",
    "                                    kernel_coeff = 1e-5,\n",
    "                                    bias_coeff = 1e-6,\n",
    "                                    lr = 1e-3,)\n",
    "\n",
    "        # Save model config and weights\n",
    "        model_json = model.to_json()\n",
    "        with open(model_name+\".json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)  \n",
    "    \n",
    "        ## temp function to quantify the spots and classes present in a batch\n",
    "        trainy_inbatch = array_generator_verify(save_directory+\"//training_data\", batch_size, \n",
    "                                                len(classhkl), loc_new, print)\n",
    "        print(\"Number of spots in a batch of %i files : %i\" %(batch_size, len(trainy_inbatch)))\n",
    "        print(\"Min, Max class ID is %i, %i\" %(np.min(trainy_inbatch), np.max(trainy_inbatch)))\n",
    "        \n",
    "        ## Batch loading for numpy grain files (Keep low value to avoid overcharging the RAM)\n",
    "        nb_grains_list = []\n",
    "        for ino, imat in enumerate(material_):\n",
    "            nb_grains_list.append(list(range(input_params[\"nb_grains_per_lp\"][ino]+1)))\n",
    "        list_permute = list(itertools.product(*nb_grains_list))\n",
    "        list_permute.pop(0)\n",
    "        steps_per_epoch = len(list_permute)*(input_params[\"grains_nb_simulate\"])//batch_size        \n",
    "        val_steps_per_epoch = int(steps_per_epoch / 5)\n",
    "        if steps_per_epoch == 0:\n",
    "            steps_per_epoch = 1\n",
    "        if val_steps_per_epoch == 0:\n",
    "            val_steps_per_epoch = 1 \n",
    "            \n",
    "        ## Load generator objects from filepaths (iterators for Training and Testing datasets)\n",
    "        training_data_generator = array_generator(save_directory+\"//training_data\", batch_size,                                          \n",
    "                                                  len(classhkl), loc_new, print)\n",
    "        testing_data_generator = array_generator(save_directory+\"//testing_data\", batch_size,                                           \n",
    "                                                 len(classhkl), loc_new, print)\n",
    "        \n",
    "        \n",
    "        ######### TRAIN THE DATA\n",
    "        es = EarlyStopping(monitor='val_accuracy', mode='max', patience=2)\n",
    "        ms = ModelCheckpoint(save_directory+\"//best_val_acc_model.h5\", monitor='val_accuracy', \n",
    "                              mode='max', save_best_only=True)\n",
    "        lc = LoggingCallback(None, None, None, model, model_name)\n",
    "        ## Fitting function\n",
    "        stats_model = model.fit(\n",
    "                                training_data_generator, \n",
    "                                epochs=epochs, \n",
    "                                steps_per_epoch=steps_per_epoch,\n",
    "                                validation_data=testing_data_generator,\n",
    "                                validation_steps=val_steps_per_epoch,\n",
    "                                verbose=1,\n",
    "                                class_weight=class_weights,\n",
    "                                callbacks=[es, ms, lc]\n",
    "                                )          \n",
    "        # serialize weights to HDF5\n",
    "        model.save_weights(model_name+\".h5\")\n",
    "        print(\"Saved model to disk\")\n",
    "        \n",
    "        print( \"Training Accuracy: \"+str( stats_model.history['accuracy'][-1]))\n",
    "        print( \"Training Loss: \"+str( stats_model.history['loss'][-1]))\n",
    "        print( \"Validation Accuracy: \"+str( stats_model.history['val_accuracy'][-1]))\n",
    "        print( \"Validation Loss: \"+str( stats_model.history['val_loss'][-1]))\n",
    "        \n",
    "        # Plot the accuracy/loss v Epochs\n",
    "        epochs = range(1, len(model.history.history['loss']) + 1)\n",
    "        fig, ax = plt.subplots(1,2)\n",
    "        ax[0].plot(epochs, model.history.history['loss'], 'r', label='Training loss')\n",
    "        ax[0].plot(epochs, model.history.history['val_loss'], 'r', ls=\"dashed\", label='Validation loss')\n",
    "        ax[0].legend()\n",
    "        ax[1].plot(epochs, model.history.history['accuracy'], 'g', label='Training Accuracy')\n",
    "        ax[1].plot(epochs, model.history.history['val_accuracy'], 'g', ls=\"dashed\", label='Validation Accuracy')\n",
    "        ax[1].legend()\n",
    "        plt.savefig(save_directory+\"//loss_accuracy_\"+prefix_mat+\".png\", bbox_inches='tight',format='png', dpi=1000)\n",
    "        plt.close()\n",
    "        \n",
    "        text_file = open(save_directory+\"//loss_accuracy_logger_\"+prefix_mat+\".txt\", \"w\")\n",
    "        text_file.write(\"# EPOCH, LOSS, VAL_LOSS, ACCURACY, VAL_ACCURACY\" + \"\\n\")\n",
    "        for inj in range(len(epochs)):\n",
    "            string1 = str(epochs[inj]) + \",\"+ str(model.history.history['loss'][inj])+\\\n",
    "                            \",\"+str(model.history.history['val_loss'][inj])+\",\"+str(model.history.history['accuracy'][inj])+\\\n",
    "                            \",\"+str(model.history.history['val_accuracy'][inj])+\" \\n\"  \n",
    "            text_file.write(string1)\n",
    "        text_file.close() \n",
    "\n",
    "        # ## Stats on the trained model with sklearn metrics\n",
    "        from sklearn.metrics import classification_report\n",
    "        ## verify the statistics\n",
    "        x_test, y_test = vali_array(save_directory+\"//testing_data\", 50, len(classhkl), loc_new, print)\n",
    "        y_test = np.argmax(y_test, axis=-1)\n",
    "        y_pred = np.argmax(model.predict(x_test), axis=-1)\n",
    "        print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02734465-4f6a-40f5-b322-c426e4b36b0a",
   "metadata": {},
   "source": [
    "## Index Laue patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59988fb3-5ecf-45d1-ada2-9671dec9e30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing settings file in /home/esrf/purushot/anaconda3/envs/lauenn/lib/python3.7/site-packages/lauetoolsnn/settings.ini\n",
      "Directory where trained model is stored : /home/esrf/purushot/Desktop/LaueNN_tutorial/TUtorial_1/LaueNN_script/GaN_Si_MultiMaterial\n",
      "expected 2 files based on the XY grid (1,2) defined by user\n",
      "and found 2 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting for /home/esrf/purushot/Desktop/LaueNN_tutorial/TUtorial_1/LaueNN_script/nw1_0000.tif\n",
      "Skimage mode (strict constraints) of PeakSearch is used for the PeakSearch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 24.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting for /home/esrf/purushot/Desktop/LaueNN_tutorial/TUtorial_1/LaueNN_script/nw1_0001.tif\n",
      "Skimage mode (strict constraints) of PeakSearch is used for the PeakSearch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_prediction_MM = True\n",
    "\n",
    "if __name__ == '__main__':     #enclosing required because of multiprocessing\n",
    "\n",
    "    if run_prediction_MM:\n",
    "        ## Import modules used for this Notebook\n",
    "        import numpy as np\n",
    "        import os\n",
    "        import multiprocessing\n",
    "        import time, datetime\n",
    "        import glob, re\n",
    "        import configparser\n",
    "        from itertools import accumulate\n",
    "        ## if LaueToolsNN is properly installed\n",
    "        from lauetoolsnn.utils_lauenn import get_multimaterial_detail, new_MP_multimat_function, resource_path, global_plots_MM\n",
    "        from lauetoolsnn.lauetools import dict_LaueTools as dictLT\n",
    "        from lauetoolsnn.NNmodels import read_hdf5\n",
    "        \n",
    "        import _pickle as cPickle\n",
    "        from tqdm import tqdm\n",
    "        ncpu = multiprocessing.cpu_count()\n",
    "        \n",
    "        material_= input_params[\"material_\"]\n",
    "        detectorparameters = input_params[\"detectorparameters\"]\n",
    "        pixelsize = input_params[\"pixelsize\"]\n",
    "        emax = input_params[\"emax\"]\n",
    "        emin = input_params[\"emin\"]\n",
    "        dim1 = input_params[\"dim1\"]\n",
    "        dim2 = input_params[\"dim2\"]\n",
    "        symm_ = input_params[\"symmetry\"]\n",
    "        SG = input_params[\"SG\"]\n",
    "        tolerance = input_params[\"matrix_tolerance\"]\n",
    "        tolerance_strain = input_params[\"tolerance_strain_refinement\"]\n",
    "        strain_free_parameters = input_params[\"free_parameters\"]\n",
    "        material_limit = input_params[\"material_limit\"]\n",
    "        material_phase_always_present = input_params[\"material_phase_always_present\"]\n",
    "        model_annote = \"DNN\"\n",
    "\n",
    "        ## Requirements\n",
    "        ## Experimental peak search parameters in case of RAW LAUE PATTERNS from detector\n",
    "        intensity_threshold = input_params[\"intensity_threshold\"]\n",
    "        boxsize = input_params[\"boxsize\"]\n",
    "        fit_peaks_gaussian = input_params[\"fit_peaks_gaussian\"]\n",
    "        FitPixelDev = input_params[\"FitPixelDev\"]\n",
    "        NumberMaxofFits = input_params[\"NumberMaxofFits\"]\n",
    "        bkg_treatment = \"A-B\"\n",
    "        mode_peaksearch = input_params[\"mode\"]\n",
    "        \n",
    "        ubmat = input_params[\"UB_matrix_to_detect\"] # How many orientation matrix to detect per Laue pattern\n",
    "        mode_spotCycle = input_params[\"mode_spotCycle\"] ## mode of calculation\n",
    "        use_previous_UBmatrix_name = input_params[\"use_previous\"] ## Try previous indexation solutions to speed up the process\n",
    "        strain_calculation = input_params[\"strain_compute\"] ## Strain refinement is required or not\n",
    "        ccd_label_global = input_params[\"ccd_label\"]\n",
    "        \n",
    "        ## Parameters to control the orientation matrix indexation\n",
    "        softmax_threshold_global = input_params[\"softmax_threshold_global\"] # softmax_threshold of the Neural network to consider\n",
    "        mr_threshold_global = 0.90 # match rate threshold to accept a solution immediately\n",
    "        cap_matchrate = input_params[\"cap_matchrate\"] * 100 ## any UB matrix providing MR less than this will be ignored\n",
    "        coeff = 0.30            ## coefficient to calculate the overlap of two solutions\n",
    "        coeff_overlap = input_params[\"coeff_overlap\"]    ##10% spots overlap is allowed with already indexed orientation\n",
    "\n",
    "        ## Additional parameters to refine the orientation matrix construction process\n",
    "        use_om_user = str(input_params[\"use_om_user\"]).lower()\n",
    "        path_user_OM = input_params[\"path_user_OM\"]\n",
    "        nb_spots_consider = input_params[\"nb_spots_consider\"]\n",
    "        residues_threshold = input_params[\"residues_threshold\"]\n",
    "        nb_spots_global_threshold = input_params[\"nb_spots_global_threshold\"]\n",
    "        option_global = \"v2\"\n",
    "        additional_expression = [\"none\"] # for strain assumptions, like a==b for HCP\n",
    "        \n",
    "        config_setting = configparser.ConfigParser()\n",
    "        filepath = resource_path('settings.ini')\n",
    "        print(\"Writing settings file in \" + filepath)\n",
    "        config_setting.read(filepath)\n",
    "        config_setting.set('CALLER', 'residues_threshold',str(residues_threshold))\n",
    "        config_setting.set('CALLER', 'nb_spots_global_threshold',str(nb_spots_global_threshold))\n",
    "        config_setting.set('CALLER', 'option_global',option_global)\n",
    "        config_setting.set('CALLER', 'use_om_user',use_om_user)\n",
    "        config_setting.set('CALLER', 'nb_spots_consider',str(nb_spots_consider))\n",
    "        config_setting.set('CALLER', 'path_user_OM',str(path_user_OM))\n",
    "        config_setting.set('CALLER', 'intensity', str(intensity_threshold))\n",
    "        config_setting.set('CALLER', 'boxsize', str(boxsize))\n",
    "        config_setting.set('CALLER', 'pixdev', str(FitPixelDev))\n",
    "        config_setting.set('CALLER', 'cap_softmax', str(softmax_threshold_global))\n",
    "        config_setting.set('CALLER', 'cap_mr', str(cap_matchrate/100.))\n",
    "        config_setting.set('CALLER', 'strain_free_parameters', \",\".join(strain_free_parameters))\n",
    "        config_setting.set('CALLER', 'additional_expression', \",\".join(additional_expression))\n",
    "        config_setting.set('CALLER', 'mode_peaksearch', str(mode_peaksearch))\n",
    "        with open(filepath, 'w') as configfile:\n",
    "            config_setting.write(configfile)\n",
    "\n",
    "        if len(material_) > 1:\n",
    "            prefix_mat = material_[0]\n",
    "            for ino, imat in enumerate(material_):\n",
    "                if ino == 0:\n",
    "                    continue\n",
    "                prefix_mat = prefix_mat + \"_\" + imat\n",
    "        else:\n",
    "            prefix_mat = material_[0]\n",
    "        \n",
    "        model_direc = os.path.join(global_path,prefix_mat+input_params[\"prefix\"])\n",
    "        \n",
    "        if not os.path.exists(model_direc):\n",
    "            print(\"The directory doesn't exists; please veify the path\")\n",
    "        else:\n",
    "            print(\"Directory where trained model is stored : \"+model_direc)\n",
    "            \n",
    "        ## get unit cell parameters and other details required for simulating Laue patterns\n",
    "        rules, symmetry, lattice_material, \\\n",
    "                            crystal, SG = get_multimaterial_detail(material_, SG, symm_)\n",
    "\n",
    "        filenameDirec = input_params[\"experimental_directory\"]\n",
    "        experimental_prefix = input_params[\"experimental_prefix\"]\n",
    "        lim_x, lim_y = input_params[\"grid_size_x\"], input_params[\"grid_size_y\"] \n",
    "        format_file = dictLT.dict_CCD[ccd_label_global][7]\n",
    "        \n",
    "        hkl_all_class0 = []\n",
    "        for ino, imat in enumerate(material_):\n",
    "            with open(model_direc+\"//classhkl_data_nonpickled_\"+imat+\".pickle\", \"rb\") as input_file:\n",
    "                hkl_all_class_load = cPickle.load(input_file)[0]\n",
    "            hkl_all_class0.append(hkl_all_class_load)\n",
    "            \n",
    "        ## load model related files and generate the model\n",
    "        classhkl = np.load(model_direc+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_0\"]\n",
    "        angbins = np.load(model_direc+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_1\"]\n",
    "        \n",
    "        # from two phase training dataset FIX\n",
    "        # if len(material_) <= 2: \n",
    "        #     ind_mat0 = np.load(model_direc+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_5\"]\n",
    "        #     ind_mat1 = np.load(model_direc+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_6\"] \n",
    "        #     ind_mat = [ind_mat0, ind_mat0+ind_mat1]\n",
    "        # else:\n",
    "        ##Below lines are for dataset generated with multimat code\n",
    "        ind_mat_all = np.load(model_direc+\"//MOD_grain_classhkl_angbin.npz\",allow_pickle=True)[\"arr_5\"]\n",
    "        ind_mat = []\n",
    "        for inni in ind_mat_all:\n",
    "            ind_mat.append(len(inni))\n",
    "        ind_mat = [int(item) for item in accumulate(ind_mat)]\n",
    "        \n",
    "        # json_file = open(model_direc+\"//model_\"+prefix_mat+\".json\", 'r')\n",
    "        load_weights = model_direc + \"//model_\"+prefix_mat+\".h5\"\n",
    "        wb = read_hdf5(load_weights)\n",
    "        temp_key = list(wb.keys())\n",
    "        \n",
    "        ct = time.time()\n",
    "        now = datetime.datetime.fromtimestamp(ct)\n",
    "        c_time = now.strftime(\"%Y-%m-%d_%H-%M-%S\")   \n",
    "\n",
    "        ## Step 3: Initialize variables and prepare arguments for multiprocessing module\n",
    "\n",
    "        col = [[] for i in range(int(ubmat))]\n",
    "        colx = [[] for i in range(int(ubmat))]\n",
    "        coly = [[] for i in range(int(ubmat))]\n",
    "        rotation_matrix = [[] for i in range(int(ubmat))]\n",
    "        strain_matrix = [[] for i in range(int(ubmat))]\n",
    "        strain_matrixs = [[] for i in range(int(ubmat))]\n",
    "        match_rate = [[] for i in range(int(ubmat))]\n",
    "        spots_len = [[] for i in range(int(ubmat))]\n",
    "        iR_pix = [[] for i in range(int(ubmat))]\n",
    "        fR_pix = [[] for i in range(int(ubmat))]\n",
    "        mat_global = [[] for i in range(int(ubmat))]\n",
    "        best_match = [[] for i in range(int(ubmat))]\n",
    "        spots1_global = [[] for i in range(int(ubmat))]\n",
    "        for i in range(int(ubmat)):\n",
    "            col[i].append(np.zeros((lim_x*lim_y,3)))\n",
    "            colx[i].append(np.zeros((lim_x*lim_y,3)))\n",
    "            coly[i].append(np.zeros((lim_x*lim_y,3)))\n",
    "            rotation_matrix[i].append(np.zeros((lim_x*lim_y,3,3)))\n",
    "            strain_matrix[i].append(np.zeros((lim_x*lim_y,3,3)))\n",
    "            strain_matrixs[i].append(np.zeros((lim_x*lim_y,3,3)))\n",
    "            match_rate[i].append(np.zeros((lim_x*lim_y,1)))\n",
    "            spots_len[i].append(np.zeros((lim_x*lim_y,1)))\n",
    "            iR_pix[i].append(np.zeros((lim_x*lim_y,1)))\n",
    "            fR_pix[i].append(np.zeros((lim_x*lim_y,1)))\n",
    "            mat_global[i].append(np.zeros((lim_x*lim_y,1)))\n",
    "            best_match[i].append([[] for jk in range(lim_x*lim_y)])\n",
    "            spots1_global[i].append([[] for jk in range(lim_x*lim_y)])\n",
    "\n",
    "        # =============================================================================\n",
    "        #         ## Multi-processing routine\n",
    "        # =============================================================================        \n",
    "        ## Number of files to generate\n",
    "        grid_files = np.zeros((lim_x,lim_y))\n",
    "        filenm = np.chararray((lim_x,lim_y), itemsize=1000)\n",
    "        grid_files = grid_files.ravel()\n",
    "        filenm = filenm.ravel()\n",
    "        count_global = lim_x * lim_y\n",
    "        list_of_files = glob.glob(filenameDirec+'//'+experimental_prefix+'*.'+format_file)\n",
    "        ## sort files\n",
    "        list_of_files.sort(key=lambda var:[int(x) if x.isdigit() else x for x in re.findall(r'[^0-9]|[0-9]+', var)])\n",
    "\n",
    "        if len(list_of_files) == count_global:\n",
    "            for ii in range(len(list_of_files)):\n",
    "                grid_files[ii] = ii\n",
    "                filenm[ii] = list_of_files[ii]     \n",
    "            print(\"expected \"+str(count_global)+\" files based on the XY grid (\"+str(lim_x)+\",\"+str(lim_y)+\") defined by user\")\n",
    "            print(\"and found \"+str(len(list_of_files))+\" files\")\n",
    "        else:\n",
    "            print(\"expected \"+str(count_global)+\" files based on the XY grid (\"+str(lim_x)+\",\"+str(lim_y)+\") defined by user\")\n",
    "            print(\"But found \"+str(len(list_of_files))+\" files (either all data is not written yet or maybe XY grid definition is not proper)\")\n",
    "            digits = len(str(count_global))\n",
    "            digits = max(digits,4)\n",
    "            # Temp fix\n",
    "            for ii in range(count_global):\n",
    "                text = str(ii)\n",
    "                if ii < 10000:\n",
    "                    string = text.zfill(4)\n",
    "                else:\n",
    "                    string = text.zfill(5)\n",
    "                file_name_temp = filenameDirec+'//'+experimental_prefix + string+'.'+format_file\n",
    "                ## store it in a grid \n",
    "                filenm[ii] = file_name_temp\n",
    "\n",
    "        check = np.zeros((count_global,int(ubmat)))\n",
    "        # =============================================================================\n",
    "        blacklist = None\n",
    "\n",
    "        ### Create a COR directory to be loaded in LaueTools\n",
    "        cor_file_directory = filenameDirec + \"//\" + experimental_prefix+\"CORfiles\"\n",
    "        if list_of_files[0].split(\".\")[-1] in ['cor',\"COR\",\"Cor\"]:\n",
    "            cor_file_directory = filenameDirec \n",
    "        if not os.path.exists(cor_file_directory):\n",
    "            os.makedirs(cor_file_directory)\n",
    "\n",
    "        try_prevs = False\n",
    "        files_treated = []\n",
    "\n",
    "        valu12 = [[ filenm[ii].decode(), \n",
    "                    ii,\n",
    "                    rotation_matrix,\n",
    "                    strain_matrix,\n",
    "                    strain_matrixs,\n",
    "                    col,\n",
    "                    colx,\n",
    "                    coly,\n",
    "                    match_rate,\n",
    "                    spots_len, \n",
    "                    iR_pix, \n",
    "                    fR_pix,\n",
    "                    best_match,\n",
    "                    mat_global,\n",
    "                    check,\n",
    "                    detectorparameters,\n",
    "                    pixelsize,\n",
    "                    angbins,\n",
    "                    classhkl,\n",
    "                    hkl_all_class0,\n",
    "                    emin,\n",
    "                    emax,\n",
    "                    material_,\n",
    "                    symmetry,\n",
    "                    lim_x,\n",
    "                    lim_y,\n",
    "                    strain_calculation, \n",
    "                    ind_mat, \n",
    "                    model_direc, \n",
    "                    tolerance,\n",
    "                    int(ubmat), ccd_label_global, \n",
    "                    None,\n",
    "                    float(intensity_threshold),\n",
    "                    int(boxsize),\n",
    "                    bkg_treatment,\n",
    "                    filenameDirec, \n",
    "                    experimental_prefix,\n",
    "                    blacklist,\n",
    "                    None,\n",
    "                    files_treated,\n",
    "                    try_prevs, ## try previous is kept true, incase if its stuck in loop\n",
    "                    wb,\n",
    "                    temp_key,\n",
    "                    cor_file_directory,\n",
    "                    mode_spotCycle,\n",
    "                    softmax_threshold_global,\n",
    "                    mr_threshold_global,\n",
    "                    cap_matchrate,\n",
    "                    tolerance_strain,\n",
    "                    NumberMaxofFits,\n",
    "                    fit_peaks_gaussian,\n",
    "                    FitPixelDev,\n",
    "                    coeff,\n",
    "                    coeff_overlap,\n",
    "                    material_limit,\n",
    "                    use_previous_UBmatrix_name,\n",
    "                    material_phase_always_present,\n",
    "                    crystal,\n",
    "                    strain_free_parameters,\n",
    "                    model_annote] for ii in range(count_global)]\n",
    "\n",
    "        #% Launch multiprocessing prediction     \n",
    "        args = zip(valu12)\n",
    "        with multiprocessing.Pool(ncpu) as pool:\n",
    "            results = pool.starmap(new_MP_multimat_function, tqdm(args, total=len(valu12)), chunksize=1)\n",
    "\n",
    "            for r in results:\n",
    "                r_message_mpdata = r\n",
    "                strain_matrix_mpdata, strain_matrixs_mpdata, rotation_matrix_mpdata, col_mpdata,\\\n",
    "                colx_mpdata, coly_mpdata, match_rate_mpdata, mat_global_mpdata,\\\n",
    "                    cnt_mpdata, meta_mpdata, files_treated_mpdata, spots_len_mpdata, \\\n",
    "                        iR_pixel_mpdata, fR_pixel_mpdata, best_match_mpdata, check_mpdata = r_message_mpdata\n",
    "\n",
    "                for i_mpdata in files_treated_mpdata:\n",
    "                    files_treated.append(i_mpdata)\n",
    "\n",
    "                for intmat_mpdata in range(int(ubmat)):\n",
    "                    check[cnt_mpdata,intmat_mpdata] = check_mpdata[cnt_mpdata,intmat_mpdata]\n",
    "                    mat_global[intmat_mpdata][0][cnt_mpdata] = mat_global_mpdata[intmat_mpdata][0][cnt_mpdata]\n",
    "                    strain_matrix[intmat_mpdata][0][cnt_mpdata,:,:] = strain_matrix_mpdata[intmat_mpdata][0][cnt_mpdata,:,:]\n",
    "                    strain_matrixs[intmat_mpdata][0][cnt_mpdata,:,:] = strain_matrixs_mpdata[intmat_mpdata][0][cnt_mpdata,:,:]\n",
    "                    rotation_matrix[intmat_mpdata][0][cnt_mpdata,:,:] = rotation_matrix_mpdata[intmat_mpdata][0][cnt_mpdata,:,:]\n",
    "                    col[intmat_mpdata][0][cnt_mpdata,:] = col_mpdata[intmat_mpdata][0][cnt_mpdata,:]\n",
    "                    colx[intmat_mpdata][0][cnt_mpdata,:] = colx_mpdata[intmat_mpdata][0][cnt_mpdata,:]\n",
    "                    coly[intmat_mpdata][0][cnt_mpdata,:] = coly_mpdata[intmat_mpdata][0][cnt_mpdata,:]\n",
    "                    match_rate[intmat_mpdata][0][cnt_mpdata] = match_rate_mpdata[intmat_mpdata][0][cnt_mpdata]\n",
    "                    spots_len[intmat_mpdata][0][cnt_mpdata] = spots_len_mpdata[intmat_mpdata][0][cnt_mpdata]\n",
    "                    iR_pix[intmat_mpdata][0][cnt_mpdata] = iR_pixel_mpdata[intmat_mpdata][0][cnt_mpdata]\n",
    "                    fR_pix[intmat_mpdata][0][cnt_mpdata] = fR_pixel_mpdata[intmat_mpdata][0][cnt_mpdata]\n",
    "                    best_match[intmat_mpdata][0][cnt_mpdata] = best_match_mpdata[intmat_mpdata][0][cnt_mpdata]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc02537-31ad-469e-a7c7-b11891954ddd",
   "metadata": {},
   "source": [
    "## save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6cde8a9-5b87-4216-8b12-39e911fd658b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data saved in  /home/esrf/purushot/Desktop/LaueNN_tutorial/TUtorial_1/LaueNN_script/GaN_Si_MultiMaterial//results_GaN_Si_2022-11-15_12-51-50\n",
      "Number of Phases present (includes non indexed phase zero also) 3\n",
      "building KD tree...\n",
      "prediction statistics are generated\n"
     ]
    }
   ],
   "source": [
    "from lauetoolsnn.utils_lauenn import global_plots_MM, write_average_orientationMM, convert_pickle_to_hdf5MM, write_prediction_statsMM, write_MTEXdataMM\n",
    "#% Save results\n",
    "save_directory_ = model_direc+\"//results_\"+prefix_mat+\"_\"+c_time\n",
    "if not os.path.exists(save_directory_):\n",
    "    os.makedirs(save_directory_)\n",
    "\n",
    "## intermediate saving of pickle objects with results\n",
    "np.savez_compressed(save_directory_+ \"//results.npz\", \n",
    "                    best_match, mat_global, rotation_matrix, strain_matrix, \n",
    "                    strain_matrixs, col, colx, coly, match_rate, files_treated,\n",
    "                    lim_x, lim_y, spots_len, iR_pix, fR_pix,\n",
    "                    material_)\n",
    "## intermediate saving of pickle objects with results\n",
    "with open(save_directory_+ \"//results.pickle\", \"wb\") as output_file:\n",
    "        cPickle.dump([best_match, mat_global, rotation_matrix, strain_matrix, \n",
    "                      strain_matrixs, col, colx, coly, match_rate, files_treated,\n",
    "                      lim_x, lim_y, spots_len, iR_pix, fR_pix,\n",
    "                      material_, lattice_material,\n",
    "                      symmetry, crystal], output_file)\n",
    "print(\"data saved in \", save_directory_)\n",
    "\n",
    "## Lets save also a set of average UB matrix in text file to be used with user_OM setting    \n",
    "try:\n",
    "    write_average_orientationMM(save_directory_, mat_global, rotation_matrix,\n",
    "                                  match_rate, lim_x, lim_y, crystal,\n",
    "                                  radius=10, grain_ang=5, pixel_grain_definition=3)\n",
    "except:\n",
    "    print(\"Error in the average orientaiton module\")\n",
    "    \n",
    "try:\n",
    "    convert_pickle_to_hdf5MM(save_directory_, files_treated, rotation_matrix, strain_matrix, \n",
    "                           strain_matrixs, match_rate, spots_len, iR_pix, \n",
    "                           fR_pix, colx, coly, col, mat_global,\n",
    "                           material_, lim_x, lim_y)\n",
    "except:\n",
    "    print(\"Error in the hdf5 conversion module\")\n",
    "    \n",
    "try:\n",
    "    write_prediction_statsMM(save_directory_, material_, files_treated,\\\n",
    "                            lim_x, lim_y, best_match, strain_matrixs, strain_matrix, iR_pix,\\\n",
    "                            fR_pix,  mat_global)\n",
    "except:\n",
    "    print(\"Error in the Prediction statistic module\")\n",
    "    \n",
    "try:\n",
    "    write_MTEXdataMM(save_directory_, material_, rotation_matrix,\\\n",
    "                   lattice_material, lim_x, lim_y, mat_global,\\\n",
    "                    input_params[\"symmetry\"])\n",
    "except:\n",
    "    print(\"Error in the Prediction statistic module\")\n",
    "\n",
    "try:\n",
    "    global_plots_MM(lim_x, lim_y, rotation_matrix, strain_matrix, strain_matrixs, \n",
    "                  col, colx, coly, match_rate, mat_global, spots_len, \n",
    "                  iR_pix, fR_pix, save_directory_, material_,\n",
    "                  match_rate_threshold=5, bins=30)\n",
    "except:\n",
    "    print(\"Error in the global plots module\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b489c1a5-1301-4f4d-ac16-cbb79d876dd3",
   "metadata": {},
   "source": [
    "## Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2e028f6-1658-476f-abff-d28e1e16d9dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/x-hdf5": "/home/esrf/purushot/Desktop/LaueNN_tutorial/LaueNN_script/GaN_Si_MultiMaterial/results_GaN_Si_2022-11-15_00-45-42/grain_all.h5",
      "text/plain": [
       "<jupyterlab_h5web.widget.H5Web object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jupyterlab_h5web import H5Web\n",
    "H5Web(os.path.join(save_directory_,\"grain_all.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6245d458-3e78-4d9f-8fc8-183dee4392cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lauenn",
   "language": "python",
   "name": "lauenn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
