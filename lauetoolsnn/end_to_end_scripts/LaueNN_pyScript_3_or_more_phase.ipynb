{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61bda473-6e51-4914-826e-21c50f0f61d1",
   "metadata": {},
   "source": [
    "# Notebook script for Training the neural network (supports More than 2 phases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296c62c0-e898-4212-a783-412947bd31a3",
   "metadata": {},
   "source": [
    "## Define material and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2ad7bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaueNN path is /home/esrf/purushot/anaconda3/envs/lauenn/lib/python3.7/site-packages/lauetoolsnn\n",
      "['GaN', [3.189, 3.189, 5.185, 90, 90, 120], 'wurtzite']\n",
      "['Si', [5.4309, 5.4309, 5.4309, 90, 90, 90], 'dia']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':     #enclosing required because of multiprocessing\n",
    "\n",
    "    ## Import modules used for this Notebook\n",
    "    import os\n",
    "    import json\n",
    "    \n",
    "    ## Get the path of the lauetoolsnn library\n",
    "    import lauetoolsnn\n",
    "    laueNN_path = os.path.dirname(lauetoolsnn.__file__)\n",
    "    print(\"LaueNN path is\", laueNN_path)\n",
    "    \n",
    "    ## Load the json of material and extinctions\n",
    "    with open(os.path.join(laueNN_path, 'lauetools','material.json'),'r') as f:\n",
    "        dict_Materials = json.load(f)\n",
    "    with open(os.path.join(laueNN_path, 'lauetools','extinction.json'),'r') as f:\n",
    "        extinction_json = json.load(f)\n",
    "\n",
    "    ## Modify the dictionary values to add new entries\n",
    "    dict_Materials[\"GaN\"] = [\"GaN\", [3.189, 3.189, 5.185, 90, 90, 120], \"wurtzite\"]\n",
    "    dict_Materials[\"Si\"] = [\"Si\", [5.4309, 5.4309, 5.4309, 90, 90, 90], \"dia\"]\n",
    "\n",
    "    extinction_json[\"wurtzite\"] = \"wurtzite\"\n",
    "    extinction_json[\"dia\"] = \"dia\"\n",
    "\n",
    "    ## verify if extinction is present in CrystalParameters.py file of lauetools (Manually done for now)\n",
    "\n",
    "    ## dump the json back with new values\n",
    "    with open(os.path.join(laueNN_path, 'lauetools','material.json'), 'w') as fp:\n",
    "        json.dump(dict_Materials, fp)\n",
    "    with open(os.path.join(laueNN_path, 'lauetools','extinction.json'), 'w') as fp:\n",
    "        json.dump(extinction_json, fp)\n",
    "\n",
    "    ## Verify if the material is added to the library or not;\n",
    "    from lauetoolsnn.lauetools.dict_LaueTools import dict_Materials\n",
    "    ## if not, restart the console\n",
    "    print(dict_Materials[\"GaN\"])\n",
    "    print(dict_Materials[\"Si\"])\n",
    "    \n",
    "    # =============================================================================\n",
    "    # Step 0: Define the dictionary with all parameters \n",
    "    # =============================================================================\n",
    "    ## User Input dictionary with parameters\n",
    "    ## In case of only one phase/material, keep same value for material_ and material1_ key\n",
    "    input_params = {\n",
    "                    \"global_path\" : os.getcwd(),\n",
    "                    \"prefix\" : \"_MultiMaterial\",                 ## prefix for the folder to be created for training dataset\n",
    "\n",
    "                    \"material_\": [\"GaN\", \"Si\"],             ## same key as used in dict_LaueTools\n",
    "                    \"symmetry\": [\"hexagonal\", \"cubic\"],           ## crystal symmetry of material_\n",
    "                    \"SG\": [191, 227], #186                    ## Space group of material_ (None if not known)\n",
    "                    \"hkl_max_identify\" : [6,5],        ## Maximum hkl index to classify in a Laue pattern\n",
    "                    \"nb_grains_per_lp\" : [2,1],        ## max grains to be generated in a Laue Image\n",
    "\n",
    "                    ## hkl_max_identify : can be \"auto\" or integer: Maximum index of HKL to build output classes\n",
    "                    \n",
    "                    # =============================================================================\n",
    "                    # ## Data generation settings\n",
    "                    # =============================================================================\n",
    "                    \"grains_nb_simulate\" : 500,    ## Number of orientations to generate (takes advantage of crystal symmetry)\n",
    "                    \"classes_with_frequency_to_remove\": [100,100], ## classes_with_frequency_to_remove: HKL class with less appearance than \n",
    "                                                                            ##  specified will be ignored in output\n",
    "                    \"desired_classes_output\": [\"all\",\"all\"], ## desired_classes_output : can be all or an integer: to limit the number of output classes\n",
    "                    \"list_hkl_keep\" : None, #[[(0,0,1)],[(0,0,0)]],\n",
    "                    \"maximum_angle_to_search\":60, ## Angle of radial distribution to reconstruct the histogram (in deg)\n",
    "                    \"step_for_binning\" : 0.1,      ## bin widht of angular radial distribution in degree\n",
    "                    \n",
    "                    # =============================================================================\n",
    "                    #  ## Training parameters\n",
    "                    # =============================================================================\n",
    "                    \"orientation_generation\": \"uniform\", ## could be \"uniform\" or \"random\"\n",
    "                    \"batch_size\":100,               ## batches of files to use while training\n",
    "                    \"epochs\":10,                    ## number of epochs for training\n",
    "\n",
    "                    # =============================================================================\n",
    "                    # ## Detector parameters of the Experimental setup\n",
    "                    # =============================================================================\n",
    "                    ## Sample-detector distance, X center, Y center, two detector angles\n",
    "                    \"detectorparameters\" :  [79.61200, 977.8100, 932.1700, 0.4770000, 0.4470000], \n",
    "                    \"pixelsize\" : 0.0734,          ## Detector pixel size\n",
    "                    \"dim1\":2018,                   ## Dimensions of detector in pixels\n",
    "                    \"dim2\":2016,\n",
    "                    \"emin\" : 5,                    ## Minimum and maximum energy to use for simulating Laue Patterns\n",
    "                    \"emax\" : 22,\n",
    "                    \"ccd_label\" : \"sCMOS\",\n",
    "                    \n",
    "                    # =============================================================================\n",
    "                    # ## Prediction parameters\n",
    "                    # =============================================================================\n",
    "                    \"experimental_directory\": os.getcwd(),\n",
    "                    \"experimental_prefix\": r\"nw1_\",\n",
    "                    \"grid_size_x\" : 1,            ## Grid X and Y limit to generate the simulated dataset (a rectangular scan region)\n",
    "                    \"grid_size_y\" : 2,\n",
    "                    \n",
    "                    # =============================================================================\n",
    "                    # ## Prediction Settings\n",
    "                    # =============================================================================\n",
    "                    # model_weight_file: if none, it will select by default the latest H5 weight file, else provide a specific model\n",
    "                    # softmax_threshold_global: thresholding to limit the predicted spots search zone\n",
    "                    # cap_matchrate: any UB matrix providing MR less than this will be ignored\n",
    "                    # coeff: should be same as cap_matchrate or no? (this is for when use_previous is True)\n",
    "                    # coeff_overlap: coefficient to limit the overlapping between spots; if more than this, new solution will be computed\n",
    "                    # mode_spotCycle: How to cycle through predicted spots (slow or graphmode )\n",
    "                    \"UB_matrix_to_detect\" : 3,\n",
    "                    \"matrix_tolerance\" : [0.6, 0.6],\n",
    "                    \"material_limit\" : [2, 1],\n",
    "                    \"material_phase_always_present\" : None,#[2,1,1],\n",
    "                    \"softmax_threshold_global\" : 0.85,\n",
    "                    \"cap_matchrate\" : 0.10,\n",
    "                    \"coeff_overlap\" : 0.3,\n",
    "                    \"mode_spotCycle\" : \"graphmode\",\n",
    "                    ##true for few crystal and prefered texture case, otherwise time consuming; advised for single phase alone\n",
    "                    \"use_previous\" : False,\n",
    "                    \n",
    "                    # =============================================================================\n",
    "                    # # [PEAKSEARCH]\n",
    "                    # =============================================================================\n",
    "                    \"intensity_threshold\" : 2,## for skimage this is of image standard deviation\n",
    "                    \"boxsize\" : 10,## for skimage this is box size to fit\n",
    "                    \"fit_peaks_gaussian\" : 1,## for skimage this is of no sense\n",
    "                    \"FitPixelDev\" : 15, ## for skimage this is distance between peaks to avoid\n",
    "                    \"NumberMaxofFits\" : 3000,## for skimage this is maximum leastquare attempts before giving up\n",
    "                    \"mode\": \"skimage\",\n",
    "\n",
    "                    # =============================================================================\n",
    "                    # # [STRAINCALCULATION]\n",
    "                    # =============================================================================\n",
    "                    \"strain_compute\" : True,\n",
    "                    \"tolerance_strain_refinement\" : [[0.6,0.5,0.4,0.3,0.2,0.1],\n",
    "                                                     [0.6,0.5,0.4,0.3,0.2,0.1]],\n",
    "                    \"free_parameters\" : [\"b\",\"c\",\"alpha\",\"beta\",\"gamma\"],\n",
    "                    \n",
    "                    # =============================================================================\n",
    "                    # # [Additional settings]\n",
    "                    # =============================================================================\n",
    "                    \"residues_threshold\":0.25,\n",
    "                    \"nb_spots_global_threshold\":8,\n",
    "                    \"nb_spots_consider\" : 500,\n",
    "                    # User defined orientation matrix supplied in a file\n",
    "                    \"use_om_user\" : False,\n",
    "                    \"path_user_OM\" : \"\",\n",
    "                    }    \n",
    "    # =============================================================================\n",
    "    # END OF USER INPUT\n",
    "    # ============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa0a493-5a72-4c14-be8d-01d54f073260",
   "metadata": {},
   "source": [
    "## Generate dataset for neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebdbbba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_dataset_MM = False\n",
    "\n",
    "if __name__ == '__main__':     #enclosing required because of multiprocessing\n",
    "    # global_path: path where all model related files will be saved\n",
    "    global_path = input_params[\"global_path\"]\n",
    "    \n",
    "    if generate_dataset_MM:\n",
    "        import os\n",
    "        from tqdm import trange\n",
    "        ## if LaueToolsNN is properly installed\n",
    "        from lauetoolsnn.utils_lauenn import generate_classHKL, generate_multimat_dataset, \\\n",
    "                                        rmv_freq_class_MM, get_multimaterial_detail\n",
    "                            \n",
    "        material_= input_params[\"material_\"]\n",
    "        n = input_params[\"hkl_max_identify\"]\n",
    "        maximum_angle_to_search = input_params[\"maximum_angle_to_search\"]\n",
    "        step_for_binning = input_params[\"step_for_binning\"]\n",
    "        nb_grains_per_lp = input_params[\"nb_grains_per_lp\"]\n",
    "        grains_nb_simulate = input_params[\"grains_nb_simulate\"]\n",
    "        detectorparameters = input_params[\"detectorparameters\"]\n",
    "        pixelsize = input_params[\"pixelsize\"]\n",
    "        emax = input_params[\"emax\"]\n",
    "        emin = input_params[\"emin\"]\n",
    "        symm_ = input_params[\"symmetry\"]\n",
    "        SG = input_params[\"SG\"]\n",
    "        \n",
    "        if len(material_) > 1:\n",
    "            prefix_mat = material_[0]\n",
    "            for ino, imat in enumerate(material_):\n",
    "                if ino == 0:\n",
    "                    continue\n",
    "                prefix_mat = prefix_mat + \"_\" + imat\n",
    "        else:\n",
    "            prefix_mat = material_[0]\n",
    "        \n",
    "        save_directory = os.path.join(global_path,prefix_mat+input_params[\"prefix\"])\n",
    "\n",
    "        print(\"save directory is : \"+save_directory)\n",
    "        if not os.path.exists(save_directory):\n",
    "            os.makedirs(save_directory)\n",
    "        \n",
    "        ## get unit cell parameters and other details required for simulating Laue patterns\n",
    "        rules, symmetry, lattice_material, \\\n",
    "                                crystal, SG = get_multimaterial_detail(material_, SG, symm_)\n",
    "            \n",
    "        ### generate_classHKL_multimat\n",
    "        ## procedure for generation of GROUND TRUTH classes\n",
    "        # general_diff_cond = True will eliminate the hkl index that does not satisfy the general reflection conditions\n",
    "        # mat_listHKl: provide a numpy array of hkls to be added to the list of output hkl\n",
    "        for ino in trange(len(material_)):\n",
    "            generate_classHKL(n[ino], rules[ino], lattice_material[ino], \\\n",
    "                              symmetry[ino], material_[ino], \\\n",
    "                              crystal=crystal[ino], SG=SG[ino], general_diff_cond=False,\n",
    "                              save_directory=save_directory, write_to_console=print, \\\n",
    "                              ang_maxx = maximum_angle_to_search, \\\n",
    "                              step = step_for_binning, mat_listHKl = None)\n",
    "        \n",
    "        # ## Generate Training and Testing dataset only for the output classes (Laue spot hkls) calculated in the Step 3\n",
    "        # ### Uses multiprocessing library\n",
    "        ############ GENERATING MULTI MATERIAL TRAINING DATA ##############\n",
    "        # data_realism =True ; will introduce noise and partial Laue patterns in the training dataset\n",
    "        # modelp can have either \"random\" for random orientation generation or \"uniform\" for uniform orientation generation\n",
    "        # include_scm (if True; misorientation_angle parameter need to be defined): this parameter introduces misoriented crystal of \n",
    "        # specific angle along a crystal axis in the training dataset    \n",
    "        generate_multimat_dataset(material_=material_, \n",
    "                                 ang_maxx=maximum_angle_to_search,\n",
    "                                 step=step_for_binning, \n",
    "                                 nb_grains=nb_grains_per_lp, \n",
    "                                 grains_nb_simulate=grains_nb_simulate, \n",
    "                                 data_realism = True, \n",
    "                                 detectorparameters=detectorparameters, \n",
    "                                 pixelsize=pixelsize, \n",
    "                                 type_=\"training_data\",\n",
    "                                 var0 = 1, \n",
    "                                 dim1=input_params[\"dim1\"], \n",
    "                                 dim2=input_params[\"dim2\"], \n",
    "                                 removeharmonics=1, \n",
    "                                 save_directory=save_directory,\n",
    "                                 write_to_console=print, \n",
    "                                 emin=emin, \n",
    "                                 emax=emax, \n",
    "                                 modelp = input_params[\"orientation_generation\"],\n",
    "                                 general_diff_rules = False, \n",
    "                                 crystal = crystal,)\n",
    "        \n",
    "        ############ GENERATING TESTING DATA ##############\n",
    "        factor = 5 # validation split for the training dataset  --> corresponds to 20% of total training dataset\n",
    "        generate_multimat_dataset(material_=material_, \n",
    "                                 ang_maxx=maximum_angle_to_search,\n",
    "                                 step=step_for_binning, \n",
    "                                 nb_grains=nb_grains_per_lp, \n",
    "                                 grains_nb_simulate=grains_nb_simulate//factor, \n",
    "                                 data_realism = True, \n",
    "                                 detectorparameters=detectorparameters, \n",
    "                                 pixelsize=pixelsize, \n",
    "                                 type_=\"testing_data\",\n",
    "                                 var0 = 1, \n",
    "                                 dim1=input_params[\"dim1\"], \n",
    "                                 dim2=input_params[\"dim2\"], \n",
    "                                 removeharmonics=1, \n",
    "                                 save_directory=save_directory,\n",
    "                                 write_to_console=print, \n",
    "                                 emin=emin, \n",
    "                                 emax=emax, \n",
    "                                 modelp = input_params[\"orientation_generation\"],\n",
    "                                 general_diff_rules = False, \n",
    "                                 crystal = crystal,)\n",
    "        ### Updating the ClassHKL list by removing the non-common HKL or less frequent HKL from the list\n",
    "        ## The non-common HKL can occur as a result of the detector position and energy used\n",
    "        # freq_rmv: remove output hkl if the training dataset has less tha 100 occurances of the considered hkl (freq_rmv1 for second phase)\n",
    "        # Weights (penalty during training) are also calculated based on the occurance\n",
    "\n",
    "        freq_rmv = input_params[\"classes_with_frequency_to_remove\"]\n",
    "        elements = input_params[\"desired_classes_output\"]\n",
    "        list_hkl_keep = input_params[\"list_hkl_keep\"]\n",
    "        \n",
    "        rmv_freq_class_MM(freq_rmv = freq_rmv, elements = elements,\n",
    "                          save_directory = save_directory, material_ = material_,\n",
    "                          write_to_console = print, progress=None, qapp=None,\n",
    "                          list_hkl_keep = list_hkl_keep)\n",
    "        ## End of data generation for Neural network training: all files are saved in the same folder \n",
    "        ## to be later used for training and prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c4edfe-1ecc-4707-8a21-9b646c98f2cd",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9dda2478-2ff0-4d2a-87d1-f06a7d330446",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_network_MM = False\n",
    "\n",
    "if __name__ == '__main__':     #enclosing required because of multiprocessing\n",
    "\n",
    "    if train_network_MM:\n",
    "        import os\n",
    "        import numpy as np\n",
    "        import _pickle as cPickle\n",
    "        import itertools\n",
    "        from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "        import matplotlib.pyplot as plt\n",
    "        ## if LaueToolsNN is properly installed\n",
    "        from lauetoolsnn.utils_lauenn import array_generator, array_generator_verify, vali_array\n",
    "        from lauetoolsnn.NNmodels import model_arch_general, LoggingCallback\n",
    "        \n",
    "        material_= input_params[\"material_\"]\n",
    "        epochs = input_params[\"epochs\"]\n",
    "        batch_size = input_params[\"batch_size\"] \n",
    "        # ### number of files it will generate fro training\n",
    "        nb_grains_list = []\n",
    "        for ino, imat in enumerate(material_):\n",
    "            nb_grains_list.append(list(range(input_params[\"nb_grains_per_lp\"][ino]+1)))\n",
    "        list_permute = list(itertools.product(*nb_grains_list))\n",
    "        list_permute.pop(0)\n",
    "        print(len(list_permute)*input_params[\"grains_nb_simulate\"])\n",
    "        # ## Get material parameters \n",
    "        # ### Generates a folder with material name and gets material unit cell parameters and symmetry object \n",
    "        # from the get_material_detail function        \n",
    "        if len(material_) > 1:\n",
    "            prefix_mat = material_[0]\n",
    "            for ino, imat in enumerate(material_):\n",
    "                if ino == 0:\n",
    "                    continue\n",
    "                prefix_mat = prefix_mat + \"_\" + imat\n",
    "        else:\n",
    "            prefix_mat = material_[0]\n",
    "        \n",
    "        save_directory = os.path.join(global_path,prefix_mat+input_params[\"prefix\"])\n",
    "\n",
    "        print(\"save directory is : \"+save_directory)\n",
    "        if not os.path.exists(save_directory):\n",
    "            os.makedirs(save_directory)\n",
    "        \n",
    "        classhkl = np.load(save_directory+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_0\"]\n",
    "        angbins = np.load(save_directory+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_1\"]\n",
    "        loc_new = np.load(save_directory+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_2\"]\n",
    "        with open(save_directory+\"//class_weights.pickle\", \"rb\") as input_file:\n",
    "            class_weights = cPickle.load(input_file)\n",
    "        class_weights = class_weights[0]\n",
    "        \n",
    "        # ##  Training\n",
    "        # model save directory and filename\n",
    "        if len(material_) > 1:\n",
    "            prefix_mat = material_[0]\n",
    "            for ino, imat in enumerate(material_):\n",
    "                if ino == 0:\n",
    "                    continue\n",
    "                prefix_mat = prefix_mat + \"_\" + imat\n",
    "        else:\n",
    "            prefix_mat = material_[0]\n",
    "            \n",
    "        model_name = os.path.join(save_directory,\"model_\"+prefix_mat)\n",
    "        \n",
    "        # Define model and train\n",
    "        model = model_arch_general( len(angbins)-1, len(classhkl),\n",
    "                                    kernel_coeff = 1e-5,\n",
    "                                    bias_coeff = 1e-6,\n",
    "                                    lr = 1e-3,)\n",
    "\n",
    "        # Save model config and weights\n",
    "        model_json = model.to_json()\n",
    "        with open(model_name+\".json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)  \n",
    "    \n",
    "        ## temp function to quantify the spots and classes present in a batch\n",
    "        trainy_inbatch = array_generator_verify(save_directory+\"//training_data\", batch_size, \n",
    "                                                len(classhkl), loc_new, print)\n",
    "        print(\"Number of spots in a batch of %i files : %i\" %(batch_size, len(trainy_inbatch)))\n",
    "        print(\"Min, Max class ID is %i, %i\" %(np.min(trainy_inbatch), np.max(trainy_inbatch)))\n",
    "        \n",
    "        ## Batch loading for numpy grain files (Keep low value to avoid overcharging the RAM)\n",
    "        nb_grains_list = []\n",
    "        for ino, imat in enumerate(material_):\n",
    "            nb_grains_list.append(list(range(input_params[\"nb_grains_per_lp\"][ino]+1)))\n",
    "        list_permute = list(itertools.product(*nb_grains_list))\n",
    "        list_permute.pop(0)\n",
    "        steps_per_epoch = len(list_permute)*(input_params[\"grains_nb_simulate\"])//batch_size        \n",
    "        val_steps_per_epoch = int(steps_per_epoch / 5)\n",
    "        if steps_per_epoch == 0:\n",
    "            steps_per_epoch = 1\n",
    "        if val_steps_per_epoch == 0:\n",
    "            val_steps_per_epoch = 1 \n",
    "            \n",
    "        ## Load generator objects from filepaths (iterators for Training and Testing datasets)\n",
    "        training_data_generator = array_generator(save_directory+\"//training_data\", batch_size,                                          \n",
    "                                                  len(classhkl), loc_new, print)\n",
    "        testing_data_generator = array_generator(save_directory+\"//testing_data\", batch_size,                                           \n",
    "                                                 len(classhkl), loc_new, print)\n",
    "        \n",
    "        \n",
    "        ######### TRAIN THE DATA\n",
    "        es = EarlyStopping(monitor='val_accuracy', mode='max', patience=2)\n",
    "        ms = ModelCheckpoint(save_directory+\"//best_val_acc_model.h5\", monitor='val_accuracy', \n",
    "                              mode='max', save_best_only=True)\n",
    "        lc = LoggingCallback(None, None, None, model, model_name)\n",
    "        ## Fitting function\n",
    "        stats_model = model.fit(\n",
    "                                training_data_generator, \n",
    "                                epochs=epochs, \n",
    "                                steps_per_epoch=steps_per_epoch,\n",
    "                                validation_data=testing_data_generator,\n",
    "                                validation_steps=val_steps_per_epoch,\n",
    "                                verbose=1,\n",
    "                                class_weight=class_weights,\n",
    "                                callbacks=[es, ms, lc]\n",
    "                                )          \n",
    "        # serialize weights to HDF5\n",
    "        model.save_weights(model_name+\".h5\")\n",
    "        print(\"Saved model to disk\")\n",
    "        \n",
    "        print( \"Training Accuracy: \"+str( stats_model.history['accuracy'][-1]))\n",
    "        print( \"Training Loss: \"+str( stats_model.history['loss'][-1]))\n",
    "        print( \"Validation Accuracy: \"+str( stats_model.history['val_accuracy'][-1]))\n",
    "        print( \"Validation Loss: \"+str( stats_model.history['val_loss'][-1]))\n",
    "        \n",
    "        # Plot the accuracy/loss v Epochs\n",
    "        epochs = range(1, len(model.history.history['loss']) + 1)\n",
    "        fig, ax = plt.subplots(1,2)\n",
    "        ax[0].plot(epochs, model.history.history['loss'], 'r', label='Training loss')\n",
    "        ax[0].plot(epochs, model.history.history['val_loss'], 'r', ls=\"dashed\", label='Validation loss')\n",
    "        ax[0].legend()\n",
    "        ax[1].plot(epochs, model.history.history['accuracy'], 'g', label='Training Accuracy')\n",
    "        ax[1].plot(epochs, model.history.history['val_accuracy'], 'g', ls=\"dashed\", label='Validation Accuracy')\n",
    "        ax[1].legend()\n",
    "        plt.savefig(save_directory+\"//loss_accuracy_\"+prefix_mat+\".png\", bbox_inches='tight',format='png', dpi=1000)\n",
    "        plt.close()\n",
    "        \n",
    "        text_file = open(save_directory+\"//loss_accuracy_logger_\"+prefix_mat+\".txt\", \"w\")\n",
    "        text_file.write(\"# EPOCH, LOSS, VAL_LOSS, ACCURACY, VAL_ACCURACY\" + \"\\n\")\n",
    "        for inj in range(len(epochs)):\n",
    "            string1 = str(epochs[inj]) + \",\"+ str(model.history.history['loss'][inj])+\\\n",
    "                            \",\"+str(model.history.history['val_loss'][inj])+\",\"+str(model.history.history['accuracy'][inj])+\\\n",
    "                            \",\"+str(model.history.history['val_accuracy'][inj])+\" \\n\"  \n",
    "            text_file.write(string1)\n",
    "        text_file.close() \n",
    "\n",
    "        # ## Stats on the trained model with sklearn metrics\n",
    "        from sklearn.metrics import classification_report\n",
    "        ## verify the statistics\n",
    "        x_test, y_test = vali_array(save_directory+\"//testing_data\", 50, len(classhkl), loc_new, print)\n",
    "        y_test = np.argmax(y_test, axis=-1)\n",
    "        y_pred = np.argmax(model.predict(x_test), axis=-1)\n",
    "        print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02734465-4f6a-40f5-b322-c426e4b36b0a",
   "metadata": {},
   "source": [
    "## Index Laue patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59988fb3-5ecf-45d1-ada2-9671dec9e30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing settings file in /home/esrf/purushot/anaconda3/envs/lauenn/lib/python3.7/site-packages/lauetoolsnn/settings.ini\n",
      "Directory where trained model is stored : /home/esrf/purushot/Desktop/LaueNN_tutorial/LaueNN_script/GaN_Si_MultiMaterial\n",
      "expected 2 files based on the XY grid (1,2) defined by user\n",
      "and found 2 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting for /home/esrf/purushot/Desktop/LaueNN_tutorial/LaueNN_script/nw1_0000.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 41.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skimage mode (strict constraints) of PeakSearch is used for the PeakSearch\n",
      "Predicting for /home/esrf/purushot/Desktop/LaueNN_tutorial/LaueNN_script/nw1_0001.tif\n",
      "Skimage mode (strict constraints) of PeakSearch is used for the PeakSearch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_prediction_MM = True\n",
    "\n",
    "if __name__ == '__main__':     #enclosing required because of multiprocessing\n",
    "\n",
    "    if run_prediction_MM:\n",
    "        ## Import modules used for this Notebook\n",
    "        import numpy as np\n",
    "        import os\n",
    "        import multiprocessing\n",
    "        import time, datetime\n",
    "        import glob, re\n",
    "        import configparser\n",
    "        from itertools import accumulate\n",
    "        ## if LaueToolsNN is properly installed\n",
    "        from lauetoolsnn.utils_lauenn import get_multimaterial_detail, new_MP_multimat_function, resource_path, global_plots_MM\n",
    "        from lauetoolsnn.lauetools import dict_LaueTools as dictLT\n",
    "        from lauetoolsnn.NNmodels import read_hdf5\n",
    "        \n",
    "        import _pickle as cPickle\n",
    "        from tqdm import tqdm\n",
    "        ncpu = multiprocessing.cpu_count()\n",
    "        \n",
    "        material_= input_params[\"material_\"]\n",
    "        detectorparameters = input_params[\"detectorparameters\"]\n",
    "        pixelsize = input_params[\"pixelsize\"]\n",
    "        emax = input_params[\"emax\"]\n",
    "        emin = input_params[\"emin\"]\n",
    "        dim1 = input_params[\"dim1\"]\n",
    "        dim2 = input_params[\"dim2\"]\n",
    "        symm_ = input_params[\"symmetry\"]\n",
    "        SG = input_params[\"SG\"]\n",
    "        tolerance = input_params[\"matrix_tolerance\"]\n",
    "        tolerance_strain = input_params[\"tolerance_strain_refinement\"]\n",
    "        strain_free_parameters = input_params[\"free_parameters\"]\n",
    "        material_limit = input_params[\"material_limit\"]\n",
    "        material_phase_always_present = input_params[\"material_phase_always_present\"]\n",
    "        model_annote = \"DNN\"\n",
    "\n",
    "        ## Requirements\n",
    "        ## Experimental peak search parameters in case of RAW LAUE PATTERNS from detector\n",
    "        intensity_threshold = input_params[\"intensity_threshold\"]\n",
    "        boxsize = input_params[\"boxsize\"]\n",
    "        fit_peaks_gaussian = input_params[\"fit_peaks_gaussian\"]\n",
    "        FitPixelDev = input_params[\"FitPixelDev\"]\n",
    "        NumberMaxofFits = input_params[\"NumberMaxofFits\"]\n",
    "        bkg_treatment = \"A-B\"\n",
    "        mode_peaksearch = input_params[\"mode\"]\n",
    "        \n",
    "        ubmat = input_params[\"UB_matrix_to_detect\"] # How many orientation matrix to detect per Laue pattern\n",
    "        mode_spotCycle = input_params[\"mode_spotCycle\"] ## mode of calculation\n",
    "        use_previous_UBmatrix_name = input_params[\"use_previous\"] ## Try previous indexation solutions to speed up the process\n",
    "        strain_calculation = input_params[\"strain_compute\"] ## Strain refinement is required or not\n",
    "        ccd_label_global = input_params[\"ccd_label\"]\n",
    "        \n",
    "        ## Parameters to control the orientation matrix indexation\n",
    "        softmax_threshold_global = input_params[\"softmax_threshold_global\"] # softmax_threshold of the Neural network to consider\n",
    "        mr_threshold_global = 0.90 # match rate threshold to accept a solution immediately\n",
    "        cap_matchrate = input_params[\"cap_matchrate\"] * 100 ## any UB matrix providing MR less than this will be ignored\n",
    "        coeff = 0.30            ## coefficient to calculate the overlap of two solutions\n",
    "        coeff_overlap = input_params[\"coeff_overlap\"]    ##10% spots overlap is allowed with already indexed orientation\n",
    "\n",
    "        ## Additional parameters to refine the orientation matrix construction process\n",
    "        use_om_user = str(input_params[\"use_om_user\"]).lower()\n",
    "        path_user_OM = input_params[\"path_user_OM\"]\n",
    "        nb_spots_consider = input_params[\"nb_spots_consider\"]\n",
    "        residues_threshold = input_params[\"residues_threshold\"]\n",
    "        nb_spots_global_threshold = input_params[\"nb_spots_global_threshold\"]\n",
    "        option_global = \"v2\"\n",
    "        additional_expression = [\"none\"] # for strain assumptions, like a==b for HCP\n",
    "        \n",
    "        config_setting = configparser.ConfigParser()\n",
    "        filepath = resource_path('settings.ini')\n",
    "        print(\"Writing settings file in \" + filepath)\n",
    "        config_setting.read(filepath)\n",
    "        config_setting.set('CALLER', 'residues_threshold',str(residues_threshold))\n",
    "        config_setting.set('CALLER', 'nb_spots_global_threshold',str(nb_spots_global_threshold))\n",
    "        config_setting.set('CALLER', 'option_global',option_global)\n",
    "        config_setting.set('CALLER', 'use_om_user',use_om_user)\n",
    "        config_setting.set('CALLER', 'nb_spots_consider',str(nb_spots_consider))\n",
    "        config_setting.set('CALLER', 'path_user_OM',str(path_user_OM))\n",
    "        config_setting.set('CALLER', 'intensity', str(intensity_threshold))\n",
    "        config_setting.set('CALLER', 'boxsize', str(boxsize))\n",
    "        config_setting.set('CALLER', 'pixdev', str(FitPixelDev))\n",
    "        config_setting.set('CALLER', 'cap_softmax', str(softmax_threshold_global))\n",
    "        config_setting.set('CALLER', 'cap_mr', str(cap_matchrate/100.))\n",
    "        config_setting.set('CALLER', 'strain_free_parameters', \",\".join(strain_free_parameters))\n",
    "        config_setting.set('CALLER', 'additional_expression', \",\".join(additional_expression))\n",
    "        config_setting.set('CALLER', 'mode_peaksearch', str(mode_peaksearch))\n",
    "        with open(filepath, 'w') as configfile:\n",
    "            config_setting.write(configfile)\n",
    "\n",
    "        if len(material_) > 1:\n",
    "            prefix_mat = material_[0]\n",
    "            for ino, imat in enumerate(material_):\n",
    "                if ino == 0:\n",
    "                    continue\n",
    "                prefix_mat = prefix_mat + \"_\" + imat\n",
    "        else:\n",
    "            prefix_mat = material_[0]\n",
    "        \n",
    "        model_direc = os.path.join(global_path,prefix_mat+input_params[\"prefix\"])\n",
    "        \n",
    "        if not os.path.exists(model_direc):\n",
    "            print(\"The directory doesn't exists; please veify the path\")\n",
    "        else:\n",
    "            print(\"Directory where trained model is stored : \"+model_direc)\n",
    "            \n",
    "        ## get unit cell parameters and other details required for simulating Laue patterns\n",
    "        rules, symmetry, lattice_material, \\\n",
    "                            crystal, SG = get_multimaterial_detail(material_, SG, symm_)\n",
    "\n",
    "        filenameDirec = input_params[\"experimental_directory\"]\n",
    "        experimental_prefix = input_params[\"experimental_prefix\"]\n",
    "        lim_x, lim_y = input_params[\"grid_size_x\"], input_params[\"grid_size_y\"] \n",
    "        format_file = dictLT.dict_CCD[ccd_label_global][7]\n",
    "        \n",
    "        hkl_all_class0 = []\n",
    "        for ino, imat in enumerate(material_):\n",
    "            with open(model_direc+\"//classhkl_data_nonpickled_\"+imat+\".pickle\", \"rb\") as input_file:\n",
    "                hkl_all_class_load = cPickle.load(input_file)[0]\n",
    "            hkl_all_class0.append(hkl_all_class_load)\n",
    "            \n",
    "        ## load model related files and generate the model\n",
    "        classhkl = np.load(model_direc+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_0\"]\n",
    "        angbins = np.load(model_direc+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_1\"]\n",
    "        \n",
    "        # from two phase training dataset FIX\n",
    "        # if len(material_) <= 2: \n",
    "        #     ind_mat0 = np.load(model_direc+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_5\"]\n",
    "        #     ind_mat1 = np.load(model_direc+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_6\"] \n",
    "        #     ind_mat = [ind_mat0, ind_mat0+ind_mat1]\n",
    "        # else:\n",
    "        ##Below lines are for dataset generated with multimat code\n",
    "        ind_mat_all = np.load(model_direc+\"//MOD_grain_classhkl_angbin.npz\",allow_pickle=True)[\"arr_5\"]\n",
    "        ind_mat = []\n",
    "        for inni in ind_mat_all:\n",
    "            ind_mat.append(len(inni))\n",
    "        ind_mat = [int(item) for item in accumulate(ind_mat)]\n",
    "        \n",
    "        # json_file = open(model_direc+\"//model_\"+prefix_mat+\".json\", 'r')\n",
    "        load_weights = model_direc + \"//model_\"+prefix_mat+\".h5\"\n",
    "        wb = read_hdf5(load_weights)\n",
    "        temp_key = list(wb.keys())\n",
    "        \n",
    "        ct = time.time()\n",
    "        now = datetime.datetime.fromtimestamp(ct)\n",
    "        c_time = now.strftime(\"%Y-%m-%d_%H-%M-%S\")   \n",
    "\n",
    "        ## Step 3: Initialize variables and prepare arguments for multiprocessing module\n",
    "\n",
    "        col = [[] for i in range(int(ubmat))]\n",
    "        colx = [[] for i in range(int(ubmat))]\n",
    "        coly = [[] for i in range(int(ubmat))]\n",
    "        rotation_matrix = [[] for i in range(int(ubmat))]\n",
    "        strain_matrix = [[] for i in range(int(ubmat))]\n",
    "        strain_matrixs = [[] for i in range(int(ubmat))]\n",
    "        match_rate = [[] for i in range(int(ubmat))]\n",
    "        spots_len = [[] for i in range(int(ubmat))]\n",
    "        iR_pix = [[] for i in range(int(ubmat))]\n",
    "        fR_pix = [[] for i in range(int(ubmat))]\n",
    "        mat_global = [[] for i in range(int(ubmat))]\n",
    "        best_match = [[] for i in range(int(ubmat))]\n",
    "        spots1_global = [[] for i in range(int(ubmat))]\n",
    "        for i in range(int(ubmat)):\n",
    "            col[i].append(np.zeros((lim_x*lim_y,3)))\n",
    "            colx[i].append(np.zeros((lim_x*lim_y,3)))\n",
    "            coly[i].append(np.zeros((lim_x*lim_y,3)))\n",
    "            rotation_matrix[i].append(np.zeros((lim_x*lim_y,3,3)))\n",
    "            strain_matrix[i].append(np.zeros((lim_x*lim_y,3,3)))\n",
    "            strain_matrixs[i].append(np.zeros((lim_x*lim_y,3,3)))\n",
    "            match_rate[i].append(np.zeros((lim_x*lim_y,1)))\n",
    "            spots_len[i].append(np.zeros((lim_x*lim_y,1)))\n",
    "            iR_pix[i].append(np.zeros((lim_x*lim_y,1)))\n",
    "            fR_pix[i].append(np.zeros((lim_x*lim_y,1)))\n",
    "            mat_global[i].append(np.zeros((lim_x*lim_y,1)))\n",
    "            best_match[i].append([[] for jk in range(lim_x*lim_y)])\n",
    "            spots1_global[i].append([[] for jk in range(lim_x*lim_y)])\n",
    "\n",
    "        # =============================================================================\n",
    "        #         ## Multi-processing routine\n",
    "        # =============================================================================        \n",
    "        ## Number of files to generate\n",
    "        grid_files = np.zeros((lim_x,lim_y))\n",
    "        filenm = np.chararray((lim_x,lim_y), itemsize=1000)\n",
    "        grid_files = grid_files.ravel()\n",
    "        filenm = filenm.ravel()\n",
    "        count_global = lim_x * lim_y\n",
    "        list_of_files = glob.glob(filenameDirec+'//'+experimental_prefix+'*.'+format_file)\n",
    "        ## sort files\n",
    "        list_of_files.sort(key=lambda var:[int(x) if x.isdigit() else x for x in re.findall(r'[^0-9]|[0-9]+', var)])\n",
    "\n",
    "        if len(list_of_files) == count_global:\n",
    "            for ii in range(len(list_of_files)):\n",
    "                grid_files[ii] = ii\n",
    "                filenm[ii] = list_of_files[ii]     \n",
    "            print(\"expected \"+str(count_global)+\" files based on the XY grid (\"+str(lim_x)+\",\"+str(lim_y)+\") defined by user\")\n",
    "            print(\"and found \"+str(len(list_of_files))+\" files\")\n",
    "        else:\n",
    "            print(\"expected \"+str(count_global)+\" files based on the XY grid (\"+str(lim_x)+\",\"+str(lim_y)+\") defined by user\")\n",
    "            print(\"But found \"+str(len(list_of_files))+\" files (either all data is not written yet or maybe XY grid definition is not proper)\")\n",
    "            digits = len(str(count_global))\n",
    "            digits = max(digits,4)\n",
    "            # Temp fix\n",
    "            for ii in range(count_global):\n",
    "                text = str(ii)\n",
    "                if ii < 10000:\n",
    "                    string = text.zfill(4)\n",
    "                else:\n",
    "                    string = text.zfill(5)\n",
    "                file_name_temp = filenameDirec+'//'+experimental_prefix + string+'.'+format_file\n",
    "                ## store it in a grid \n",
    "                filenm[ii] = file_name_temp\n",
    "\n",
    "        check = np.zeros((count_global,int(ubmat)))\n",
    "        # =============================================================================\n",
    "        blacklist = None\n",
    "\n",
    "        ### Create a COR directory to be loaded in LaueTools\n",
    "        cor_file_directory = filenameDirec + \"//\" + experimental_prefix+\"CORfiles\"\n",
    "        if list_of_files[0].split(\".\")[-1] in ['cor',\"COR\",\"Cor\"]:\n",
    "            cor_file_directory = filenameDirec \n",
    "        if not os.path.exists(cor_file_directory):\n",
    "            os.makedirs(cor_file_directory)\n",
    "\n",
    "        try_prevs = False\n",
    "        files_treated = []\n",
    "\n",
    "        valu12 = [[ filenm[ii].decode(), \n",
    "                    ii,\n",
    "                    rotation_matrix,\n",
    "                    strain_matrix,\n",
    "                    strain_matrixs,\n",
    "                    col,\n",
    "                    colx,\n",
    "                    coly,\n",
    "                    match_rate,\n",
    "                    spots_len, \n",
    "                    iR_pix, \n",
    "                    fR_pix,\n",
    "                    best_match,\n",
    "                    mat_global,\n",
    "                    check,\n",
    "                    detectorparameters,\n",
    "                    pixelsize,\n",
    "                    angbins,\n",
    "                    classhkl,\n",
    "                    hkl_all_class0,\n",
    "                    emin,\n",
    "                    emax,\n",
    "                    material_,\n",
    "                    symmetry,\n",
    "                    lim_x,\n",
    "                    lim_y,\n",
    "                    strain_calculation, \n",
    "                    ind_mat, \n",
    "                    model_direc, \n",
    "                    tolerance,\n",
    "                    int(ubmat), ccd_label_global, \n",
    "                    None,\n",
    "                    float(intensity_threshold),\n",
    "                    int(boxsize),\n",
    "                    bkg_treatment,\n",
    "                    filenameDirec, \n",
    "                    experimental_prefix,\n",
    "                    blacklist,\n",
    "                    None,\n",
    "                    files_treated,\n",
    "                    try_prevs, ## try previous is kept true, incase if its stuck in loop\n",
    "                    wb,\n",
    "                    temp_key,\n",
    "                    cor_file_directory,\n",
    "                    mode_spotCycle,\n",
    "                    softmax_threshold_global,\n",
    "                    mr_threshold_global,\n",
    "                    cap_matchrate,\n",
    "                    tolerance_strain,\n",
    "                    NumberMaxofFits,\n",
    "                    fit_peaks_gaussian,\n",
    "                    FitPixelDev,\n",
    "                    coeff,\n",
    "                    coeff_overlap,\n",
    "                    material_limit,\n",
    "                    use_previous_UBmatrix_name,\n",
    "                    material_phase_always_present,\n",
    "                    crystal,\n",
    "                    strain_free_parameters,\n",
    "                    model_annote] for ii in range(count_global)]\n",
    "\n",
    "        #% Launch multiprocessing prediction     \n",
    "        args = zip(valu12)\n",
    "        with multiprocessing.Pool(ncpu) as pool:\n",
    "            results = pool.starmap(new_MP_multimat_function, tqdm(args, total=len(valu12)), chunksize=1)\n",
    "\n",
    "            for r in results:\n",
    "                r_message_mpdata = r\n",
    "                strain_matrix_mpdata, strain_matrixs_mpdata, rotation_matrix_mpdata, col_mpdata,\\\n",
    "                colx_mpdata, coly_mpdata, match_rate_mpdata, mat_global_mpdata,\\\n",
    "                    cnt_mpdata, meta_mpdata, files_treated_mpdata, spots_len_mpdata, \\\n",
    "                        iR_pixel_mpdata, fR_pixel_mpdata, best_match_mpdata, check_mpdata = r_message_mpdata\n",
    "\n",
    "                for i_mpdata in files_treated_mpdata:\n",
    "                    files_treated.append(i_mpdata)\n",
    "\n",
    "                for intmat_mpdata in range(int(ubmat)):\n",
    "                    check[cnt_mpdata,intmat_mpdata] = check_mpdata[cnt_mpdata,intmat_mpdata]\n",
    "                    mat_global[intmat_mpdata][0][cnt_mpdata] = mat_global_mpdata[intmat_mpdata][0][cnt_mpdata]\n",
    "                    strain_matrix[intmat_mpdata][0][cnt_mpdata,:,:] = strain_matrix_mpdata[intmat_mpdata][0][cnt_mpdata,:,:]\n",
    "                    strain_matrixs[intmat_mpdata][0][cnt_mpdata,:,:] = strain_matrixs_mpdata[intmat_mpdata][0][cnt_mpdata,:,:]\n",
    "                    rotation_matrix[intmat_mpdata][0][cnt_mpdata,:,:] = rotation_matrix_mpdata[intmat_mpdata][0][cnt_mpdata,:,:]\n",
    "                    col[intmat_mpdata][0][cnt_mpdata,:] = col_mpdata[intmat_mpdata][0][cnt_mpdata,:]\n",
    "                    colx[intmat_mpdata][0][cnt_mpdata,:] = colx_mpdata[intmat_mpdata][0][cnt_mpdata,:]\n",
    "                    coly[intmat_mpdata][0][cnt_mpdata,:] = coly_mpdata[intmat_mpdata][0][cnt_mpdata,:]\n",
    "                    match_rate[intmat_mpdata][0][cnt_mpdata] = match_rate_mpdata[intmat_mpdata][0][cnt_mpdata]\n",
    "                    spots_len[intmat_mpdata][0][cnt_mpdata] = spots_len_mpdata[intmat_mpdata][0][cnt_mpdata]\n",
    "                    iR_pix[intmat_mpdata][0][cnt_mpdata] = iR_pixel_mpdata[intmat_mpdata][0][cnt_mpdata]\n",
    "                    fR_pix[intmat_mpdata][0][cnt_mpdata] = fR_pixel_mpdata[intmat_mpdata][0][cnt_mpdata]\n",
    "                    best_match[intmat_mpdata][0][cnt_mpdata] = best_match_mpdata[intmat_mpdata][0][cnt_mpdata]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc02537-31ad-469e-a7c7-b11891954ddd",
   "metadata": {},
   "source": [
    "## save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6cde8a9-5b87-4216-8b12-39e911fd658b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data saved in  /home/esrf/purushot/Desktop/LaueNN_tutorial/LaueNN_script/GaN_Si_MultiMaterial//results_GaN_Si_2022-11-15_00-45-42\n"
     ]
    }
   ],
   "source": [
    "from lauetoolsnn.utils_lauenn import global_plots_MM, write_average_orientationMM, convert_pickle_to_hdf5MM, write_prediction_statsMM, write_MTEXdataMM\n",
    "#% Save results\n",
    "save_directory_ = model_direc+\"//results_\"+prefix_mat+\"_\"+c_time\n",
    "if not os.path.exists(save_directory_):\n",
    "    os.makedirs(save_directory_)\n",
    "\n",
    "## intermediate saving of pickle objects with results\n",
    "np.savez_compressed(save_directory_+ \"//results.npz\", \n",
    "                    best_match, mat_global, rotation_matrix, strain_matrix, \n",
    "                    strain_matrixs, col, colx, coly, match_rate, files_treated,\n",
    "                    lim_x, lim_y, spots_len, iR_pix, fR_pix,\n",
    "                    material_)\n",
    "## intermediate saving of pickle objects with results\n",
    "with open(save_directory_+ \"//results.pickle\", \"wb\") as output_file:\n",
    "        cPickle.dump([best_match, mat_global, rotation_matrix, strain_matrix, \n",
    "                      strain_matrixs, col, colx, coly, match_rate, files_treated,\n",
    "                      lim_x, lim_y, spots_len, iR_pix, fR_pix,\n",
    "                      material_, lattice_material,\n",
    "                      symmetry, crystal], output_file)\n",
    "print(\"data saved in \", save_directory_)\n",
    "\n",
    "## Lets save also a set of average UB matrix in text file to be used with user_OM setting    \n",
    "try:\n",
    "    write_average_orientationMM(save_directory_, mat_global, rotation_matrix,\n",
    "                                  match_rate, lim_x, lim_y, crystal,\n",
    "                                  radius=10, grain_ang=5, pixel_grain_definition=3)\n",
    "except:\n",
    "    print(\"Error in the average orientaiton module\")\n",
    "    \n",
    "try:\n",
    "    convert_pickle_to_hdf5MM(save_directory_, files_treated, rotation_matrix, strain_matrix, \n",
    "                           strain_matrixs, match_rate, spots_len, iR_pix, \n",
    "                           fR_pix, colx, coly, col, mat_global,\n",
    "                           material_, lim_x, lim_y)\n",
    "except:\n",
    "    print(\"Error in the hdf5 conversion module\")\n",
    "    \n",
    "try:\n",
    "    write_prediction_statsMM(save_directory_, material_, files_treated,\\\n",
    "                            lim_x, lim_y, best_match, strain_matrixs, strain_matrix, iR_pix,\\\n",
    "                            fR_pix,  mat_global)\n",
    "except:\n",
    "    print(\"Error in the Prediction statistic module\")\n",
    "    \n",
    "## Need to rewrite the function for write_prediction_stats in MultiMaterial case\n",
    "#write_MTEXdata(save_directory_, material_, material1_, rotation_matrix,\\\n",
    "#                   lattice_material, lattice_material1, lim_x, lim_y, mat_global,\\\n",
    "#                    input_params[\"symmetry\"][0], input_params[\"symmetry\"][1])\n",
    "\n",
    "try:\n",
    "    global_plots_MM(lim_x, lim_y, rotation_matrix, strain_matrix, strain_matrixs, \n",
    "                  col, colx, coly, match_rate, mat_global, spots_len, \n",
    "                  iR_pix, fR_pix, save_directory_, material_,\n",
    "                  match_rate_threshold=5, bins=30)\n",
    "except:\n",
    "    print(\"Error in the global plots module\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b489c1a5-1301-4f4d-ac16-cbb79d876dd3",
   "metadata": {},
   "source": [
    "## Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2e028f6-1658-476f-abff-d28e1e16d9dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/x-hdf5": "/home/esrf/purushot/Desktop/LaueNN_tutorial/LaueNN_script/GaN_Si_MultiMaterial/results_GaN_Si_2022-11-15_00-45-42/grain_all.h5",
      "text/plain": [
       "<jupyterlab_h5web.widget.H5Web object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jupyterlab_h5web import H5Web\n",
    "H5Web(os.path.join(save_directory_,\"grain_all.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6245d458-3e78-4d9f-8fc8-183dee4392cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lauenn",
   "language": "python",
   "name": "lauenn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
