{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61bda473-6e51-4914-826e-21c50f0f61d1",
   "metadata": {},
   "source": [
    "# Notebook script for Training the neural network (supports More than 2 phases)\n",
    "\n",
    "### Load the data generated in Step 1\n",
    "### Define the Neural network architecture\n",
    "### Train the network\n",
    "### Do prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2ad7bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':     #enclosing required because of multiprocessing\n",
    "\n",
    "    ## Import modules used for this Notebook\n",
    "    import os\n",
    "    import json\n",
    "    \n",
    "    ## Get the path of the lauetoolsnn library\n",
    "    import lauetoolsnn\n",
    "    laueNN_path = os.path.dirname(lauetoolsnn.__file__)\n",
    "    print(\"LaueNN path is\", laueNN_path)\n",
    "    \n",
    "    ## Load the json of material and extinctions\n",
    "    with open(os.path.join(laueNN_path, 'lauetools\\material.json'),'r') as f:\n",
    "        dict_Materials = json.load(f)\n",
    "    with open(os.path.join(laueNN_path, 'lauetools\\extinction.json'),'r') as f:\n",
    "        extinction_json = json.load(f)\n",
    "\n",
    "    ## Modify the dictionary values to add new entries\n",
    "    dict_Materials[\"GaN\"] = [\"GaN\", [3.189, 3.189, 5.185, 90, 90, 120], \"wurtzite\"]\n",
    "    dict_Materials[\"Si\"] = [\"Si\", [5.4309, 5.4309, 5.4309, 90, 90, 90], \"dia\"]\n",
    "\n",
    "    extinction_json[\"wurtzite\"] = \"wurtzite\"\n",
    "    extinction_json[\"dia\"] = \"dia\"\n",
    "\n",
    "    ## verify if extinction is present in CrystalParameters.py file of lauetools (Manually done for now)\n",
    "\n",
    "    ## dump the json back with new values\n",
    "    with open(os.path.join(laueNN_path, 'lauetools\\material.json'), 'w') as fp:\n",
    "        json.dump(dict_Materials, fp)\n",
    "    with open(os.path.join(laueNN_path, 'lauetools\\extinction.json'), 'w') as fp:\n",
    "        json.dump(extinction_json, fp)\n",
    "\n",
    "    ## Verify if the material is added to the library or not;\n",
    "    from lauetoolsnn.lauetools.dict_LaueTools import dict_Materials\n",
    "    ## if not, restart the console\n",
    "    print(dict_Materials[\"GaN\"])\n",
    "    print(dict_Materials[\"Si\"])\n",
    "    \n",
    "    # =============================================================================\n",
    "    # Step 0: Define the dictionary with all parameters \n",
    "    # =============================================================================\n",
    "    ## User Input dictionary with parameters\n",
    "    ## In case of only one phase/material, keep same value for material_ and material1_ key\n",
    "    input_params = {\n",
    "                    \"global_path\" : r\"C:\\Users\\purushot\\Desktop\\LaueNN_script\",\n",
    "                    \"prefix\" : \"_MMrand\",                 ## prefix for the folder to be created for training dataset\n",
    "\n",
    "                    \"material_\": [\"GaN\", \"Si\"],             ## same key as used in dict_LaueTools\n",
    "                    \"symmetry\": [\"hexagonal\", \"cubic\"],           ## crystal symmetry of material_\n",
    "                    \"SG\": [191, 227], #186                    ## Space group of material_ (None if not known)\n",
    "                    \"hkl_max_identify\" : [7,5],        ## Maximum hkl index to classify in a Laue pattern\n",
    "                    \"nb_grains_per_lp\" : [2,1],        ## max grains to be generated in a Laue Image\n",
    "\n",
    "                    ## hkl_max_identify : can be \"auto\" or integer: Maximum index of HKL to build output classes\n",
    "                    \n",
    "                    # =============================================================================\n",
    "                    # ## Data generation settings\n",
    "                    # =============================================================================\n",
    "                    \"grains_nb_simulate\" : 500,    ## Number of orientations to generate (takes advantage of crystal symmetry)\n",
    "                    \"classes_with_frequency_to_remove\": [100,100], ## classes_with_frequency_to_remove: HKL class with less appearance than \n",
    "                                                                            ##  specified will be ignored in output\n",
    "                    \"desired_classes_output\": [\"all\",\"all\"], ## desired_classes_output : can be all or an integer: to limit the number of output classes\n",
    "                    \"list_hkl_keep\" : None, #[[(0,0,1)],[(0,0,0)]],\n",
    "                    \"maximum_angle_to_search\":60, ## Angle of radial distribution to reconstruct the histogram (in deg)\n",
    "                    \"step_for_binning\" : 0.1,      ## bin widht of angular radial distribution in degree\n",
    "                    \n",
    "                    # =============================================================================\n",
    "                    #  ## Training parameters\n",
    "                    # =============================================================================\n",
    "                    \"orientation_generation\": \"uniform\", ## could be \"uniform\" or \"random\"\n",
    "                    \"batch_size\":100,               ## batches of files to use while training\n",
    "                    \"epochs\":10,                    ## number of epochs for training\n",
    "\n",
    "                    # =============================================================================\n",
    "                    # ## Detector parameters of the Experimental setup\n",
    "                    # =============================================================================\n",
    "                    ## Sample-detector distance, X center, Y center, two detector angles\n",
    "                    \"detectorparameters\" :  [79.61200, 977.8100, 932.1700, 0.4770000, 0.4470000], \n",
    "                    \"pixelsize\" : 0.0734,          ## Detector pixel size\n",
    "                    \"dim1\":2018,                   ## Dimensions of detector in pixels\n",
    "                    \"dim2\":2016,\n",
    "                    \"emin\" : 5,                    ## Minimum and maximum energy to use for simulating Laue Patterns\n",
    "                    \"emax\" : 22,\n",
    "                    \"ccd_label\" : \"sCMOS\",\n",
    "                    \n",
    "                    # =============================================================================\n",
    "                    # ## Prediction parameters\n",
    "                    # =============================================================================\n",
    "                    \"experimental_directory\": r\"D:\\some_projects\\GaN\\BLC12834\\NW1\",\n",
    "                    \"experimental_prefix\": r\"nw1_\",\n",
    "                    \"grid_size_x\" : 61,            ## Grid X and Y limit to generate the simulated dataset (a rectangular scan region)\n",
    "                    \"grid_size_y\" : 61,\n",
    "                    \n",
    "                    # =============================================================================\n",
    "                    # ## Prediction Settings\n",
    "                    # =============================================================================\n",
    "                    # model_weight_file: if none, it will select by default the latest H5 weight file, else provide a specific model\n",
    "                    # softmax_threshold_global: thresholding to limit the predicted spots search zone\n",
    "                    # cap_matchrate: any UB matrix providing MR less than this will be ignored\n",
    "                    # coeff: should be same as cap_matchrate or no? (this is for when use_previous is True)\n",
    "                    # coeff_overlap: coefficient to limit the overlapping between spots; if more than this, new solution will be computed\n",
    "                    # mode_spotCycle: How to cycle through predicted spots (slow or graphmode )\n",
    "                    \"UB_matrix_to_detect\" : 3,\n",
    "                    \"matrix_tolerance\" : [0.6, 0.6],\n",
    "                    \"material_limit\" : [2, 1],\n",
    "                    \"material_phase_always_present\" : [2,1,1],\n",
    "                    \"softmax_threshold_global\" : 0.85,\n",
    "                    \"cap_matchrate\" : 0.30,\n",
    "                    \"coeff_overlap\" : 0.3,\n",
    "                    \"mode_spotCycle\" : \"graphmode\",\n",
    "                    ##true for few crystal and prefered texture case, otherwise time consuming; advised for single phase alone\n",
    "                    \"use_previous\" : False,\n",
    "                    \n",
    "                    # =============================================================================\n",
    "                    # # [PEAKSEARCH]\n",
    "                    # =============================================================================\n",
    "                    \"intensity_threshold\" : 1,\n",
    "                    \"boxsize\" : 10,\n",
    "                    \"fit_peaks_gaussian\" : 1,\n",
    "                    \"FitPixelDev\" : 15,\n",
    "                    \"NumberMaxofFits\" : 3000,\n",
    "                    \"mode\": \"skimage\",\n",
    "\n",
    "                    # =============================================================================\n",
    "                    # # [STRAINCALCULATION]\n",
    "                    # =============================================================================\n",
    "                    \"strain_compute\" : True,\n",
    "                    \"tolerance_strain_refinement\" : [[0.6,0.5,0.4,0.3,0.2,0.1],\n",
    "                                                     [0.6,0.5,0.4,0.3,0.2,0.1]],\n",
    "                    \"free_parameters\" : [\"b\",\"c\",\"alpha\",\"beta\",\"gamma\"],\n",
    "                    \n",
    "                    # =============================================================================\n",
    "                    # # [Additional settings]\n",
    "                    # =============================================================================\n",
    "                    \"residues_threshold\":0.25,\n",
    "                    \"nb_spots_global_threshold\":8,\n",
    "                    \"nb_spots_consider\" : 500,\n",
    "                    # User defined orientation matrix supplied in a file\n",
    "                    \"use_om_user\" : False,\n",
    "                    \"path_user_OM\" : \"\",\n",
    "                    }\n",
    "\n",
    "    generate_dataset_MM = False\n",
    "    train_network_MM = False\n",
    "    run_prediction_MM = True\n",
    "    prediction_GUI = False\n",
    "    \n",
    "    # =============================================================================\n",
    "    # END OF USER INPUT\n",
    "    # ============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa0a493-5a72-4c14-be8d-01d54f073260",
   "metadata": {},
   "source": [
    "## Launch Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebdbbba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory where training dataset is stored is : C:\\Users\\purushot\\Desktop\\github_version_simple\\lauetoolsnn\\example_notebook_scripts//Cu\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':     #enclosing required because of multiprocessing\n",
    "    # global_path: path where all model related files will be saved\n",
    "    global_path = input_params[\"global_path\"]\n",
    "    \n",
    "    if generate_dataset_MM:\n",
    "        import os\n",
    "        from tqdm import trange\n",
    "        ## if LaueToolsNN is properly installed\n",
    "        from lauetoolsnn.utils_lauenn import generate_classHKL, generate_multimat_dataset, \\\n",
    "                                        rmv_freq_class_MM, get_multimaterial_detail\n",
    "                            \n",
    "        material_= input_params[\"material_\"]\n",
    "        n = input_params[\"hkl_max_identify\"]\n",
    "        maximum_angle_to_search = input_params[\"maximum_angle_to_search\"]\n",
    "        step_for_binning = input_params[\"step_for_binning\"]\n",
    "        nb_grains_per_lp = input_params[\"nb_grains_per_lp\"]\n",
    "        grains_nb_simulate = input_params[\"grains_nb_simulate\"]\n",
    "        detectorparameters = input_params[\"detectorparameters\"]\n",
    "        pixelsize = input_params[\"pixelsize\"]\n",
    "        emax = input_params[\"emax\"]\n",
    "        emin = input_params[\"emin\"]\n",
    "        symm_ = input_params[\"symmetry\"]\n",
    "        SG = input_params[\"SG\"]\n",
    "        \n",
    "        if len(material_) > 1:\n",
    "            prefix_mat = material_[0]\n",
    "            for ino, imat in enumerate(material_):\n",
    "                if ino == 0:\n",
    "                    continue\n",
    "                prefix_mat = prefix_mat + \"_\" + imat\n",
    "        else:\n",
    "            prefix_mat = material_[0]\n",
    "        \n",
    "        save_directory = os.path.join(global_path,prefix_mat+input_params[\"prefix\"])\n",
    "\n",
    "        print(\"save directory is : \"+save_directory)\n",
    "        if not os.path.exists(save_directory):\n",
    "            os.makedirs(save_directory)\n",
    "        \n",
    "        ## get unit cell parameters and other details required for simulating Laue patterns\n",
    "        rules, symmetry, lattice_material, \\\n",
    "                                crystal, SG = get_multimaterial_detail(material_, SG, symm_)\n",
    "            \n",
    "        ### generate_classHKL_multimat\n",
    "        ## procedure for generation of GROUND TRUTH classes\n",
    "        # general_diff_cond = True will eliminate the hkl index that does not satisfy the general reflection conditions\n",
    "        # mat_listHKl: provide a numpy array of hkls to be added to the list of output hkl\n",
    "        for ino in trange(len(material_)):\n",
    "            generate_classHKL(n[ino], rules[ino], lattice_material[ino], \\\n",
    "                              symmetry[ino], material_[ino], \\\n",
    "                              crystal=crystal[ino], SG=SG[ino], general_diff_cond=False,\n",
    "                              save_directory=save_directory, write_to_console=print, \\\n",
    "                              ang_maxx = maximum_angle_to_search, \\\n",
    "                              step = step_for_binning, mat_listHKl = None)\n",
    "        \n",
    "        # ## Generate Training and Testing dataset only for the output classes (Laue spot hkls) calculated in the Step 3\n",
    "        # ### Uses multiprocessing library\n",
    "        ############ GENERATING MULTI MATERIAL TRAINING DATA ##############\n",
    "        # data_realism =True ; will introduce noise and partial Laue patterns in the training dataset\n",
    "        # modelp can have either \"random\" for random orientation generation or \"uniform\" for uniform orientation generation\n",
    "        # include_scm (if True; misorientation_angle parameter need to be defined): this parameter introduces misoriented crystal of \n",
    "        # specific angle along a crystal axis in the training dataset    \n",
    "        generate_multimat_dataset(material_=material_, \n",
    "                                 ang_maxx=maximum_angle_to_search,\n",
    "                                 step=step_for_binning, \n",
    "                                 nb_grains=nb_grains_per_lp, \n",
    "                                 grains_nb_simulate=grains_nb_simulate, \n",
    "                                 data_realism = True, \n",
    "                                 detectorparameters=detectorparameters, \n",
    "                                 pixelsize=pixelsize, \n",
    "                                 type_=\"training_data\",\n",
    "                                 var0 = 1, \n",
    "                                 dim1=input_params[\"dim1\"], \n",
    "                                 dim2=input_params[\"dim2\"], \n",
    "                                 removeharmonics=1, \n",
    "                                 save_directory=save_directory,\n",
    "                                 write_to_console=print, \n",
    "                                 emin=emin, \n",
    "                                 emax=emax, \n",
    "                                 modelp = input_params[\"orientation_generation\"],\n",
    "                                 general_diff_rules = False, \n",
    "                                 crystal = crystal,)\n",
    "        \n",
    "        ############ GENERATING TESTING DATA ##############\n",
    "        factor = 5 # validation split for the training dataset  --> corresponds to 20% of total training dataset\n",
    "        generate_multimat_dataset(material_=material_, \n",
    "                                 ang_maxx=maximum_angle_to_search,\n",
    "                                 step=step_for_binning, \n",
    "                                 nb_grains=nb_grains_per_lp, \n",
    "                                 grains_nb_simulate=grains_nb_simulate//factor, \n",
    "                                 data_realism = True, \n",
    "                                 detectorparameters=detectorparameters, \n",
    "                                 pixelsize=pixelsize, \n",
    "                                 type_=\"testing_data\",\n",
    "                                 var0 = 1, \n",
    "                                 dim1=input_params[\"dim1\"], \n",
    "                                 dim2=input_params[\"dim2\"], \n",
    "                                 removeharmonics=1, \n",
    "                                 save_directory=save_directory,\n",
    "                                 write_to_console=print, \n",
    "                                 emin=emin, \n",
    "                                 emax=emax, \n",
    "                                 modelp = input_params[\"orientation_generation\"],\n",
    "                                 general_diff_rules = False, \n",
    "                                 crystal = crystal,)\n",
    "        ### Updating the ClassHKL list by removing the non-common HKL or less frequent HKL from the list\n",
    "        ## The non-common HKL can occur as a result of the detector position and energy used\n",
    "        # freq_rmv: remove output hkl if the training dataset has less tha 100 occurances of the considered hkl (freq_rmv1 for second phase)\n",
    "        # Weights (penalty during training) are also calculated based on the occurance\n",
    "\n",
    "        freq_rmv = input_params[\"classes_with_frequency_to_remove\"]\n",
    "        elements = input_params[\"desired_classes_output\"]\n",
    "        list_hkl_keep = input_params[\"list_hkl_keep\"]\n",
    "        \n",
    "        rmv_freq_class_MM(freq_rmv = freq_rmv, elements = elements,\n",
    "                          save_directory = save_directory, material_ = material_,\n",
    "                          write_to_console = print, progress=None, qapp=None,\n",
    "                          list_hkl_keep = list_hkl_keep)\n",
    "        ## End of data generation for Neural network training: all files are saved in the same folder \n",
    "        ## to be later used for training and prediction\n",
    "    \n",
    "    if train_network_MM:\n",
    "        import os\n",
    "        import numpy as np\n",
    "        import _pickle as cPickle\n",
    "        import itertools\n",
    "        from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "        import matplotlib.pyplot as plt\n",
    "        ## if LaueToolsNN is properly installed\n",
    "        from lauetoolsnn.utils_lauenn import array_generator, array_generator_verify, vali_array\n",
    "        from lauetoolsnn.NNmodels import model_arch_general, LoggingCallback\n",
    "        \n",
    "        material_= input_params[\"material_\"]\n",
    "        epochs = input_params[\"epochs\"]\n",
    "        batch_size = input_params[\"batch_size\"] \n",
    "        # ### number of files it will generate fro training\n",
    "        nb_grains_list = []\n",
    "        for ino, imat in enumerate(material_):\n",
    "            nb_grains_list.append(list(range(input_params[\"nb_grains_per_lp\"][ino]+1)))\n",
    "        list_permute = list(itertools.product(*nb_grains_list))\n",
    "        list_permute.pop(0)\n",
    "        print(len(list_permute)*input_params[\"grains_nb_simulate\"])\n",
    "        # ## Get material parameters \n",
    "        # ### Generates a folder with material name and gets material unit cell parameters and symmetry object \n",
    "        # from the get_material_detail function        \n",
    "        if len(material_) > 1:\n",
    "            prefix_mat = material_[0]\n",
    "            for ino, imat in enumerate(material_):\n",
    "                if ino == 0:\n",
    "                    continue\n",
    "                prefix_mat = prefix_mat + \"_\" + imat\n",
    "        else:\n",
    "            prefix_mat = material_[0]\n",
    "        \n",
    "        save_directory = os.path.join(global_path,prefix_mat+input_params[\"prefix\"])\n",
    "\n",
    "        print(\"save directory is : \"+save_directory)\n",
    "        if not os.path.exists(save_directory):\n",
    "            os.makedirs(save_directory)\n",
    "        \n",
    "        classhkl = np.load(save_directory+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_0\"]\n",
    "        angbins = np.load(save_directory+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_1\"]\n",
    "        loc_new = np.load(save_directory+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_2\"]\n",
    "        with open(save_directory+\"//class_weights.pickle\", \"rb\") as input_file:\n",
    "            class_weights = cPickle.load(input_file)\n",
    "        class_weights = class_weights[0]\n",
    "        \n",
    "        # ##  Training\n",
    "        # model save directory and filename\n",
    "        if len(material_) > 1:\n",
    "            prefix_mat = material_[0]\n",
    "            for ino, imat in enumerate(material_):\n",
    "                if ino == 0:\n",
    "                    continue\n",
    "                prefix_mat = prefix_mat + \"_\" + imat\n",
    "        else:\n",
    "            prefix_mat = material_[0]\n",
    "            \n",
    "        model_name = os.path.join(save_directory,\"model_\"+prefix_mat)\n",
    "        \n",
    "        # Define model and train\n",
    "        model = model_arch_general( len(angbins)-1, len(classhkl),\n",
    "                                    kernel_coeff = 1e-5,\n",
    "                                    bias_coeff = 1e-6,\n",
    "                                    lr = 1e-3,)\n",
    "\n",
    "        # Save model config and weights\n",
    "        model_json = model.to_json()\n",
    "        with open(model_name+\".json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)  \n",
    "    \n",
    "        ## temp function to quantify the spots and classes present in a batch\n",
    "        trainy_inbatch = array_generator_verify(save_directory+\"//training_data\", batch_size, \n",
    "                                                len(classhkl), loc_new, print)\n",
    "        print(\"Number of spots in a batch of %i files : %i\" %(batch_size, len(trainy_inbatch)))\n",
    "        print(\"Min, Max class ID is %i, %i\" %(np.min(trainy_inbatch), np.max(trainy_inbatch)))\n",
    "        \n",
    "        ## Batch loading for numpy grain files (Keep low value to avoid overcharging the RAM)\n",
    "        nb_grains_list = []\n",
    "        for ino, imat in enumerate(material_):\n",
    "            nb_grains_list.append(list(range(input_params[\"nb_grains_per_lp\"][ino]+1)))\n",
    "        list_permute = list(itertools.product(*nb_grains_list))\n",
    "        list_permute.pop(0)\n",
    "        steps_per_epoch = len(list_permute)*(input_params[\"grains_nb_simulate\"])//batch_size        \n",
    "        val_steps_per_epoch = int(steps_per_epoch / 5)\n",
    "        if steps_per_epoch == 0:\n",
    "            steps_per_epoch = 1\n",
    "        if val_steps_per_epoch == 0:\n",
    "            val_steps_per_epoch = 1 \n",
    "            \n",
    "        ## Load generator objects from filepaths (iterators for Training and Testing datasets)\n",
    "        training_data_generator = array_generator(save_directory+\"//training_data\", batch_size,                                          \n",
    "                                                  len(classhkl), loc_new, print)\n",
    "        testing_data_generator = array_generator(save_directory+\"//testing_data\", batch_size,                                           \n",
    "                                                 len(classhkl), loc_new, print)\n",
    "        \n",
    "        \n",
    "        ######### TRAIN THE DATA\n",
    "        es = EarlyStopping(monitor='val_accuracy', mode='max', patience=2)\n",
    "        ms = ModelCheckpoint(save_directory+\"//best_val_acc_model.h5\", monitor='val_accuracy', \n",
    "                              mode='max', save_best_only=True)\n",
    "        lc = LoggingCallback(None, None, None, model, model_name)\n",
    "        ## Fitting function\n",
    "        stats_model = model.fit(\n",
    "                                training_data_generator, \n",
    "                                epochs=epochs, \n",
    "                                steps_per_epoch=steps_per_epoch,\n",
    "                                validation_data=testing_data_generator,\n",
    "                                validation_steps=val_steps_per_epoch,\n",
    "                                verbose=1,\n",
    "                                class_weight=class_weights,\n",
    "                                callbacks=[es, ms, lc]\n",
    "                                )          \n",
    "        # serialize weights to HDF5\n",
    "        model.save_weights(model_name+\".h5\")\n",
    "        print(\"Saved model to disk\")\n",
    "        \n",
    "        print( \"Training Accuracy: \"+str( stats_model.history['accuracy'][-1]))\n",
    "        print( \"Training Loss: \"+str( stats_model.history['loss'][-1]))\n",
    "        print( \"Validation Accuracy: \"+str( stats_model.history['val_accuracy'][-1]))\n",
    "        print( \"Validation Loss: \"+str( stats_model.history['val_loss'][-1]))\n",
    "        \n",
    "        # Plot the accuracy/loss v Epochs\n",
    "        epochs = range(1, len(model.history.history['loss']) + 1)\n",
    "        fig, ax = plt.subplots(1,2)\n",
    "        ax[0].plot(epochs, model.history.history['loss'], 'r', label='Training loss')\n",
    "        ax[0].plot(epochs, model.history.history['val_loss'], 'r', ls=\"dashed\", label='Validation loss')\n",
    "        ax[0].legend()\n",
    "        ax[1].plot(epochs, model.history.history['accuracy'], 'g', label='Training Accuracy')\n",
    "        ax[1].plot(epochs, model.history.history['val_accuracy'], 'g', ls=\"dashed\", label='Validation Accuracy')\n",
    "        ax[1].legend()\n",
    "        plt.savefig(save_directory+\"//loss_accuracy_\"+prefix_mat+\".png\", bbox_inches='tight',format='png', dpi=1000)\n",
    "        plt.close()\n",
    "        \n",
    "        text_file = open(save_directory+\"//loss_accuracy_logger_\"+prefix_mat+\".txt\", \"w\")\n",
    "        text_file.write(\"# EPOCH, LOSS, VAL_LOSS, ACCURACY, VAL_ACCURACY\" + \"\\n\")\n",
    "        for inj in range(len(epochs)):\n",
    "            string1 = str(epochs[inj]) + \",\"+ str(model.history.history['loss'][inj])+\\\n",
    "                            \",\"+str(model.history.history['val_loss'][inj])+\",\"+str(model.history.history['accuracy'][inj])+\\\n",
    "                            \",\"+str(model.history.history['val_accuracy'][inj])+\" \\n\"  \n",
    "            text_file.write(string1)\n",
    "        text_file.close() \n",
    "\n",
    "        # ## Stats on the trained model with sklearn metrics\n",
    "        from sklearn.metrics import classification_report\n",
    "        ## verify the statistics\n",
    "        x_test, y_test = vali_array(save_directory+\"//testing_data\", 50, len(classhkl), loc_new, print)\n",
    "        y_test = np.argmax(y_test, axis=-1)\n",
    "        y_pred = np.argmax(model.predict(x_test), axis=-1)\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "    if run_prediction_MM:\n",
    "        ## Import modules used for this Notebook\n",
    "        import numpy as np\n",
    "        import os\n",
    "        import multiprocessing\n",
    "        import time, datetime\n",
    "        import glob, re\n",
    "        import configparser\n",
    "        from itertools import accumulate\n",
    "        ## if LaueToolsNN is properly installed\n",
    "        from lauetoolsnn.utils_lauenn import get_multimaterial_detail, new_MP_multimat_function, resource_path, global_plots_MM\n",
    "        from lauetoolsnn.lauetools import dict_LaueTools as dictLT\n",
    "        from lauetoolsnn.NNmodels import read_hdf5\n",
    "        \n",
    "        import _pickle as cPickle\n",
    "        from tqdm import tqdm\n",
    "        ncpu = multiprocessing.cpu_count()\n",
    "        \n",
    "        material_= input_params[\"material_\"]\n",
    "        detectorparameters = input_params[\"detectorparameters\"]\n",
    "        pixelsize = input_params[\"pixelsize\"]\n",
    "        emax = input_params[\"emax\"]\n",
    "        emin = input_params[\"emin\"]\n",
    "        dim1 = input_params[\"dim1\"]\n",
    "        dim2 = input_params[\"dim2\"]\n",
    "        symm_ = input_params[\"symmetry\"]\n",
    "        SG = input_params[\"SG\"]\n",
    "        tolerance = input_params[\"matrix_tolerance\"]\n",
    "        tolerance_strain = input_params[\"tolerance_strain_refinement\"]\n",
    "        strain_free_parameters = input_params[\"free_parameters\"]\n",
    "        material_limit = input_params[\"material_limit\"]\n",
    "        material_phase_always_present = input_params[\"material_phase_always_present\"]\n",
    "        model_annote = \"DNN\"\n",
    "\n",
    "        ## Requirements\n",
    "        ## Experimental peak search parameters in case of RAW LAUE PATTERNS from detector\n",
    "        intensity_threshold = input_params[\"intensity_threshold\"]\n",
    "        boxsize = input_params[\"boxsize\"]\n",
    "        fit_peaks_gaussian = input_params[\"fit_peaks_gaussian\"]\n",
    "        FitPixelDev = input_params[\"FitPixelDev\"]\n",
    "        NumberMaxofFits = input_params[\"NumberMaxofFits\"]\n",
    "        bkg_treatment = \"A-B\"\n",
    "        mode_peaksearch = input_params[\"mode\"]\n",
    "        \n",
    "        ubmat = input_params[\"UB_matrix_to_detect\"] # How many orientation matrix to detect per Laue pattern\n",
    "        mode_spotCycle = input_params[\"mode_spotCycle\"] ## mode of calculation\n",
    "        use_previous_UBmatrix_name = input_params[\"use_previous\"] ## Try previous indexation solutions to speed up the process\n",
    "        strain_calculation = input_params[\"strain_compute\"] ## Strain refinement is required or not\n",
    "        ccd_label_global = input_params[\"ccd_label\"]\n",
    "        \n",
    "        ## Parameters to control the orientation matrix indexation\n",
    "        softmax_threshold_global = input_params[\"softmax_threshold_global\"] # softmax_threshold of the Neural network to consider\n",
    "        mr_threshold_global = 0.90 # match rate threshold to accept a solution immediately\n",
    "        cap_matchrate = input_params[\"cap_matchrate\"] * 100 ## any UB matrix providing MR less than this will be ignored\n",
    "        coeff = 0.30            ## coefficient to calculate the overlap of two solutions\n",
    "        coeff_overlap = input_params[\"coeff_overlap\"]    ##10% spots overlap is allowed with already indexed orientation\n",
    "\n",
    "        ## Additional parameters to refine the orientation matrix construction process\n",
    "        use_om_user = str(input_params[\"use_om_user\"]).lower()\n",
    "        path_user_OM = input_params[\"path_user_OM\"]\n",
    "        nb_spots_consider = input_params[\"nb_spots_consider\"]\n",
    "        residues_threshold = input_params[\"residues_threshold\"]\n",
    "        nb_spots_global_threshold = input_params[\"nb_spots_global_threshold\"]\n",
    "        option_global = \"v2\"\n",
    "        additional_expression = [\"none\"] # for strain assumptions, like a==b for HCP\n",
    "        \n",
    "        config_setting = configparser.ConfigParser()\n",
    "        filepath = resource_path('settings.ini')\n",
    "        print(\"Writing settings file in \" + filepath)\n",
    "        config_setting.read(filepath)\n",
    "        config_setting.set('CALLER', 'residues_threshold',str(residues_threshold))\n",
    "        config_setting.set('CALLER', 'nb_spots_global_threshold',str(nb_spots_global_threshold))\n",
    "        config_setting.set('CALLER', 'option_global',option_global)\n",
    "        config_setting.set('CALLER', 'use_om_user',use_om_user)\n",
    "        config_setting.set('CALLER', 'nb_spots_consider',str(nb_spots_consider))\n",
    "        config_setting.set('CALLER', 'path_user_OM',str(path_user_OM))\n",
    "        config_setting.set('CALLER', 'intensity', str(intensity_threshold))\n",
    "        config_setting.set('CALLER', 'boxsize', str(boxsize))\n",
    "        config_setting.set('CALLER', 'pixdev', str(FitPixelDev))\n",
    "        config_setting.set('CALLER', 'cap_softmax', str(softmax_threshold_global))\n",
    "        config_setting.set('CALLER', 'cap_mr', str(cap_matchrate/100.))\n",
    "        config_setting.set('CALLER', 'strain_free_parameters', \",\".join(strain_free_parameters))\n",
    "        config_setting.set('CALLER', 'additional_expression', \",\".join(additional_expression))\n",
    "        config_setting.set('CALLER', 'mode_peaksearch', str(mode_peaksearch))\n",
    "        with open(filepath, 'w') as configfile:\n",
    "            config_setting.write(configfile)\n",
    "\n",
    "        if len(material_) > 1:\n",
    "            prefix_mat = material_[0]\n",
    "            for ino, imat in enumerate(material_):\n",
    "                if ino == 0:\n",
    "                    continue\n",
    "                prefix_mat = prefix_mat + \"_\" + imat\n",
    "        else:\n",
    "            prefix_mat = material_[0]\n",
    "        \n",
    "        model_direc = os.path.join(global_path,prefix_mat+input_params[\"prefix\"])\n",
    "        \n",
    "        if not os.path.exists(model_direc):\n",
    "            print(\"The directory doesn't exists; please veify the path\")\n",
    "        else:\n",
    "            print(\"Directory where trained model is stored : \"+model_direc)\n",
    "            \n",
    "        ## get unit cell parameters and other details required for simulating Laue patterns\n",
    "        rules, symmetry, lattice_material, \\\n",
    "                            crystal, SG = get_multimaterial_detail(material_, SG, symm_)\n",
    "\n",
    "        filenameDirec = input_params[\"experimental_directory\"]\n",
    "        experimental_prefix = input_params[\"experimental_prefix\"]\n",
    "        lim_x, lim_y = input_params[\"grid_size_x\"], input_params[\"grid_size_y\"] \n",
    "        format_file = dictLT.dict_CCD[ccd_label_global][7]\n",
    "        \n",
    "        hkl_all_class0 = []\n",
    "        for ino, imat in enumerate(material_):\n",
    "            with open(model_direc+\"//classhkl_data_nonpickled_\"+imat+\".pickle\", \"rb\") as input_file:\n",
    "                hkl_all_class_load = cPickle.load(input_file)[0]\n",
    "            hkl_all_class0.append(hkl_all_class_load)\n",
    "            \n",
    "        ## load model related files and generate the model\n",
    "        classhkl = np.load(model_direc+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_0\"]\n",
    "        angbins = np.load(model_direc+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_1\"]\n",
    "        \n",
    "        # from two phase training dataset FIX\n",
    "        # if len(material_) <= 2: \n",
    "        #     ind_mat0 = np.load(model_direc+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_5\"]\n",
    "        #     ind_mat1 = np.load(model_direc+\"//MOD_grain_classhkl_angbin.npz\")[\"arr_6\"] \n",
    "        #     ind_mat = [ind_mat0, ind_mat0+ind_mat1]\n",
    "        # else:\n",
    "        ##Below lines are for dataset generated with multimat code\n",
    "        ind_mat_all = np.load(model_direc+\"//MOD_grain_classhkl_angbin.npz\",allow_pickle=True)[\"arr_5\"]\n",
    "        ind_mat = []\n",
    "        for inni in ind_mat_all:\n",
    "            ind_mat.append(len(inni))\n",
    "        ind_mat = [int(item) for item in accumulate(ind_mat)]\n",
    "        \n",
    "        # json_file = open(model_direc+\"//model_\"+prefix_mat+\".json\", 'r')\n",
    "        load_weights = model_direc + \"//model_\"+prefix_mat+\".h5\"\n",
    "        wb = read_hdf5(load_weights)\n",
    "        temp_key = list(wb.keys())\n",
    "        \n",
    "        ct = time.time()\n",
    "        now = datetime.datetime.fromtimestamp(ct)\n",
    "        c_time = now.strftime(\"%Y-%m-%d_%H-%M-%S\")   \n",
    "        \n",
    "        \n",
    "        if prediction_GUI:\n",
    "            from lauetoolsnn.GUI_multi_mat_LaueNN import start\n",
    "            if strain_calculation:\n",
    "                strain_label_global = \"YES\"\n",
    "            else:\n",
    "                strain_label_global = \"NO\"\n",
    "            ##Start the GUI plots\n",
    "            start(        \n",
    "                model_direc,\n",
    "                material_,\n",
    "                emin,\n",
    "                emax,\n",
    "                symmetry,\n",
    "                detectorparameters,\n",
    "                pixelsize,\n",
    "                lattice_material,\n",
    "                mode_spotCycle,\n",
    "                softmax_threshold_global,\n",
    "                mr_threshold_global,\n",
    "                cap_matchrate,\n",
    "                coeff,\n",
    "                coeff_overlap,\n",
    "                fit_peaks_gaussian,\n",
    "                FitPixelDev,\n",
    "                NumberMaxofFits,\n",
    "                tolerance_strain,\n",
    "                material_limit,\n",
    "                use_previous_UBmatrix_name,\n",
    "                material_phase_always_present,\n",
    "                crystal,\n",
    "                strain_free_parameters,\n",
    "                additional_expression,\n",
    "                strain_label_global, \n",
    "                ubmat, \n",
    "                boxsize, \n",
    "                intensity_threshold,\n",
    "                ccd_label_global, \n",
    "                experimental_prefix, \n",
    "                lim_x, \n",
    "                lim_y,\n",
    "                tolerance, \n",
    "                filenameDirec, \n",
    "                load_weights,\n",
    "                model_annote\n",
    "                )\n",
    "        \n",
    "        else:\n",
    "            ## Step 3: Initialize variables and prepare arguments for multiprocessing module\n",
    "            \n",
    "            col = [[] for i in range(int(ubmat))]\n",
    "            colx = [[] for i in range(int(ubmat))]\n",
    "            coly = [[] for i in range(int(ubmat))]\n",
    "            rotation_matrix = [[] for i in range(int(ubmat))]\n",
    "            strain_matrix = [[] for i in range(int(ubmat))]\n",
    "            strain_matrixs = [[] for i in range(int(ubmat))]\n",
    "            match_rate = [[] for i in range(int(ubmat))]\n",
    "            spots_len = [[] for i in range(int(ubmat))]\n",
    "            iR_pix = [[] for i in range(int(ubmat))]\n",
    "            fR_pix = [[] for i in range(int(ubmat))]\n",
    "            mat_global = [[] for i in range(int(ubmat))]\n",
    "            best_match = [[] for i in range(int(ubmat))]\n",
    "            spots1_global = [[] for i in range(int(ubmat))]\n",
    "            for i in range(int(ubmat)):\n",
    "                col[i].append(np.zeros((lim_x*lim_y,3)))\n",
    "                colx[i].append(np.zeros((lim_x*lim_y,3)))\n",
    "                coly[i].append(np.zeros((lim_x*lim_y,3)))\n",
    "                rotation_matrix[i].append(np.zeros((lim_x*lim_y,3,3)))\n",
    "                strain_matrix[i].append(np.zeros((lim_x*lim_y,3,3)))\n",
    "                strain_matrixs[i].append(np.zeros((lim_x*lim_y,3,3)))\n",
    "                match_rate[i].append(np.zeros((lim_x*lim_y,1)))\n",
    "                spots_len[i].append(np.zeros((lim_x*lim_y,1)))\n",
    "                iR_pix[i].append(np.zeros((lim_x*lim_y,1)))\n",
    "                fR_pix[i].append(np.zeros((lim_x*lim_y,1)))\n",
    "                mat_global[i].append(np.zeros((lim_x*lim_y,1)))\n",
    "                best_match[i].append([[] for jk in range(lim_x*lim_y)])\n",
    "                spots1_global[i].append([[] for jk in range(lim_x*lim_y)])\n",
    "    \n",
    "            # =============================================================================\n",
    "            #         ## Multi-processing routine\n",
    "            # =============================================================================        \n",
    "            ## Number of files to generate\n",
    "            grid_files = np.zeros((lim_x,lim_y))\n",
    "            filenm = np.chararray((lim_x,lim_y), itemsize=1000)\n",
    "            grid_files = grid_files.ravel()\n",
    "            filenm = filenm.ravel()\n",
    "            count_global = lim_x * lim_y\n",
    "            list_of_files = glob.glob(filenameDirec+'//'+experimental_prefix+'*.'+format_file)\n",
    "            ## sort files\n",
    "            list_of_files.sort(key=lambda var:[int(x) if x.isdigit() else x for x in re.findall(r'[^0-9]|[0-9]+', var)])\n",
    "            \n",
    "            if len(list_of_files) == count_global:\n",
    "                for ii in range(len(list_of_files)):\n",
    "                    grid_files[ii] = ii\n",
    "                    filenm[ii] = list_of_files[ii]     \n",
    "                print(\"expected \"+str(count_global)+\" files based on the XY grid (\"+str(lim_x)+\",\"+str(lim_y)+\") defined by user\")\n",
    "                print(\"and found \"+str(len(list_of_files))+\" files\")\n",
    "            else:\n",
    "                print(\"expected \"+str(count_global)+\" files based on the XY grid (\"+str(lim_x)+\",\"+str(lim_y)+\") defined by user\")\n",
    "                print(\"But found \"+str(len(list_of_files))+\" files (either all data is not written yet or maybe XY grid definition is not proper)\")\n",
    "                digits = len(str(count_global))\n",
    "                digits = max(digits,4)\n",
    "                # Temp fix\n",
    "                for ii in range(count_global):\n",
    "                    text = str(ii)\n",
    "                    if ii < 10000:\n",
    "                        string = text.zfill(4)\n",
    "                    else:\n",
    "                        string = text.zfill(5)\n",
    "                    file_name_temp = filenameDirec+'//'+experimental_prefix + string+'.'+format_file\n",
    "                    ## store it in a grid \n",
    "                    filenm[ii] = file_name_temp\n",
    "            \n",
    "            check = np.zeros((count_global,int(ubmat)))\n",
    "            # =============================================================================\n",
    "            blacklist = None\n",
    "            \n",
    "            ### Create a COR directory to be loaded in LaueTools\n",
    "            cor_file_directory = filenameDirec + \"//\" + experimental_prefix+\"CORfiles\"\n",
    "            if list_of_files[0].split(\".\")[-1] in ['cor',\"COR\",\"Cor\"]:\n",
    "                cor_file_directory = filenameDirec \n",
    "            if not os.path.exists(cor_file_directory):\n",
    "                os.makedirs(cor_file_directory)\n",
    "            \n",
    "            try_prevs = False\n",
    "            files_treated = []\n",
    "            \n",
    "            valu12 = [[ filenm[ii].decode(), \n",
    "                        ii,\n",
    "                        rotation_matrix,\n",
    "                        strain_matrix,\n",
    "                        strain_matrixs,\n",
    "                        col,\n",
    "                        colx,\n",
    "                        coly,\n",
    "                        match_rate,\n",
    "                        spots_len, \n",
    "                        iR_pix, \n",
    "                        fR_pix,\n",
    "                        best_match,\n",
    "                        mat_global,\n",
    "                        check,\n",
    "                        detectorparameters,\n",
    "                        pixelsize,\n",
    "                        angbins,\n",
    "                        classhkl,\n",
    "                        hkl_all_class0,\n",
    "                        emin,\n",
    "                        emax,\n",
    "                        material_,\n",
    "                        symmetry,\n",
    "                        lim_x,\n",
    "                        lim_y,\n",
    "                        strain_calculation, \n",
    "                        ind_mat, \n",
    "                        model_direc, \n",
    "                        tolerance,\n",
    "                        int(ubmat), ccd_label_global, \n",
    "                        None,\n",
    "                        float(intensity_threshold),\n",
    "                        int(boxsize),\n",
    "                        bkg_treatment,\n",
    "                        filenameDirec, \n",
    "                        experimental_prefix,\n",
    "                        blacklist,\n",
    "                        None,\n",
    "                        files_treated,\n",
    "                        try_prevs, ## try previous is kept true, incase if its stuck in loop\n",
    "                        wb,\n",
    "                        temp_key,\n",
    "                        cor_file_directory,\n",
    "                        mode_spotCycle,\n",
    "                        softmax_threshold_global,\n",
    "                        mr_threshold_global,\n",
    "                        cap_matchrate,\n",
    "                        tolerance_strain,\n",
    "                        NumberMaxofFits,\n",
    "                        fit_peaks_gaussian,\n",
    "                        FitPixelDev,\n",
    "                        coeff,\n",
    "                        coeff_overlap,\n",
    "                        material_limit,\n",
    "                        use_previous_UBmatrix_name,\n",
    "                        material_phase_always_present,\n",
    "                        crystal,\n",
    "                        strain_free_parameters,\n",
    "                        model_annote] for ii in range(count_global)]\n",
    "    \n",
    "            #% Launch multiprocessing prediction     \n",
    "            args = zip(valu12)\n",
    "            with multiprocessing.Pool(ncpu) as pool:\n",
    "                results = pool.starmap(new_MP_multimat_function, tqdm(args, total=len(valu12)), chunksize=1)\n",
    "                \n",
    "                for r in results:\n",
    "                    r_message_mpdata = r\n",
    "                    strain_matrix_mpdata, strain_matrixs_mpdata, rotation_matrix_mpdata, col_mpdata,\\\n",
    "                    colx_mpdata, coly_mpdata, match_rate_mpdata, mat_global_mpdata,\\\n",
    "                        cnt_mpdata, meta_mpdata, files_treated_mpdata, spots_len_mpdata, \\\n",
    "                            iR_pixel_mpdata, fR_pixel_mpdata, best_match_mpdata, check_mpdata = r_message_mpdata\n",
    "            \n",
    "                    for i_mpdata in files_treated_mpdata:\n",
    "                        files_treated.append(i_mpdata)\n",
    "            \n",
    "                    for intmat_mpdata in range(int(ubmat)):\n",
    "                        check[cnt_mpdata,intmat_mpdata] = check_mpdata[cnt_mpdata,intmat_mpdata]\n",
    "                        mat_global[intmat_mpdata][0][cnt_mpdata] = mat_global_mpdata[intmat_mpdata][0][cnt_mpdata]\n",
    "                        strain_matrix[intmat_mpdata][0][cnt_mpdata,:,:] = strain_matrix_mpdata[intmat_mpdata][0][cnt_mpdata,:,:]\n",
    "                        strain_matrixs[intmat_mpdata][0][cnt_mpdata,:,:] = strain_matrixs_mpdata[intmat_mpdata][0][cnt_mpdata,:,:]\n",
    "                        rotation_matrix[intmat_mpdata][0][cnt_mpdata,:,:] = rotation_matrix_mpdata[intmat_mpdata][0][cnt_mpdata,:,:]\n",
    "                        col[intmat_mpdata][0][cnt_mpdata,:] = col_mpdata[intmat_mpdata][0][cnt_mpdata,:]\n",
    "                        colx[intmat_mpdata][0][cnt_mpdata,:] = colx_mpdata[intmat_mpdata][0][cnt_mpdata,:]\n",
    "                        coly[intmat_mpdata][0][cnt_mpdata,:] = coly_mpdata[intmat_mpdata][0][cnt_mpdata,:]\n",
    "                        match_rate[intmat_mpdata][0][cnt_mpdata] = match_rate_mpdata[intmat_mpdata][0][cnt_mpdata]\n",
    "                        spots_len[intmat_mpdata][0][cnt_mpdata] = spots_len_mpdata[intmat_mpdata][0][cnt_mpdata]\n",
    "                        iR_pix[intmat_mpdata][0][cnt_mpdata] = iR_pixel_mpdata[intmat_mpdata][0][cnt_mpdata]\n",
    "                        fR_pix[intmat_mpdata][0][cnt_mpdata] = fR_pixel_mpdata[intmat_mpdata][0][cnt_mpdata]\n",
    "                        best_match[intmat_mpdata][0][cnt_mpdata] = best_match_mpdata[intmat_mpdata][0][cnt_mpdata]\n",
    "                    \n",
    "            #% Save results\n",
    "            save_directory_ = model_direc+\"//results_\"+prefix_mat+\"_\"+c_time\n",
    "            if not os.path.exists(save_directory_):\n",
    "                os.makedirs(save_directory_)\n",
    "                \n",
    "            ## intermediate saving of pickle objects with results\n",
    "            np.savez_compressed(save_directory_+ \"//results.npz\", \n",
    "                                best_match, mat_global, rotation_matrix, strain_matrix, \n",
    "                                strain_matrixs, col, colx, coly, match_rate, files_treated,\n",
    "                                lim_x, lim_y, spots_len, iR_pix, fR_pix,\n",
    "                                material_)\n",
    "            ## intermediate saving of pickle objects with results\n",
    "            with open(save_directory_+ \"//results.pickle\", \"wb\") as output_file:\n",
    "                    cPickle.dump([best_match, mat_global, rotation_matrix, strain_matrix, \n",
    "                                  strain_matrixs, col, colx, coly, match_rate, files_treated,\n",
    "                                  lim_x, lim_y, spots_len, iR_pix, fR_pix,\n",
    "                                  material_, lattice_material,\n",
    "                                  symmetry, crystal], output_file)\n",
    "            print(\"data saved in \", save_directory_)\n",
    "    \n",
    "            try:\n",
    "                global_plots_MM(lim_x, lim_y, rotation_matrix, strain_matrix, strain_matrixs, \n",
    "                              col, colx, coly, match_rate, mat_global, spots_len, \n",
    "                              iR_pix, fR_pix, save_directory_, material_,\n",
    "                              match_rate_threshold=5, bins=30)\n",
    "            except:\n",
    "                print(\"Error in the global plots module\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
